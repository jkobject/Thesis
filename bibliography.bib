@article{1000genomesprojectconsortiumGlobalReferenceHuman2015,
  title    = {A Global Reference for Human Genetic Variation},
  author   = {{1000 Genomes Project Consortium} and Auton, Adam and Brooks, Lisa D. and Durbin, Richard M. and Garrison, Erik P. and Kang, Hyun Min and Korbel, Jan O. and Marchini, Jonathan L. and McCarthy, Shane and McVean, Gil A. and Abecasis, Gon{\c c}alo R.},
  year     = 2015,
  month    = oct,
  journal  = {Nature},
  volume   = {526},
  number   = {7571},
  pages    = {68--74},
  issn     = {1476-4687},
  doi      = {10.1038/nature15393},
  abstract = {The 1000 Genomes Project set out to provide a comprehensive description of common human genetic variation by applying whole-genome sequencing to a diverse set of individuals from multiple populations. Here we report completion of the project, having reconstructed the genomes of 2,504 individuals from 26 populations using a combination of low-coverage whole-genome sequencing, deep exome sequencing, and dense microarray genotyping. We characterized a broad spectrum of genetic variation, in total over 88 million variants (84.7 million single nucleotide polymorphisms (SNPs), 3.6 million short insertions/deletions (indels), and 60,000 structural variants), all phased onto high-quality haplotypes. This resource includes {$>$}99\% of SNP variants with a frequency of {$>$}1\% for a variety of ancestries. We describe the distribution of genetic variation across the global sample, and discuss the implications for common disease studies.},
  langid   = {english},
  pmcid    = {PMC4750478},
  pmid     = {26432245},
  keywords = {Datasets as Topic,Demography,Disease Susceptibility,Exome,Genetic Variation,Genetics Medical,Genetics Population,Genome Human,Genome-Wide Association Study,Genomics,Genotype,Haplotypes,High-Throughput Nucleotide Sequencing,Humans,INDEL Mutation,Internationality,Physical Chromosome Mapping,Polymorphism Single Nucleotide,Quantitative Trait Loci,Rare Diseases,Reference Standards,Sequence Analysis DNA},
  file     = {/Users/jkobject/Zotero/storage/5MJBWKTG/1000 Genomes Project Consortium et al. - 2015 - A global reference for human genetic variation.pdf}
}

@misc{10xgenomicsXeniumPrimeFFPE2024,
  title  = {Preview Data: FFPE Human Skin Primary Dermal Melanoma with 5K Human Pan Tissue and Pathways Panel},
  author = {{10x Genomics}},
  year   = {2024},
  url    = {https://www.10xgenomics.com/datasets/xenium-prime-ffpe-human-skin}
}


@article{abelsonPredictionAcuteMyeloid2018,
  title        = {Prediction of Acute Myeloid Leukaemia Risk in Healthy Individuals},
  author       = {Abelson, Sagi and Collord, Grace and Ng, Stanley W. K. and Weissbrod, Omer and Mendelson Cohen, Netta and Niemeyer, Elisabeth and Barda, Noam and Zuzarte, Philip C. and Heisler, Lawrence and Sundaravadanam, Yogi and Luben, Robert and Hayat, Shabina and Wang, Ting Ting and Zhao, Zhen and Cirlan, Iulia and Pugh, Trevor J. and Soave, David and Ng, Karen and Latimer, Calli and Hardy, Claire and Raine, Keiran and Jones, David and Hoult, Diana and Britten, Abigail and McPherson, John D. and Johansson, Mattias and Mbabaali, Faridah and Eagles, Jenna and Miller, Jessica K. and Pasternack, Danielle and Timms, Lee and Krzyzanowski, Paul and Awadalla, Philip and Costa, Rui and Segal, Eran and Bratman, Scott V. and Beer, Philip and Behjati, Sam and Martincorena, Inigo and Wang, Jean C. Y. and Bowles, Kristian M. and Quirós, J. Ramón and Karakatsani, Anna and La Vecchia, Carlo and Trichopoulou, Antonia and Salamanca-Fernández, Elena and Huerta, José M. and Barricarte, Aurelio and Travis, Ruth C. and Tumino, Rosario and Masala, Giovanna and Boeing, Heiner and Panico, Salvatore and Kaaks, Rudolf and Krämer, Alwin and Sieri, Sabina and Riboli, Elio and Vineis, Paolo and Foll, Matthieu and McKay, James and Polidoro, Silvia and Sala, Núria and Khaw, Kay-Tee and Vermeulen, Roel and Campbell, Peter J. and Papaemmanuil, Elli and Minden, Mark D. and Tanay, Amos and Balicer, Ran D. and Wareham, Nicholas J. and Gerstung, Moritz and Dick, John E. and Brennan, Paul and Vassiliou, George S. and Shlush, Liran I.},
  date         = {2018-07},
  journaltitle = {Nature},
  volume       = {559},
  number       = {7714},
  pages        = {400--404},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gdsh88},
  url          = {http://www.nature.com/articles/s41586-018-0317-6},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@misc{abnarQuantifyingAttentionFlow2020,
  title         = {Quantifying {{Attention Flow}} in {{Transformers}}},
  author        = {Abnar, Samira and Zuidema, Willem},
  year          = {2020},
  month         = may,
  number        = {arXiv:2005.00928},
  eprint        = {2005.00928},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2005.00928},
  urldate       = {2024-07-15},
  abstract      = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/ZKFTKP8C/Abnar and Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf;/Users/jkobject/Zotero/storage/CVXHTS9C/2005.html}
}

@article{abramsonAccurateStructurePrediction2024,
  title     = {Accurate Structure Prediction of Biomolecular Interactions with {{AlphaFold}} 3},
  author    = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O'Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e}, Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and {Cowen-Rivers}, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and {\v Z}{\'i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
  year      = {2024},
  month     = jun,
  journal   = {Nature},
  volume    = {630},
  number    = {8016},
  pages     = {493--500},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/s41586-024-07487-w},
  urldate   = {2025-02-25},
  abstract  = {The introduction of AlphaFold\,21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2--6. Here we describe our AlphaFold\,3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein--ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein--nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody--antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
  copyright = {2024 The Author(s)},
  langid    = {english},
  keywords  = {Drug discovery,Machine learning,Protein structure predictions,Structural biology},
  file      = {/Users/jkobject/Zotero/storage/AM97STPL/Abramson et al. - 2024 - Accurate structure prediction of biomolecular inte.pdf}
}


@article{adamPioneerFactorsGovern2015,
  title        = {Pioneer Factors Govern Super-Enhancer Dynamics in Stem Cell Plasticity and Lineage Choice},
  author       = {Adam, Rene C. and Yang, Hanseul and Rockowitz, Shira and Larsen, Samantha B. and Nikolova, Maria and Oristian, Daniel S. and Polak, Lisa and Kadaja, Meelis and Asare, Amma and Zheng, Deyou and Fuchs, Elaine},
  date         = {2015-03-18},
  journaltitle = {Nature},
  volume       = {521},
  number       = {7552},
  pages        = {366--370},
  issn         = {0028-0836, 1476-4687},
  doi          = {10.1038/nature14289},
  url          = {http://www.nature.com/doifinder/10.1038/nature14289},
  urldate      = {2019-03-22},
  langid       = {english}
}


@article{adamsonMultiplexedSingleCellCRISPR2016,
  title    = {A {{Multiplexed Single-Cell CRISPR Screening Platform Enables Systematic Dissection}} of the {{Unfolded Protein Response}}},
  author   = {Adamson, Britt and Norman, Thomas M. and Jost, Marco and Cho, Min Y. and Nu{\~n}ez, James K. and Chen, Yuwen and Villalta, Jacqueline E. and Gilbert, Luke A. and Horlbeck, Max A. and Hein, Marco Y. and Pak, Ryan A. and Gray, Andrew N. and Gross, Carol A. and Dixit, Atray and Parnas, Oren and Regev, Aviv and Weissman, Jonathan S.},
  year     = {2016},
  month    = dec,
  journal  = {Cell},
  volume   = {167},
  number   = {7},
  pages    = {1867-1882.e21},
  issn     = {0092-8674},
  doi      = {10.1016/j.cell.2016.11.048},
  urldate  = {2024-04-18},
  abstract = {Functional genomics efforts face tradeoffs between number of perturbations examined and complexity of phenotypes measured. We bridge this gap with Perturb-seq, which combines droplet-based single-cell RNA-seq with a strategy for barcoding CRISPR-mediated perturbations, allowing many perturbations to be profiled in pooled format. We applied Perturb-seq to dissect the mammalian unfolded protein response (UPR) using single and combinatorial CRISPR perturbations. Two genome-scale CRISPR interference (CRISPRi) screens identified genes whose repression perturbs ER homeostasis. Subjecting {$\sim$}100 hits to Perturb-seq enabled high-precision functional clustering of genes. Single-cell analyses decoupled the three UPR branches, revealed bifurcated UPR branch activation among cells~subject to the same perturbation, and uncovered differential activation of the branches across hits, including an isolated feedback loop between the translocon and IRE1{$\alpha$}. These studies provide insight into how the three sensors of ER homeostasis monitor distinct types of stress and highlight the ability of Perturb-seq to dissect complex cellular responses.},
  keywords = {cell-to-cell heterogeneity,CRIPSRi,CRISPR,genome-scale screening,single-cell genomics,Single-cell RNA-seq,unfolded protein response},
  file     = {/Users/jkobject/Zotero/storage/I53XI9MU/Adamson et al. - 2016 - A Multiplexed Single-Cell CRISPR Screening Platfor.pdf}
}

@article{adelsonSpatiotemporalEnergyModels1985,
  title        = {Spatiotemporal Energy Models for the Perception of Motion},
  author       = {Adelson, Edward H. and Bergen, James R.},
  date         = {1985-02-01},
  journaltitle = {Journal of the Optical Society of America A},
  volume       = {2},
  number       = {2},
  pages        = {284},
  issn         = {1084-7529, 1520-8532},
  doi          = {10/djvwmf},
  url          = {https://www.osapublishing.org/abstract.cfm?URI=josaa-2-2-284},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Dropbox/Journal Club/Adelson et Bergen - 1985 - Spatiotemporal energy models for the perception of.pdf}
}

@article{aguetGeneticEffectsGene2017,
  title     = {Genetic Effects on Gene Expression across Human Tissues},
  author    = {Aguet, Fran{\c c}ois and Brown, Andrew A. and Castel, Stephane E. and Davis, Joe R. and He, Yuan and Jo, Brian and Mohammadi, Pejman and Park, YoSon and Parsana, Princy and Segr{\`e}, Ayellet V. and Strober, Benjamin J. and Zappala, Zachary and Cummings, Beryl B. and Gelfand, Ellen T. and Hadley, Kane and Huang, Katherine H. and Lek, Monkol and Li, Xiao and Nedzel, Jared L. and Nguyen, Duyen Y. and Noble, Michael S. and Sullivan, Timothy J. and Tukiainen, Taru and MacArthur, Daniel G. and Getz, Gad and Addington, Anjene and Guan, Ping and Koester, Susan and Little, A. Roger and Lockhart, Nicole C. and Moore, Helen M. and Rao, Abhi and Struewing, Jeffery P. and Volpi, Simona and Brigham, Lori E. and Hasz, Richard and Hunter, Marcus and Johns, Christopher and Johnson, Mark and Kopen, Gene and Leinweber, William F. and Lonsdale, John T. and McDonald, Alisa and Mestichelli, Bernadette and Myer, Kevin and Roe, Bryan and Salvatore, Michael and Shad, Saboor and Thomas, Jeffrey A. and Walters, Gary and Washington, Michael and Wheeler, Joseph and Bridge, Jason and Foster, Barbara A. and Gillard, Bryan M. and Karasik, Ellen and Kumar, Rachna and Miklos, Mark and Moser, Michael T. and Jewell, Scott D. and Montroy, Robert G. and Rohrer, Daniel C. and Valley, Dana and Mash, Deborah C. and Davis, David A. and Sobin, Leslie and Barcus, Mary E. and Branton, Philip A. and Abell, Nathan S. and Balliu, Brunilda and Delaneau, Olivier and Fr{\'e}sard, Laure and Gamazon, Eric R. and {Garrido-Mart{\'i}n}, Diego and Gewirtz, Ariel D. H. and Gliner, Genna and Gloudemans, Michael J. and Han, Buhm and He, Amy Z. and Hormozdiari, Farhad and Li, Xin and Liu, Boxiang and Kang, Eun Yong and McDowell, Ian C. and Ongen, Halit and Palowitch, John J. and Peterson, Christine B. and Quon, Gerald and Ripke, Stephan and Saha, Ashis and Shabalin, Andrey A. and Shimko, Tyler C. and Sul, Jae Hoon and Teran, Nicole A. and Tsang, Emily K. and Zhang, Hailei and Zhou, Yi-Hui and Bustamante, Carlos D. and Cox, Nancy J. and Guig{\'o}, Roderic and Kellis, Manolis and McCarthy, Mark I. and Conrad, Donald F. and Eskin, Eleazar and Li, Gen and Nobel, Andrew B. and Sabatti, Chiara and Stranger, Barbara E. and Wen, Xiaoquan and Wright, Fred A. and Ardlie, Kristin G. and Dermitzakis, Emmanouil T. and Lappalainen, Tuuli and Aguet, Fran{\c c}ois and Ardlie, Kristin G. and Cummings, Beryl B. and Gelfand, Ellen T. and Getz, Gad and Hadley, Kane and Handsaker, Robert E. and Huang, Katherine H. and Kashin, Seva and Karczewski, Konrad J. and Lek, Monkol and Li, Xiao and MacArthur, Daniel G. and Nedzel, Jared L. and Nguyen, Duyen T. and Noble, Michael S. and Segr{\`e}, Ayellet V. and Trowbridge, Casandra A. and Tukiainen, Taru and Abell, Nathan S. and Balliu, Brunilda and Barshir, Ruth and Basha, Omer and Battle, Alexis and Bogu, Gireesh K. and Brown, Andrew and Brown, Christopher D. and Castel, Stephane E. and Chen, Lin S. and Chiang, Colby and Conrad, Donald F. and Cox, Nancy J. and Damani, Farhan N. and Davis, Joe R. and Delaneau, Olivier and Dermitzakis, Emmanouil T. and Engelhardt, Barbara E. and Eskin, Eleazar and Ferreira, Pedro G. and Fr{\'e}sard, Laure and Gamazon, Eric R. and {Garrido-Mart{\'i}n}, Diego and Gewirtz, Ariel D.H. and Gliner, Genna and Gloudemans, Michael J. and Guigo, Roderic and Hall, Ira M. and Han, Buhm and He, Yuan and Hormozdiari, Farhad and Howald, Cedric and Kyung Im, Hae and Jo, Brian and Yong Kang, Eun and Kim, Yungil and {Kim-Hellmuth}, Sarah and Lappalainen, Tuuli and Li, Gen and Li, Xin and Liu, Boxiang and Mangul, Serghei and McCarthy, Mark I. and McDowell, Ian C. and Mohammadi, Pejman and Monlong, Jean and Montgomery, Stephen B. and {Mu{\~n}oz-Aguirre}, Manuel and Ndungu, Anne W. and Nicolae, Dan L. and Nobel, Andrew B. and Oliva, Meritxell and Ongen, Halit and Palowitch, John J. and Panousis, Nikolaos and Papasaikas, Panagiotis and Park, YoSon and Parsana, Princy and Payne, Anthony J. and Peterson, Christine B. and Quan, Jie and Reverter, Ferran and Sabatti, Chiara and Saha, Ashis and Sammeth, Michael and Scott, Alexandra J. and Shabalin, Andrey A. and Sodaei, Reza and Stephens, Matthew and Stranger, Barbara E. and Strober, Benjamin J. and Sul, Jae Hoon and Tsang, Emily K. and Urbut, Sarah and {van de Bunt}, Martijn and Wang, Gao and Wen, Xiaoquan and Wright, Fred A. and Xi, Hualin S. and {Yeger-Lotem}, Esti and Zappala, Zachary and Zaugg, Judith B. and Zhou, Yi-Hui and Akey, Joshua M. and Bates, Daniel and Chan, Joanne and Chen, Lin S. and Claussnitzer, Melina and Demanelis, Kathryn and Diegel, Morgan and Doherty, Jennifer A. and Feinberg, Andrew P. and Fernando, Marian S. and Halow, Jessica and Hansen, Kasper D. and Haugen, Eric and Hickey, Peter F. and Hou, Lei and Jasmine, Farzana and Jian, Ruiqi and Jiang, Lihua and Johnson, Audra and Kaul, Rajinder and Kellis, Manolis and Kibriya, Muhammad G. and Lee, Kristen and Billy Li, Jin and Li, Qin and Li, Xiao and Lin, Jessica and Lin, Shin and Linder, Sandra and Linke, Caroline and Liu, Yaping and Maurano, Matthew T. and Molinie, Benoit and Montgomery, Stephen B. and Nelson, Jemma and Neri, Fidencio J. and Oliva, Meritxell and Park, Yongjin and Pierce, Brandon L. and Rinaldi, Nicola J. and Rizzardi, Lindsay F. and Sandstrom, Richard and Skol, Andrew and Smith, Kevin S. and Snyder, Michael P. and Stamatoyannopoulos, John and Stranger, Barbara E. and Tang, Hua and Tsang, Emily K. and Wang, Li and Wang, Meng and Van Wittenberghe, Nicholas and Wu, Fan and Zhang, Rui and Nierras, Concepcion R. and Branton, Philip A. and Carithers, Latarsha J. and Guan, Ping and Moore, Helen M. and Rao, Abhi and Vaught, Jimmie B. and Gould, Sarah E. and Lockart, Nicole C. and Martin, Casey and Struewing, Jeffery P. and Volpi, Simona and Addington, Anjene M. and Koester, Susan E. and Little, A. Roger and {GTEx Consortium} and {Lead analysts:} and Laboratory, Data Analysis \& Coordinating Center (LDACC): and {NIH program management:} and {Biospecimen collection:} and {Pathology:} and {eQTL manuscript working group:} and Laboratory, Data Analysis \& Coordinating Center (LDACC)---Analysis Working Group and {Statistical Methods groups---Analysis Working Group} and {Enhancing GTEx (eGTEx) groups} and {NIH Common Fund} and {NIH/NCI} and {NIH/NHGRI} and {NIH/NIMH} and {NIH/NIDA} and {Biospecimen Collection Source Site---NDRI}},
  year      = {2017},
  month     = oct,
  journal   = {Nature},
  volume    = {550},
  number    = {7675},
  pages     = {204--213},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/nature24277},
  urldate   = {2024-07-19},
  abstract  = {Characterization of the molecular function of the human genome and its variation across individuals is essential for identifying the cellular mechanisms that underlie human genetic traits and diseases. The Genotype-Tissue Expression (GTEx) project aims to characterize variation in gene expression levels across individuals and diverse tissues of the human body, many of which are not easily accessible. Here we describe genetic effects on gene expression levels across 44 human tissues. We find that local genetic variation affects gene expression levels for the majority of genes, and we further identify inter-chromosomal genetic effects for 93 genes and 112 loci. On the basis of the identified genetic effects, we characterize patterns of tissue specificity, compare local and distal effects, and evaluate the functional properties of the genetic effects. We also demonstrate that multi-tissue, multi-individual data can be used to identify genes and pathways affected by human disease-associated variation, enabling a mechanistic interpretation of gene regulation and the genetic basis of disease.},
  copyright = {2017 The Author(s)},
  langid    = {english},
  keywords  = {Gene regulation,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/SRTPMXMU/Aguet et al. - 2017 - Genetic effects on gene expression across human ti.pdf}
}

@article{aguiarBayesianNonparametricDiscovery2018,
  title        = {Bayesian Nonparametric Discovery of Isoforms and Individual Specific Quantification},
  author       = {Aguiar, Derek and Cheng, Li-Fang and Dumitrascu, Bianca and Mordelet, Fantine and Pai, Athma A. and Engelhardt, Barbara E.},
  date         = {2018-12},
  journaltitle = {Nature Communications},
  volume       = {9},
  number       = {1},
  issn         = {2041-1723},
  doi          = {10/gdhwpx},
  url          = {http://www.nature.com/articles/s41467-018-03402-w},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{aguirreGenomicCopyNumber2016,
  title        = {Genomic {{Copy Number Dictates}} a {{Gene-Independent Cell Response}} to {{CRISPR}}/{{Cas9 Targeting}}},
  author       = {Aguirre, A. J. and Meyers, R. M. and Weir, B. A. and Vazquez, F. and Zhang, C.-Z. and Ben-David, U. and Cook, A. and Ha, G. and Harrington, W. F. and Doshi, M. B. and Kost-Alimova, M. and Gill, S. and Xu, H. and Ali, L. D. and Jiang, G. and Pantel, S. and Lee, Y. and Goodale, A. and Cherniack, A. D. and Oh, C. and Kryukov, G. and Cowley, G. S. and Garraway, L. A. and Stegmaier, K. and Roberts, C. W. and Golub, T. R. and Meyerson, M. and Root, D. E. and Tsherniak, A. and Hahn, W. C.},
  date         = {2016-08-01},
  journaltitle = {Cancer Discovery},
  volume       = {6},
  number       = {8},
  pages        = {914--929},
  issn         = {2159-8274, 2159-8290},
  doi          = {10/bjzn},
  url          = {http://cancerdiscovery.aacrjournals.org/cgi/doi/10.1158/2159-8290.CD-16-0154},
  urldate      = {2019-03-22},
  abstract     = {The CRISPR/Cas9 system enables genome editing and somatic cell genetic screens in mammalian cells. We performed genome-scale loss-of-function screens in 33 cancer cell lines to identify genes essential for proliferation/survival and found a strong correlation between increased gene copy number and decreased cell viability after genome editing. Within regions of copy-number gain, CRISPR/Cas9 targeting of both expressed and unexpressed genes, as well as intergenic loci, led to significantly decreased cell proliferation through induction of a G2 cell-cycle arrest. By examining single-guide RNAs that map to multiple genomic sites, we found that this cell response to CRISPR/Cas9 editing correlated strongly with the number of target loci. These observations indicate that genome targeting by CRISPR/Cas9 elicits a gene-independent antiproliferative cell response. This effect has important practical implications for the interpretation of CRISPR/Cas9 screening data and confounds the use of this technology for the identification of essential genes in amplified regions.},
  langid       = {english}
}

@article{aibarSCENICSinglecellRegulatory2017,
  title      = {{{SCENIC}}: Single-Cell Regulatory Network Inference and Clustering},
  shorttitle = {{{SCENIC}}},
  author     = {Aibar, Sara and {Gonz{\'a}lez-Blas}, Carmen Bravo and Moerman, Thomas and {Huynh-Thu}, V{\^a}n Anh and Imrichova, Hana and Hulselmans, Gert and Rambow, Florian and Marine, Jean-Christophe and Geurts, Pierre and Aerts, Jan and {van den Oord}, Joost and Atak, Zeynep Kalender and Wouters, Jasper and Aerts, Stein},
  year       = {2017},
  month      = nov,
  journal    = {Nature Methods},
  volume     = {14},
  number     = {11},
  pages      = {1083--1086},
  publisher  = {Nature Publishing Group},
  issn       = {1548-7105},
  doi        = {10.1038/nmeth.4463},
  urldate    = {2024-04-18},
  abstract   = {SCENIC enables simultaneous regulatory network inference and robust cell clustering from single-cell RNA-seq data.},
  copyright  = {2017 Springer Nature America, Inc.},
  langid     = {english},
  keywords   = {Gene regulatory networks,RNA sequencing,Transcriptomics,Tumour heterogeneity},
  file       = {/Users/jkobject/Zotero/storage/N8EJMFDH/Aibar et al. - 2017 - SCENIC single-cell regulatory network inference a.pdf}
}

@incollection{Alberts2015CellsGenomes,
  author    = {Alberts, Bruce and Johnson, Alexander D. and Morgan, David and Raff, Martin and Roberts, Keith and Walter, Peter},
  title     = {Cells and Genomes},
  booktitle = {Molecular Biology of the Cell},
  year      = {2015},
  edition   = {6},
  publisher = {Garland Science},
  address   = {New York},
  chapter   = {1}
}

@article{alipanahiPredictingSequenceSpecificities2015,
  title        = {Predicting the Sequence Specificities of {{DNA-}} and {{RNA-binding}} Proteins by Deep Learning},
  author       = {Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey, Brendan J},
  date         = {2015-08},
  journaltitle = {Nature Biotechnology},
  volume       = {33},
  number       = {8},
  pages        = {831--838},
  issn         = {1087-0156, 1546-1696},
  doi          = {10/f7mkrd},
  url          = {http://www.nature.com/articles/nbt.3300},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Alipanahi et al. - 2015 - Predicting the sequence specificities of DNA- and .pdf}
}

@article{alonExpansionSequencingSpatially2021,
  title      = {Expansion Sequencing: {{Spatially}} Precise in Situ Transcriptomics in Intact Biological Systems},
  shorttitle = {Expansion Sequencing},
  author     = {Alon, Shahar and Goodwin, Daniel R. and Sinha, Anubhav and Wassie, Asmamaw T. and Chen, Fei and Daugharthy, Evan R. and Bando, Yosuke and Kajita, Atsushi and Xue, Andrew G. and Marrett, Karl and Prior, Robert and Cui, Yi and Payne, Andrew C. and Yao, Chun-Chen and Suk, Ho-Jun and Wang, Ru and Yu, Chih-Chieh (Jay) and Tillberg, Paul and Reginato, Paul and Pak, Nikita and Liu, Songlei and Punthambaker, Sukanya and Iyer, Eswar P. R. and Kohman, Richie E. and Miller, Jeremy A. and Lein, Ed S. and Lako, Ana and Cullen, Nicole and Rodig, Scott and Helvie, Karla and Abravanel, Daniel L. and Wagle, Nikhil and Johnson, Bruce E. and Klughammer, Johanna and Slyper, Michal and Waldman, Julia and {Jan{\'e}-Valbuena}, Judit and {Rozenblatt-Rosen}, Orit and Regev, Aviv and {IMAXT Consortium} and Church, George M. and Marblestone, Adam H. and Boyden, Edward S.},
  year       = {2021},
  month      = jan,
  journal    = {Science},
  volume     = {371},
  number     = {6528},
  pages      = {eaax2656},
  publisher  = {American Association for the Advancement of Science},
  doi        = {10.1126/science.aax2656},
  urldate    = {2025-02-25},
  abstract   = {Methods for highly multiplexed RNA imaging are limited in spatial resolution and thus in their ability to localize transcripts to nanoscale and subcellular compartments. We adapt expansion microscopy, which physically expands biological specimens, for long-read untargeted and targeted in situ RNA sequencing. We applied untargeted expansion sequencing (ExSeq) to the mouse brain, which yielded the readout of thousands of genes, including splice variants. Targeted ExSeq yielded nanoscale-resolution maps of RNAs throughout dendrites and spines in the neurons of the mouse hippocampus, revealing patterns across multiple cell types, layer-specific cell types across the mouse visual cortex, and the organization and position-dependent states of tumor and immune cells in a human metastatic breast cancer biopsy. Thus, ExSeq enables highly multiplexed mapping of RNAs from nanoscale to system scale.},
  file       = {/Users/jkobject/Zotero/storage/VN4JYC2N/Alon et al. - 2021 - Expansion sequencing Spatially precise in situ tr.pdf}
}

@misc{alsabbaghFoundationModelsMeet2023,
  title         = {Foundation {{Models Meet Imbalanced Single-Cell Data When Learning Cell Type Annotations}}},
  author        = {Alsabbagh, Abdel Rahman and de Infante, Alberto Maillo Ruiz and {Gomez-Cabrero}, David and Kiani, Narsis A. and Khan, Sumeer Ahmad and Tegn{\'e}r, Jesper N.},
  year          = {2023},
  month         = oct,
  primaryclass  = {New Results},
  pages         = {2023.10.24.563625},
  publisher     = {bioRxiv},
  doi           = {10.1101/2023.10.24.563625},
  urldate       = {2024-04-19},
  abstract      = {With the emergence of single-cell foundation models, an important question arises: how do these models perform when trained on datasets having an imbalance in cell type distribution due to rare cell types or biased sampling? We benchmark three foundation models, scGPT, scBERT, and Geneformer, using skewed single-cell cell-type distribution for cell-type annotation. While all models had reduced performance when challenged with rare cell types, scGPT and scBERT, performed better than Geneformer. Notably, in contrast to scGPT and scBERT, Geneformer uses ordinal positions of the tokenized genes rather than actual raw gene expression values. To mitigate the effect of a skewed distribution, we find that random oversampling, but not random undersampling, improved the performance for all three foundation models. Finally, scGPT, using FlashAttention, has the fastest computational speed, whereas scBERT is more memory-efficient. We conclude that tokenization and data representation are essential areas of research, and new strategies are needed to mitigate the effects of imbalanced learning in single-cell foundation models. Code and data for reproducibility are available at https://github.com/SabbaghCodes/ImbalancedLearningForSingleCellFoundationModels.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/XQVTSW7C/Alsabbagh et al. - 2023 - Foundation Models Meet Imbalanced Single-Cell Data.pdf}
}

@article{amodioExploringSinglecellData2019,
  title     = {Exploring Single-Cell Data with Deep Multitasking Neural Networks},
  author    = {Amodio, Matthew and {van Dijk}, David and Srinivasan, Krishnan and Chen, William S. and Mohsen, Hussein and Moon, Kevin R. and Campbell, Allison and Zhao, Yujiao and Wang, Xiaomei and Venkataswamy, Manjunatha and Desai, Anita and Ravi, V. and Kumar, Priti and Montgomery, Ruth and Wolf, Guy and Krishnaswamy, Smita},
  year      = {2019},
  month     = nov,
  journal   = {Nature Methods},
  volume    = {16},
  number    = {11},
  pages     = {1139--1145},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-019-0576-7},
  urldate   = {2025-02-25},
  abstract  = {It is currently challenging to analyze single-cell data consisting of many cells and samples, and to address variations arising from batch effects and different sample preparations. For this purpose, we present SAUCIE, a deep neural network that combines parallelization and scalability offered by neural networks, with the deep representation of data that can be learned by them to perform many single-cell data analysis tasks. Our regularizations (penalties) render features learned in hidden layers of the neural network interpretable. On large, multi-patient datasets, SAUCIE's various hidden layers contain denoised and batch-corrected data, a low-dimensional visualization and unsupervised clustering, as well as other information that can be used to explore the data. We analyze a 180-sample dataset consisting of 11 million T cells from dengue patients in India, measured with mass cytometry. SAUCIE can batch correct and identify cluster-based signatures of acute dengue infection and create a patient manifold, stratifying immune response to dengue.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Computational biology and bioinformatics,Computational models,Gene expression,Machine learning},
  file      = {/Users/jkobject/Zotero/storage/LGUL296T/Amodio et al. - 2019 - Exploring single-cell data with deep multitasking .pdf}
}

@article{andrychowiczLearningLearnGradient,
  title      = {Learning to Learn by Gradient Descent by Gradient Descent},
  author     = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio Gómez and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan},
  pages      = {17},
  abstract   = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/Andrychowicz et al. - Learning to learn by gradient descent by gradient .pdf}
}

@article{arafehPresentFutureCancer2025,
  title     = {The Present and Future of the {{Cancer Dependency Map}}},
  author    = {Arafeh, Rand and Shibue, Tsukasa and Dempster, Joshua M. and Hahn, William C. and Vazquez, Francisca},
  year      = 2025,
  month     = jan,
  journal   = {Nature Reviews Cancer},
  volume    = {25},
  number    = {1},
  pages     = {59--73},
  publisher = {Nature Publishing Group},
  issn      = {1474-1768},
  doi       = {10.1038/s41568-024-00763-x},
  urldate   = {2025-12-13},
  abstract  = {Despite tremendous progress in the past decade, the complex and heterogeneous nature of cancer complicates efforts to identify new therapies and therapeutic combinations that achieve durable responses in most patients. Further advances in cancer therapy will rely, in part, on the development of targeted therapeutics matched with the genetic and molecular characteristics of cancer. The Cancer Dependency Map (DepMap) is a large-scale data repository and research platform, aiming to systematically reveal the landscape of cancer vulnerabilities in thousands of genetically and molecularly annotated cancer models. DepMap is used routinely by cancer researchers and translational scientists and has facilitated the identification of several novel and selective therapeutic strategies for multiple cancer types that are being tested in the clinic. However, it is also clear that the current version of DepMap is not yet comprehensive. In this Perspective, we review (1) the impact and current uses of DepMap, (2) the opportunities to enhance DepMap to overcome its current limitations, and (3) the ongoing efforts to further improve and expand DepMap.},
  copyright = {2024 Springer Nature Limited},
  langid    = {english},
  keywords  = {Cancer models,Drug development},
  file      = {/Users/jkobject/Zotero/storage/MBNCDDDE/Arafeh et al. - 2025 - The present and future of the Cancer Dependency Map.pdf}
}


@unpublished{arandjelovicLookListenLearn2017,
  title       = {Look, {{Listen}} and {{Learn}}},
  author      = {Arandjelović, Relja and Zisserman, Andrew},
  date        = {2017-05-23},
  eprint      = {1705.08168},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1705.08168},
  urldate     = {2019-03-22},
  abstract    = {We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself – the correspondence between the visual and the audio streams, and we introduce a novel “Audio-Visual Correspondence” learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation  = {00000}
}


@inproceedings{arandjelovicLookListenLearn2017a,
  title      = {Look, {{Listen}} and {{Learn}}},
  author     = {Arandjelovic, Relja and Zisserman, Andrew},
  date       = {2017-10},
  pages      = {609--617},
  publisher  = {IEEE},
  doi        = {10.1109/ICCV.2017.73},
  url        = {http://ieeexplore.ieee.org/document/8237335/},
  urldate    = {2018-04-11},
  abstract   = {We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself – the correspondence between the visual and the audio streams, and we introduce a novel “Audio-Visual Correspondence” learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.},
  isbn       = {978-1-5386-1032-9},
  langid     = {english},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Arandjelovic et Zisserman - 2017 - Look, Listen and Learn.pdf}
}

@article{argelaguetMultiOmicsFactorAnalysis2018,
  title        = {Multi-{{Omics}} Factor Analysis - a Framework for Unsupervised Integration of Multi-Omic Data Sets},
  author       = {Argelaguet, Ricard and Velten, Britta and Arnol, Damien and Dietrich, Sascha and Zenz, Thorsten and Marioni, John C. and Huber, Wolfgang and Buettner, Florian and Stegle, Oliver},
  date         = {2018-04-11},
  journaltitle = {bioRxiv},
  doi          = {10.1101/217554},
  url          = {http://biorxiv.org/lookup/doi/10.1101/217554},
  urldate      = {2019-03-22},
  abstract     = {Multi-omic studies promise the improved characterization of biological processes across molecular layers. However, methods for the unsupervised integration of the resulting heterogeneous datasets are lacking. We present Multi-Omics Factor Analysis (MOFA), a computational method for discovering the principal sources of variation in multi-omic datasets. MOFA infers a set of (hidden) factors that capture biological and technical sources of variability. It disentangles axes of heterogeneity that are shared across multiple modalities and those specific to individual data modalities. The learnt factors enable a variety of downstream analyses, including identification of sample subgroups, data imputation, and the detection of outlier samples. We applied MOFA to a cohort of 200 patient samples of chronic lymphocytic leukaemia, profiled for somatic mutations, RNA expression, DNA methylation and ex-vivo drug responses. MOFA identified major dimensions of disease heterogeneity, including immunoglobulin heavy chain variable region status, trisomy of chromosome 12 and previously underappreciated drivers, such as response to oxidative stress. In a second application, we used MOFA to analyse single-cell multiomics data, identifying coordinated transcriptional and epigenetic changes along cell differentiation.},
  langid       = {english}
}

@unpublished{artetxeMassivelyMultilingualSentence2018,
  title       = {Massively {{Multilingual Sentence Embeddings}} for {{Zero-Shot Cross-Lingual Transfer}} and {{Beyond}}},
  author      = {Artetxe, Mikel and Schwenk, Holger},
  date        = {2018-12-26},
  eprint      = {1812.10464},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1812.10464},
  urldate     = {2019-03-22},
  abstract    = {We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different language families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our approach sets a new state-of-the-art on zero-shot cross-lingual natural language inference for all the 14 languages in the XNLI dataset but one. We also achieve very competitive results in cross-lingual document classification (MLDoc dataset). Our sentence embeddings are also strong at parallel corpus mining, establishing a new state-of-the-art in the BUCC shared task for 3 of its 4 language pairs. Finally, we introduce a new test set of aligned sentences in 122 languages based on the Tatoeba corpus, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low-resource languages. Our PyTorch implementation, pre-trained encoder and the multilingual test set will be freely available.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation  = {00000}
}


@article{ArXiv09080050v2StatML,
  title      = {{{arXiv}}:0908.0050v2  [Stat.{{ML}}]  11 {{Feb}} 2010},
  pages      = {45},
  abstract   = {Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.},
  langid     = {english},
  annotation = {00000}
}

@misc{athiwaratkunThereAreMany2019,
  title         = {There {{Are Many Consistent Explanations}} of {{Unlabeled Data}}: {{Why You Should Average}}},
  shorttitle    = {There {{Are Many Consistent Explanations}} of {{Unlabeled Data}}},
  author        = {Athiwaratkun, Ben and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
  year          = {2019},
  month         = feb,
  number        = {arXiv:1806.05594},
  eprint        = {1806.05594},
  primaryclass  = {cs, stat},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1806.05594},
  urldate       = {2024-07-21},
  abstract      = {Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0\% error on CIFAR-10 with only 4000 labels, compared to the previous best result in the literature of 6.3\%.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/93S87XS3/Athiwaratkun et al. - 2019 - There Are Many Consistent Explanations of Unlabele.pdf;/Users/jkobject/Zotero/storage/FSHQKE3E/1806.html}
}

@article{atickWhatDoesRetina1992,
  title        = {What {{Does}} the {{Retina Know}} about {{Natural Scenes}}?},
  author       = {Atick, Joseph J. and Redlich, A. Norman},
  date         = {1992-03},
  journaltitle = {Neural Computation},
  volume       = {4},
  number       = {2},
  pages        = {196--210},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/bnbd8w},
  url          = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.2.196},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Dropbox/Journal Club/Atick et Redlich - 1992 - What Does the Retina Know about Natural Scenes.pdf}
}

@misc{attiFullArticleFundamental2025,
  title  = {Fundamental Limitations of Foundation Models in Single-Cell Transcriptomics},
  author = {Atti, Shahin and Subramaniam, Shankar},
  year   = {2025},
  eprint = {2025.06.26.661767},
  doi    = {10.1101/2025.06.26.661767}
}

@article{averbeckNeuralCorrelationsPopulation2006,
  title        = {Neural Correlations, Population Coding and Computation},
  author       = {Averbeck, Bruno B. and Latham, Peter E. and Pouget, Alexandre},
  date         = {2006-05},
  journaltitle = {Nature Reviews Neuroscience},
  volume       = {7},
  number       = {5},
  pages        = {358--366},
  issn         = {1471-003X, 1471-0048},
  doi          = {10/cwxhpt},
  url          = {http://www.nature.com/articles/nrn1888},
  urldate      = {2018-04-11},
  abstract     = {How the brain encodes information in population activity, and how it combines and manipulates that activity as it carries out computations, are questions that lie at the heart of systems neuroscience. During the past decade, with the advent of multi-electrode recording and improved theoretical models, these questions have begun to yield answers. However, a complete understanding of neuronal variability, and, in particular, how it affects population codes, is missing. This is because variability in the brain is typically correlated, and although the exact effects of these correlations are not known, it is known that they can be large. Here, we review studies that address the interaction between neuronal noise and population codes, and discuss their implications for population coding in general.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Averbeck et al. - 2006 - Neural correlations, population coding and computa.pdf}
}

@unpublished{aytarPlayingHardExploration2018,
  title       = {Playing Hard Exploration Games by Watching {{YouTube}}},
  author      = {Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date        = {2018-05-29},
  eprint      = {1805.11592},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  doi         = {1805.11592v2},
  url         = {http://arxiv.org/abs/1805.11592},
  urldate     = {2019-03-22},
  abstract    = {Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent’s exact environment setup and the demonstrator’s action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games MONTEZUMA’S REVENGE, PITFALL! and PRIVATE EYE for the first time†, even if the agent is not presented with any environment rewards.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{abramsonAccurateStructurePrediction2024a,
  title = {Accurate Structure Prediction of Biomolecular Interactions with {{AlphaFold}} 3},
  author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O'Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e}, Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and {Cowen-Rivers}, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and {\v Z}{\'i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
  year = 2024,
  month = jun,
  journal = {Nature},
  volume = {630},
  number = {8016},
  pages = {493--500},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07487-w},
  urldate = {2026-01-15},
  abstract = {The introduction of AlphaFold\,21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2--6. Here we describe our AlphaFold\,3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein--ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein--nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody--antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Drug discovery,Machine learning,Protein structure predictions,Structural biology},
  file = {/Users/jkobject/Zotero/storage/DQE6DAXB/Abramson et al. - 2024 - Accurate structure prediction of biomolecular interactions with AlphaFold 3.pdf}
}

@misc{beltagyLongformerLongDocumentTransformer2020,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year = 2020,
  month = dec,
  number = {arXiv:2004.05150},
  eprint = {2004.05150},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.05150},
  urldate = {2026-01-15},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jkobject/Zotero/storage/JRDI2D5J/Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf;/Users/jkobject/Zotero/storage/TEQ5ATNJ/2004.html}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = 2018,
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10903},
  urldate = {2026-01-15},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/jkobject/Zotero/storage/CLM3BATG/Veličković et al. - 2018 - Graph Attention Networks.pdf;/Users/jkobject/Zotero/storage/XJT4TVJV/1710.html}
}

@misc{zaheerBigBirdTransformers2021,
  title = {Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}},
  shorttitle = {Big {{Bird}}},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  year = 2021,
  month = jan,
  number = {arXiv:2007.14062},
  eprint = {2007.14062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.14062},
  urldate = {2026-01-15},
  abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jkobject/Zotero/storage/T89T7ESC/Zaheer et al. - 2021 - Big Bird Transformers for Longer Sequences.pdf;/Users/jkobject/Zotero/storage/RSIRTTPU/2007.html}
}

@article{faravelliBrainOrganoidsTools2025,
  title = {Brain {{Organoids}}: {{Tools}} for {{Understanding}} the {{Uniqueness}} and {{Individual Variability}} of the {{Human Brain}}},
  shorttitle = {Brain {{Organoids}}},
  author = {Faravelli, Irene and {Ant{\'o}n-Bola{\~n}os}, Noelia and Brown, Juliana R. and Arlotta, Paola},
  year = 2025,
  month = aug,
  journal = {Annual Review of Genomics and Human Genetics},
  volume = {26},
  number = {1},
  pages = {299--320},
  issn = {1545-293X},
  doi = {10.1146/annurev-genom-111522-014009},
  abstract = {Understanding the drivers of human brain specialization, and how specialized properties are codified during development and evolution, seems to be within reach for the first time. Improved cell-based experimental models of the human brain have empowered the field to address some of the most fundamental questions about our brains, including mechanisms of neurodevelopment, the etiology of neurological disease, and the underpinnings of human-to-human variation in brain function and response. The emergence of scalable in vitro systems has enabled investigation of interindividual variation within large human cohorts in both normal development and disease processes, which is fundamental to developing effective and personalized treatments. This review explores recent advancements in organoid technology, highlighting future directions that employ interdisciplinary approaches to enhance the physiological relevance of these models. This work promises to bring us ever closer to understanding not only what makes a brain human but also how each of our brains is human in unique ways.},
  langid = {english},
  pmid = {40854826},
  keywords = {Animals,Brain,genetic variation,human brain organoids,human brain specialization,Humans,neurological diseases,Organoids}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = 2021,
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2026-01-15},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50~years9. Despite recent progress10--14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
  file = {/Users/jkobject/Zotero/storage/4BDPGN4V/Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf}
}


@misc{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = 2022,
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2026-01-15},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jkobject/Zotero/storage/TX7KE6E8/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf;/Users/jkobject/Zotero/storage/3N56VJVU/2112.html}
}

@misc{shenNaturalSpeech2Latent2023,
  title = {{{NaturalSpeech}} 2: {{Latent Diffusion Models}} Are {{Natural}} and {{Zero-Shot Speech}} and {{Singing Synthesizers}}},
  shorttitle = {{{NaturalSpeech}} 2},
  author = {Shen, Kai and Ju, Zeqian and Tan, Xu and Liu, Yanqing and Leng, Yichong and He, Lei and Qin, Tao and Zhao, Sheng and Bian, Jiang},
  year = 2023,
  month = may,
  number = {arXiv:2304.09116},
  eprint = {2304.09116},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.09116},
  urldate = {2026-01-15},
  abstract = {Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/jkobject/Zotero/storage/IRPC8H8A/Shen et al. - 2023 - NaturalSpeech 2 Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers.pdf;/Users/jkobject/Zotero/storage/MSCRBZ58/2304.html}
}

@misc{singerMakeAVideoTexttoVideoGeneration2022,
  title = {Make-{{A-Video}}: {{Text-to-Video Generation}} without {{Text-Video Data}}},
  shorttitle = {Make-{{A-Video}}},
  author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  year = 2022,
  month = sep,
  number = {arXiv:2209.14792},
  eprint = {2209.14792},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.14792},
  urldate = {2026-01-15},
  abstract = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/jkobject/Zotero/storage/KACZNWLQ/Singer et al. - 2022 - Make-A-Video Text-to-Video Generation without Text-Video Data.pdf;/Users/jkobject/Zotero/storage/AB3QXLHL/2209.html}
}


@article{badia-i-mompelGeneRegulatoryNetwork2023,
  title     = {Gene Regulatory Network Inference in the Era of Single-Cell Multi-Omics},
  author    = {{Badia-i-Mompel}, Pau and Wessels, Lorna and {M{\"u}ller-Dott}, Sophia and Trimbour, R{\'e}mi and Ramirez Flores, Ricardo O. and Argelaguet, Ricard and {Saez-Rodriguez}, Julio},
  year      = {2023},
  month     = nov,
  journal   = {Nature Reviews Genetics},
  volume    = {24},
  number    = {11},
  pages     = {739--754},
  publisher = {Nature Publishing Group},
  issn      = {1471-0064},
  doi       = {10.1038/s41576-023-00618-5},
  urldate   = {2024-04-18},
  abstract  = {The interplay between chromatin, transcription factors and genes generates complex regulatory circuits that can be represented as gene regulatory networks (GRNs). The study of GRNs is useful to understand how cellular identity is established, maintained and disrupted in disease. GRNs can be inferred from experimental data --- historically, bulk omics data --- and/or from the literature. The advent of single-cell multi-omics technologies has led to the development of novel computational methods that leverage genomic, transcriptomic and chromatin accessibility information to infer GRNs at an unprecedented resolution. Here, we review the key principles of inferring GRNs that encompass transcription factor--gene interactions from transcriptomics and chromatin accessibility data. We focus on the comparison and classification of methods that use single-cell multimodal data. We highlight challenges in GRN inference, in particular with respect to benchmarking, and potential further developments using additional data modalities.},
  copyright = {2023 Springer Nature Limited},
  langid    = {english},
  keywords  = {Gene regulatory networks,Regulatory networks},
  file      = {/Users/jkobject/Zotero/storage/5JLAHSXN/Badia-i-Mompel et al. - 2023 - Gene regulatory network inference in the era of si.pdf}
}

@unpublished{badrinarayananSegNetDeepConvolutional2015,
  title       = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle  = {{{SegNet}}},
  author      = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  date        = {2015-11-02},
  eprint      = {1511.00561},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1511.00561},
  urldate     = {2019-03-22},
  abstract    = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation  = {00000}
}

@article{bahdanauNEURALMACHINETRANSLATION2015,
  title      = {{{NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE}}},
  author     = {Bahdanau, Dzmitry and Bengio, KyungHyun Cho Yoshua},
  date       = {2015},
  pages      = {15},
  abstract   = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Bahdanau et Bengio - 2015 - NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO .pdf}
}

@misc{baiScLongBillionParameter2024,
  title  = {scLong: A Billion-Parameter Foundation Model for Capturing Long-Range Gene Context in Single-Cell Transcriptomics},
  author = {Bai, Ding and others},
  year   = {2024},
  eprint = {2024.11.09.622759},
  doi    = {10.1101/2024.11.09.622759}
}

@article{bank3DView1NKP,
  title      = {{{3D View}}: {{1NKP}}},
  shorttitle = {{{3D View}}},
  author     = {Bank, RCSB Protein Data},
  journal    = {Crystal structure of Myc-Max recognizing DNA},
  urldate    = {2025-02-17},
  abstract   = {1NKP: Crystal structure of Myc-Max recognizing DNA},
  langid     = {american},
  file       = {/Users/jkobject/Zotero/storage/K5FIBT9N/1NKP.html}
}

@article{barashDecipheringSplicingCode2010,
  title        = {Deciphering the Splicing Code},
  author       = {Barash, Yoseph and Calarco, John A. and Gao, Weijun and Pan, Qun and Wang, Xinchen and Shai, Ofer and Blencowe, Benjamin J. and Frey, Brendan J.},
  date         = {2010-05},
  journaltitle = {Nature},
  volume       = {465},
  number       = {7294},
  pages        = {53--59},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/cvk585},
  url          = {http://www.nature.com/articles/nature09000},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{baratoStochasticThermodynamicsInformation2014,
  title        = {Stochastic Thermodynamics with Information Reservoirs},
  author       = {Barato, Andre C. and Seifert, Udo},
  date         = {2014-10-31},
  journaltitle = {Physical Review E},
  volume       = {90},
  number       = {4},
  eprint       = {1408.1224},
  eprinttype   = {arXiv},
  issn         = {1539-3755, 1550-2376},
  doi          = {10/gfxbfg},
  url          = {http://arxiv.org/abs/1408.1224},
  urldate      = {2019-03-22},
  abstract     = {We generalize stochastic thermodynamics to include information reservoirs. Such information reservoirs, which can be modeled as a sequence of bits, modify the second law. For example, work extraction from a system in contact with a single heat bath becomes possible if the system also interacts with an information reservoir. We obtain an inequality, and the corresponding fluctuation theorem, generalizing the standard entropy production of stochastic thermodynamics. From this inequality we can derive an information processing entropy production, which gives the second law in the presence of information reservoirs. We also develop a systematic linear response theory for information processing machines. For a unicyclic machine powered by an information reservoir, the efficiency at maximum power can deviate from the standard value of 1/2. For the case where energy is consumed to erase the tape, the efficiency at maximum erasure rate is found to be 1/2.},
  langid       = {english},
  keywords     = {Condensed Matter - Statistical Mechanics},
  annotation   = {00000}
}

@article{baronCancerArchetypesCoopt2018,
  title        = {Cancer Archetypes Co-Opt and Adapt the Transcriptional Programs of Existing Cellular States},
  author       = {Baron, Maayan and Kim, Isabella S and Moncada, Reuben and Yan, Yun and Campbell, Nathaniel R and White, Richard M and Yanai, Itai},
  date         = {2018-08-22},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbfv},
  url          = {http://biorxiv.org/lookup/doi/10.1101/396622},
  urldate      = {2019-03-22},
  abstract     = {Tumors evolve as independent systems comprising complex survival-ensuring functions, however the nature of these distinct processes and their recurrence across cancers is not clear. Here we propose that melanoma cancer-cells can be classified to three 'archetypes' that co-opt the neural crest, mature melanocytes, and stress gene expression programs, respectively, have a unique subclonal structure, and are conserved between zebrafish and human melanomas. Studying the natural history of a zebrafish melanoma tumor at the single-cell level, we found that one archetype exclusively exhibits the signature of the Warburg effect, suggesting that a shifting balance in energy production occurs differentially in the tumor. Deconvolving bulk human melanomas, we found that patients with a dominant fraction of the neural crest archetype show worse survival rates, indicating a clinical relevance for the composition of archetypes. Finally, we provide evidence that extending our approach to other cancer types can reveal universal and cancer-specific archetypes.},
  langid       = {english},
  annotation   = {00000}
}

@article{barretinaCancerCellLine2012,
  title        = {The {{Cancer Cell Line Encyclopedia}} Enables Predictive Modelling of Anticancer Drug Sensitivity},
  author       = {Barretina, Jordi and Caponigro, Giordano and Stransky, Nicolas and Venkatesan, Kavitha and Margolin, Adam A. and Kim, Sungjoon and Wilson, Christopher J. and Lehár, Joseph and Kryukov, Gregory V. and Sonkin, Dmitriy and Reddy, Anupama and Liu, Manway and Murray, Lauren and Berger, Michael F. and Monahan, John E. and Morais, Paula and Meltzer, Jodi and Korejwa, Adam and Jané-Valbuena, Judit and Mapa, Felipa A. and Thibault, Joseph and Bric-Furlong, Eva and Raman, Pichai and Shipway, Aaron and Engels, Ingo H. and Cheng, Jill and Yu, Guoying K. and Yu, Jianjun and Aspesi, Peter and family=Silva, given=Melanie, prefix=de, useprefix=true and Jagtap, Kalpana and Jones, Michael D. and Wang, Li and Hatton, Charles and Palescandolo, Emanuele and Gupta, Supriya and Mahan, Scott and Sougnez, Carrie and Onofrio, Robert C. and Liefeld, Ted and MacConaill, Laura and Winckler, Wendy and Reich, Michael and Li, Nanxin and Mesirov, Jill P. and Gabriel, Stacey B. and Getz, Gad and Ardlie, Kristin and Chan, Vivien and Myer, Vic E. and Weber, Barbara L. and Porter, Jeff and Warmuth, Markus and Finan, Peter and Harris, Jennifer L. and Meyerson, Matthew and Golub, Todd R. and Morrissey, Michael P. and Sellers, William R. and Schlegel, Robert and Garraway, Levi A.},
  date         = {2012-03},
  journaltitle = {Nature},
  volume       = {483},
  number       = {7391},
  pages        = {603--607},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/f4kjq3},
  url          = {http://www.nature.com/articles/nature11003},
  urldate      = {2019-03-22},
  langid       = {english}
}

@unpublished{battagliaInteractionNetworksLearning2016,
  title       = {Interaction {{Networks}} for {{Learning}} about {{Objects}}, {{Relations}} and {{Physics}}},
  author      = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
  date        = {2016-12-01},
  eprint      = {1612.00222},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1612.00222},
  urldate     = {2019-03-22},
  abstract    = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation  = {00000}
}

@online{battagliaInteractionNetworksLearning2016a,
  title       = {Interaction {{Networks}} for {{Learning}} about {{Objects}}, {{Relations}} and {{Physics}}},
  author      = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
  date        = {2016-12-01},
  eprint      = {1612.00222},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1612.00222},
  urldate     = {2023-01-19},
  abstract    = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{batznerE3equivariantGraphNeural2022,
  title     = {E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials},
  author    = {Batzner, Simon and Musaelian, Albert and Sun, Lixin and Geiger, Mario and Mailoa, Jonathan P. and Kornbluth, Mordechai and Molinari, Nicola and Smidt, Tess E. and Kozinsky, Boris},
  year      = {2022},
  month     = may,
  journal   = {Nature Communications},
  volume    = {13},
  number    = {1},
  pages     = {2453},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-022-29939-5},
  urldate   = {2025-02-25},
  abstract  = {This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales.},
  copyright = {2022 The Author(s)},
  langid    = {english},
  keywords  = {Atomistic models,Computational chemistry,Computational methods,Computer science,Molecular dynamics},
  file      = {/Users/jkobject/Zotero/storage/2UVT68PG/Batzner et al. - 2022 - E(3)-equivariant graph neural networks for data-ef.pdf}
}

@misc{BcgscNtEmbd2025,
  title        = {Bcgsc/{{ntEmbd}}},
  year         = {2025},
  month        = jan,
  urldate      = {2025-02-06},
  abstract     = {Deep learning embedding for nucleotide sequences},
  howpublished = {BC Cancer Canada's Michael Smith Genome Sciences Centre}
}

@misc{Bcl2AssociatedAthanogene,
  title        = {Bcl-2 Associated Athanogene 5 ({{Bag5}}) Is Overexpressed in Prostate Cancer and Inhibits {{ER-stress}} Induced Apoptosis - {{PMC}}},
  urldate      = {2024-07-25},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3598994/},
  file         = {/Users/jkobject/Zotero/storage/6Q4MFUDD/PMC3598994.html}
}

@article{behanPrioritisationOncologyTherapeutic2018,
  title        = {Prioritisation of Oncology Therapeutic Targets Using {{CRISPR-Cas9}} Screening},
  author       = {Behan, Fiona M and Iorio, Francesco and Gonçalves, Emanuel and Picco, Gabriele and Beaver, Charlotte M and Santos, Rita and Rao, Yanhua and Ansari, Rizwan and Harper, Sarah and Jackson, David Adam and McRae, Rebecca and Pooley, Rachel and Wilkinson, Piers and Dow, David and Buser-Doepner, Carolyn and Stronach, Euan A and Saez-Rodriguez, Julio and Yusa, Kosuke and Mathew, Garnett J},
  date         = {2018-12-20},
  journaltitle = {bioRxiv},
  doi          = {10/gfscrv},
  url          = {http://biorxiv.org/lookup/doi/10.1101/502005},
  urldate      = {2019-03-22},
  abstract     = {Functional genomics approaches can overcome current limitations that hamper oncology drug development such as lack of robust target identification and clinical efficacy. Here we performed genome-scale CRISPR-Cas9 screens in 204 human cancer cell lines from 12 cancer-types and developed a data-driven framework to prioritise cancer therapeutic candidates. We integrated gene cell fitness effects with genomic biomarkers and target tractability for drug development to systematically prioritise new oncology targets in defined tissues and genotypes. Furthermore, we took one of our most promising dependencies, Werner syndrome RecQ helicase, and verified it as a candidate target for tumours with microsatellite instability. Our analysis provides a comprehensive resource of cancer dependencies, a framework to prioritise oncology targets, and nominates specific new candidates. The principles described in this study can transform the initial stages of the drug development process contributing to a new, diverse and more effective portfolio of oncology targets.},
  langid       = {english}
}

@article{bellInformationMaximizationApproachBlind1995,
  title        = {An {{Information-Maximization Approach}} to {{Blind Separation}} and {{Blind Deconvolution}}},
  author       = {Bell, Anthony J. and Sejnowski, Terrence J.},
  date         = {1995-11},
  journaltitle = {Neural Computation},
  volume       = {7},
  number       = {6},
  pages        = {1129--1159},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/b88mxd},
  url          = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.6.1129},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Dropbox/Journal Club/Bell et Sejnowski - 1995 - An Information-Maximization Approach to Blind Sepa.pdf}
}

@article{bellotCanDeepLearning2018,
  title        = {Can {{Deep Learning Improve Genomic Prediction}} of {{Complex Human Traits}}?},
  author       = {Bellot, Pau and family=Campos, given=Gustavo, prefix=de los, useprefix=true and Pérez-Enciso, Miguel},
  date         = {2018-11},
  journaltitle = {Genetics},
  volume       = {210},
  number       = {3},
  pages        = {809--819},
  issn         = {0016-6731, 1943-2631},
  doi          = {10/gfmtvh},
  url          = {http://www.genetics.org/lookup/doi/10.1534/genetics.118.301298},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{ben-davidGeneticTranscriptionalEvolution2018,
  title        = {Genetic and Transcriptional Evolution Alters Cancer Cell Line Drug Response},
  author       = {Ben-David, Uri and Siranosian, Benjamin and Ha, Gavin and Tang, Helen and Oren, Yaara and Hinohara, Kunihiko and Strathdee, Craig A. and Dempster, Joshua and Lyons, Nicholas J. and Burns, Robert and Nag, Anwesha and Kugener, Guillaume and Cimini, Beth and Tsvetkov, Peter and Maruvka, Yosef E. and O’Rourke, Ryan and Garrity, Anthony and Tubelli, Andrew A. and Bandopadhayay, Pratiti and Tsherniak, Aviad and Vazquez, Francisca and Wong, Bang and Birger, Chet and Ghandi, Mahmoud and Thorner, Aaron R. and Bittker, Joshua A. and Meyerson, Matthew and Getz, Gad and Beroukhim, Rameen and Golub, Todd R.},
  date         = {2018-08},
  journaltitle = {Nature},
  volume       = {560},
  number       = {7718},
  pages        = {325--330},
  issn         = {0028-0836, 1476-4687},
  doi          = {10.1038/s41586-018-0409-3},
  url          = {http://www.nature.com/articles/s41586-018-0409-3},
  urldate      = {2019-03-22},
  langid       = {english}
}

@misc{benali2025pushingaccuracylimitfoundation,
  title         = {Pushing the Accuracy Limit of Foundation Neural Network Models with Quantum Monte Carlo Forces and Path Integrals},
  author        = {Anouar Benali and Thomas Plé and Olivier Adjoua and Valay Agarawal and Thomas Applencourt and Marharyta Blazhynska and Raymond Clay III and Kevin Gasperich and Khalid Hossain and Jeongnim Kim and Christopher Knight and Jaron T. Krogel and Yvon Maday and Maxime Maria and Matthieu Montes and Ye Luo and Evgeny Posenitskiy and Corentin Villot and Venkatram Vishwanath and Louis Lagardère and Jean-Philip Piquemal},
  year          = {2025},
  eprint        = {2504.07948},
  archiveprefix = {arXiv},
  primaryclass  = {physics.chem-ph},
  url           = {https://arxiv.org/abs/2504.07948}
}

@misc{bendidiBenchmarkingTranscriptomicsFoundation2024,
  title         = {Benchmarking {{Transcriptomics Foundation Models}} for {{Perturbation Analysis}} : One {{PCA}} Still Rules Them All},
  shorttitle    = {Benchmarking {{Transcriptomics Foundation Models}} for {{Perturbation Analysis}}},
  author        = {Bendidi, Ihab and Whitfield, Shawn and {Kenyon-Dean}, Kian and Yedder, Hanene Ben and Mesbahi, Yassir El and Noutahi, Emmanuel and Denton, Alisandra K.},
  year          = {2024},
  month         = nov,
  number        = {arXiv:2410.13956},
  eprint        = {2410.13956},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2410.13956},
  urldate       = {2025-02-25},
  abstract      = {Understanding the relationships among genes, compounds, and their interactions in living organisms remains limited due to technological constraints and the complexity of biological data. Deep learning has shown promise in exploring these relationships using various data types. However, transcriptomics, which provides detailed insights into cellular states, is still underused due to its high noise levels and limited data availability. Recent advancements in transcriptomics sequencing provide new opportunities to uncover valuable insights, especially with the rise of many new foundation models for transcriptomics, yet no benchmark has been made to robustly evaluate the effectiveness of these rising models for perturbation analysis. This article presents a novel biologically motivated evaluation framework and a hierarchy of perturbation analysis tasks for comparing the performance of pretrained foundation models to each other and to more classical techniques of learning from transcriptomics data. We compile diverse public datasets from different sequencing techniques and cell lines to assess models performance. Our approach identifies scVI and PCA to be far better suited models for understanding biological perturbations in comparison to existing foundation models, especially in their application in real-world scenarios.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/9J4VYFJS/Bendidi et al. - 2024 - Benchmarking Transcriptomics Foundation Models for.pdf;/Users/jkobject/Zotero/storage/YHVLBKFV/2410.html}
}

@misc{bengioRepresentationLearningReview2014a,
  title         = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle    = {Representation {{Learning}}},
  author        = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year          = {2014},
  month         = apr,
  number        = {arXiv:1206.5538},
  eprint        = {1206.5538},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1206.5538},
  urldate       = {2025-03-27},
  abstract      = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/GX7GRLR2/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf;/Users/jkobject/Zotero/storage/T5YVJKE7/1206.html}
}

@unpublished{benhendaChemGANChallengeDrug2017,
  title       = {{{ChemGAN}} Challenge for Drug Discovery: Can {{AI}} Reproduce Natural Chemical Diversity?},
  shorttitle  = {{{ChemGAN}} Challenge for Drug Discovery},
  author      = {Benhenda, Mostapha},
  date        = {2017-08-28},
  eprint      = {1708.08227},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1708.08227},
  urldate     = {2019-03-22},
  abstract    = {Generating molecules with desired chemical properties is important for drug discovery. The use of generative neural networks is promising for this task. However, from visual inspection, it often appears that generated samples lack diversity. In this paper, we quantify this internal chemical diversity, and we raise the following challenge: can a nontrivial AI model reproduce natural chemical diversity for desired molecules? To illustrate this question, we consider two generative models: a Reinforcement Learning model and the recently introduced ORGAN. Both fail at this challenge. We hope this challenge will stimulate research in this direction.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{berkesSlowFeatureAnalysis2005,
  title        = {Slow Feature Analysis Yields a Rich Repertoire of Complex Cell Properties},
  author       = {Berkes, P. and Wiskott, L.},
  date         = {2005-07-01},
  journaltitle = {Journal of Vision},
  volume       = {5},
  number       = {6},
  pages        = {9--9},
  issn         = {1534-7362},
  doi          = {10/cc5zck},
  url          = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/5.6.9},
  urldate      = {2018-04-11},
  abstract     = {In this study we investigate temporal slowness as a learning principle for receptive fields using slow feature analysis, a new algorithm to determine functions that extract slowly varying signals from the input data. We find a good qualitative and quantitative match between the set of learned functions trained on image sequences and the population of complex cells in the primary visual cortex (V1). The functions show many properties found also experimentally in complex cells, such as direction selectivity, non-orthogonal inhibition, end-inhibition, and side-inhibition. Our results demonstrate that a single unsupervised learning principle can account for such a rich repertoire of receptive field properties.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Dropbox/Journal Club/Berkes et Wiskott - 2005 - Slow feature analysis yields a rich repertoire of .pdf}
}

@article{bernsLargescaleRNAiScreen2004,
  title        = {A Large-Scale {{RNAi}} Screen in Human Cells Identifies New Components of the P53 Pathway},
  author       = {Berns, Katrien and Hijmans, E. Marielle and Mullenders, Jasper and Brummelkamp, Thijn R. and Velds, Arno and Heimerikx, Mike and Kerkhoven, Ron M. and Madiredjo, Mandy and Nijkamp, Wouter and Weigelt, Britta and Agami, Reuven and Ge, Wei and Cavet, Guy and Linsley, Peter S. and Beijersbergen, Roderick L. and Bernards, René},
  date         = {2004-03-25},
  journaltitle = {Nature},
  volume       = {428},
  number       = {6981},
  pages        = {431--437},
  issn         = {0028-0836, 1476-4679},
  doi          = {10.1038/nature02371},
  url          = {http://www.nature.com/doifinder/10.1038/nature02371},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{bertranActiveLearningCortical2018,
  title        = {Active Learning of Cortical Connectivity from Two-Photon Imaging Data},
  author       = {Bertrán, Martín A. and Martínez, Natalia L. and Wang, Ye and Dunson, David and Sapiro, Guillermo and Ringach, Dario},
  editor       = {Wennekers, Thomas},
  date         = {2018-05-02},
  journaltitle = {PLOS ONE},
  volume       = {13},
  number       = {5},
  pages        = {e0196527},
  issn         = {1932-6203},
  doi          = {10/gdhq55},
  url          = {http://dx.plos.org/10.1371/journal.pone.0196527},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{bettsSystematicIdentificationPhosphorylationmediated2017,
  title        = {Systematic Identification of Phosphorylation-Mediated Protein Interaction Switches},
  author       = {Betts, Matthew J. and Wichmann, Oliver and Utz, Mathias and Andre, Timon and Petsalaki, Evangelia and Minguez, Pablo and Parca, Luca and Roth, Frederick P. and Gavin, Anne-Claude and Bork, Peer and Russell, Robert B.},
  editor       = {Iakoucheva, Lilia M.},
  date         = {2017-03-27},
  journaltitle = {PLOS Computational Biology},
  volume       = {13},
  number       = {3},
  pages        = {e1005462},
  issn         = {1553-7358},
  doi          = {10/f9vvn4},
  url          = {http://dx.plos.org/10.1371/journal.pcbi.1005462},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{biancalaniDeepLearningAlignment2021,
  title     = {Deep Learning and Alignment of Spatially Resolved Single-Cell Transcriptomes with {{Tangram}}},
  author    = {Biancalani, Tommaso and Scalia, Gabriele and Buffoni, Lorenzo and Avasthi, Raghav and Lu, Ziqing and Sanger, Aman and Tokcan, Neriman and Vanderburg, Charles R. and Segerstolpe, {\AA}sa and Zhang, Meng and {Avraham-Davidi}, Inbal and Vickovic, Sanja and Nitzan, Mor and Ma, Sai and Subramanian, Ayshwarya and Lipinski, Michal and Buenrostro, Jason and Brown, Nik Bear and Fanelli, Duccio and Zhuang, Xiaowei and Macosko, Evan Z. and Regev, Aviv},
  year      = 2021,
  month     = nov,
  journal   = {Nature Methods},
  volume    = {18},
  number    = {11},
  pages     = {1352--1362},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-021-01264-7},
  urldate   = {2025-12-03},
  abstract  = {Charting an organs' biological atlas requires us to spatially resolve the entire single-cell transcriptome, and to relate such cellular features to the anatomical scale. Single-cell and single-nucleus RNA-seq (sc/snRNA-seq) can profile cells comprehensively, but lose spatial information. Spatial transcriptomics allows for spatial measurements, but at lower resolution and with limited sensitivity. Targeted in situ technologies solve both issues, but are limited in gene throughput. To overcome these limitations we present Tangram, a method that aligns sc/snRNA-seq data to various forms of spatial data collected from the same region, including MERFISH, STARmap, smFISH, Spatial Transcriptomics (Visium) and histological images. Tangram can map any type of sc/snRNA-seq data, including multimodal data such as those from SHARE-seq, which we used to reveal spatial patterns of chromatin accessibility. We demonstrate Tangram on healthy mouse brain tissue, by reconstructing a genome-wide anatomically integrated spatial map at single-cell resolution of the visual and somatomotor areas.},
  copyright = {2021 The Author(s)},
  langid    = {english},
  keywords  = {Imaging,Machine learning,Neuroscience,Software,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/27T9RMB8/Biancalani et al. - 2021 - Deep learning and alignment of spatially resolved single-cell transcriptomes with Tangram.pdf}
}

@inproceedings{bibalAttentionExplanationIntroduction2022,
  title      = {Is {{Attention Explanation}}? {{An Introduction}} to the {{Debate}}},
  shorttitle = {Is {{Attention Explanation}}?},
  booktitle  = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author     = {Bibal, Adrien and Cardon, R{\'e}mi and Alfter, David and Wilkens, Rodrigo and Wang, Xiaoou and Fran{\c c}ois, Thomas and Watrin, Patrick},
  editor     = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year       = {2022},
  month      = may,
  pages      = {3889--3900},
  publisher  = {Association for Computational Linguistics},
  address    = {Dublin, Ireland},
  doi        = {10.18653/v1/2022.acl-long.269},
  urldate    = {2024-07-15},
  abstract   = {The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.},
  file       = {/Users/jkobject/Zotero/storage/JJLY6KIB/Bibal et al. - 2022 - Is Attention Explanation An Introduction to the D.pdf}
}

@book{blochHamiltonianGradientFlows1995,
  title     = {Hamiltonian and {{Gradient Flows}}, {{Algorithms}} and {{Control}}},
  editor    = {Bloch, Anthony},
  date      = {1995-05-02},
  publisher = {American Mathematical Society},
  location  = {Providence, Rhode Island},
  doi       = {10.1090/fic/003},
  url       = {http://www.ams.org/fic/003},
  urldate   = {2018-04-11},
  abstract  = {The techniques and analysis presented in this paper provide new methods to solve optimization problems posed on Riemannian manifolds. A new point of view is offered for the solution of constrained optimization problems. Some classical optimization techniques on Euclidean space are generalized to Riemannian manifolds. Several algorithms are presented and their convergence properties are analyzed employing the Riemannian structure of the manifold. Specifically, two apparently new algorithms, which can be thought of as Newton’s method and the conjugate gradient method on Riemannian manifolds, are presented and shown to possess, respectively, quadratic and superlinear convergence. Examples of each method on certain Riemannian manifolds are given with the results of numerical experiments. Rayleigh’s quotient defined on the sphere is one example. It is shown that Newton’s method applied to this function converges cubically, and that the Rayleigh quotient iteration is an efficient approximation of Newton’s method. The Riemannian version of the conjugate gradient method applied to this function gives a new algorithm for finding the eigenvectors corresponding to the extreme eigenvalues of a symmetric matrix. Another example arises from extremizing the function tr ΘTQΘN on the special orthogonal group. In a similar example, it is shown that Newton’s method applied to the sum of the squares of the off-diagonal entries of a symmetric matrix converges cubically.},
  isbn      = {978-0-8218-0255-7 978-1-4704-2971-3},
  langid    = {english},
  file      = {/Users/jeremie/Documents/science/ML/Bloch - 1995 - Hamiltonian and Gradient Flows, Algorithms and Con.pdf}
}

@article{bockAhReceptorVitamin2024,
  title   = {Ah receptor, vitamin B12 and itaconate: how localized decrease of vitamin B12 prevents survival of macrophage-ingested bacteria},
  author  = {Bock, Karl Walter},
  journal = {Frontiers in Toxicology},
  volume  = {6},
  pages   = {1491184},
  year    = {2024},
  doi     = {10.3389/ftox.2024.1491184}
}

@article{boehmSystematicFunctionalCharacterization2011,
  title        = {Towards Systematic Functional Characterization of Cancer Genomes},
  author       = {Boehm, Jesse S. and Hahn, William C.},
  date         = {2011-07},
  journaltitle = {Nature Reviews Genetics},
  volume       = {12},
  number       = {7},
  pages        = {487--498},
  issn         = {1471-0056, 1471-0064},
  doi          = {10.1038/nrg3013},
  url          = {http://www.nature.com/articles/nrg3013},
  urldate      = {2019-03-22},
  abstract     = {Whole-genome approaches to identify genetic and epigenetic alterations in cancer genomes have begun to provide new insights into the range of molecular events that occurs in human tumours. Although in some cases this knowledge immediately illuminates a path towards diagnostic or therapeutic implementation, the bewildering lists of mutations in each tumour make it clear that systematic functional approaches are also necessary to obtain a comprehensive molecular understanding of cancer. Here we review the current range of methods, assays and approaches for genome-scale interrogation of gene function in cancer. We also discuss the integration of functionalgenomics approaches with the outputs from cancer genome sequencing efforts.},
  langid       = {english}
}

@article{boevaAnalysisGenomicSequence2016,
  title        = {Analysis of {{Genomic Sequence Motifs}} for {{Deciphering Transcription Factor Binding}} and {{Transcriptional Regulation}} in {{Eukaryotic Cells}}},
  author       = {Boeva, Valentina},
  date         = {2016-02-23},
  journaltitle = {Frontiers in Genetics},
  volume       = {7},
  issn         = {1664-8021},
  doi          = {10/gfxbds},
  url          = {http://journal.frontiersin.org/Article/10.3389/fgene.2016.00024/abstract},
  urldate      = {2018-04-11},
  abstract     = {Eukaryotic genomes contain a variety of structured patterns: repetitive elements, binding sites of DNA and RNA associated proteins, splice sites, and so on. Often, these structured patterns can be formalized as motifs and described using a proper mathematical model such as position weight matrix and IUPAC consensus. Two key tasks are typically carried out for motifs in the context of the analysis of genomic sequences. These are: identification in a set of DNA regions of over-represented motifs from a particular motif database, and de novo discovery of over-represented motifs. Here we describe existing methodology to perform these two tasks for motifs characterizing transcription factor binding. When applied to the output of ChIP-seq and ChIP-exo experiments, or to promoter regions of co-modulated genes, motif analysis techniques allow for the prediction of transcription factor binding events and enable identification of transcriptional regulators and co-regulators. The usefulness of motif analysis is further exemplified in this review by how motif discovery improves peak calling in ChIP-seq and ChIP-exo experiments and, when coupled with information on gene expression, allows insights into physical mechanisms of transcriptional modulation.},
  langid       = {english},
  annotation   = {00000}
}


@article{boevaHeterogeneityNeuroblastomaCell2017,
  title        = {Heterogeneity of Neuroblastoma Cell Identity Defined by Transcriptional Circuitries},
  author       = {Boeva, Valentina and Louis-Brennetot, Caroline and Peltier, Agathe and Durand, Simon and Pierre-Eugène, Cécile and Raynal, Virginie and Etchevers, Heather C and Thomas, Sophie and Lermine, Alban and Daudigeos-Dubus, Estelle and Geoerger, Birgit and Orth, Martin F and Grünewald, Thomas G P and Diaz, Elise and Ducos, Bertrand and Surdez, Didier and Carcaboso, Angel M and Medvedeva, Irina and Deller, Thomas and Combaret, Valérie and Lapouble, Eve and Pierron, Gaelle and Grossetête-Lalami, Sandrine and Baulande, Sylvain and Schleiermacher, Gudrun and Barillot, Emmanuel and Rohrer, Hermann and Delattre, Olivier and Janoueix-Lerosey, Isabelle},
  date         = {2017-09},
  journaltitle = {Nature Genetics},
  volume       = {49},
  number       = {9},
  pages        = {1408--1413},
  issn         = {1061-4036, 1546-1718},
  doi          = {10.1038/ng.3921},
  url          = {http://www.nature.com/articles/ng.3921},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@misc{boiarskyDeepDiveSingleCell2023,
  title    = {A {{Deep Dive}} into {{Single-Cell RNA Sequencing Foundation Models}}},
  author   = {Boiarsky, Rebecca and Singh, Nalini and Buendia, Alejandro and Getz, Gad and Sontag, David},
  year     = {2023},
  month    = oct,
  doi      = {10.1101/2023.10.19.563100},
  urldate  = {2024-04-19},
  abstract = {Large-scale foundation models, which are pre-trained on massive, unlabeled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology. In this paper, we explore two foundation models recently developed for single-cell RNA sequencing data, scBERT and scGPT. Focusing on the fine-tuning task of cell type annotation, we explore the relative performance of pre-trained models compared to a simple baseline, L1-regularized logistic regression, including in the few-shot setting. We perform ablation studies to understand whether pretraining improves model performance and to better understand the difficulty of the pre-training task in scBERT. Finally, using scBERT as an example, we demonstrate the potential sensitivity of fine-tuning to hyperparameter settings and parameter initializations. Taken together, our results highlight the importance of rigorously testing foundation models against well established baselines, establishing challenging fine-tuning tasks on which to benchmark foundation models, and performing deep introspection into the embeddings learned by the model in order to more effectively harness these models to transform single-cell data analysis. Code is available at https://github.com/clinicalml/sc-foundation-eval.},
  langid   = {english},
  file     = {/Users/jkobject/Zotero/storage/UPL52ZMV/Boiarsky et al. - 2023 - A Deep Dive into Single-Cell RNA Sequencing Founda.pdf}
}

@article{boijaTranscriptionFactorsActivate2018,
  title    = {Transcription {{Factors Activate Genes}} through the {{Phase-Separation Capacity}} of {{Their Activation Domains}}},
  author   = {Boija, Ann and Klein, Isaac A. and Sabari, Benjamin R. and Dall'Agnese, Alessandra and Coffey, Eliot L. and Zamudio, Alicia V. and Li, Charles H. and Shrinivas, Krishna and Manteiga, John C. and Hannett, Nancy M. and Abraham, Brian J. and Afeyan, Lena K. and Guo, Yang E. and Rimel, Jenna K. and Fant, Charli B. and Schuijers, Jurian and Lee, Tong Ihn and Taatjes, Dylan J. and Young, Richard A.},
  year     = {2018},
  month    = dec,
  journal  = {Cell},
  volume   = {175},
  number   = {7},
  pages    = {1842-1855.e16},
  issn     = {0092-8674},
  doi      = {10.1016/j.cell.2018.10.042},
  urldate  = {2024-04-19},
  abstract = {Gene expression is controlled by transcription factors (TFs) that consist of DNA-binding domains (DBDs) and activation domains (ADs). The DBDs have been well characterized, but little is known about the mechanisms by which ADs effect gene activation. Here, we report that diverse ADs form phase-separated condensates with the Mediator coactivator. For the OCT4 and GCN4 TFs, we show that the ability to form phase-separated droplets with Mediator in~vitro and the ability to activate genes~in~vivo are dependent on the same amino acid residues. For the estrogen receptor (ER), a ligand-dependent activator, we show that estrogen enhances phase separation with Mediator, again linking phase separation with gene activation. These results suggest that diverse TFs can interact with Mediator through the phase-separating capacity of their ADs and that formation of condensates with Mediator is involved in gene activation.},
  keywords = {activation domain,gene activation,mediator,phase separation,transcription,transcription factor},
  file     = {/Users/jkobject/Zotero/storage/8YLVRQMM/Boija et al. - 2018 - Transcription Factors Activate Genes through the P.pdf}
}

@article{bombieriProblemsMillenniumRiemann,
  title        = {Problems of the {{Millennium}}: The {{Riemann Hypothesis}}},
  author       = {Bombieri, E},
  journaltitle = {THE RIEMANN HYPOTHESIS},
  pages        = {11},
  langid       = {english},
  keywords     = {⛔ No DOI found},
  annotation   = {00000}
}

@misc{bommasaniOpportunitiesRisksFoundation2022,
  title         = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author        = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year          = 2022,
  month         = jul,
  number        = {arXiv:2108.07258},
  eprint        = {2108.07258},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2108.07258},
  urldate       = {2025-12-02},
  abstract      = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/U4VSARHK/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Models.pdf;/Users/jkobject/Zotero/storage/M3PM792M/2108.html}
}

@article{bonacichTechniqueAnalyzingOverlapping1972,
  title      = {Technique for {{Analyzing Overlapping Memberships}}},
  author     = {Bonacich, Phillip},
  year       = {1972},
  journal    = {Sociological Methodology},
  volume     = {4},
  eprint     = {270732},
  eprinttype = {jstor},
  pages      = {176--185},
  publisher  = {[American Sociological Association, Wiley, Sage Publications, Inc.]},
  issn       = {0081-1750},
  doi        = {10.2307/270732},
  urldate    = {2024-07-10},
  file       = {/Users/jkobject/Zotero/storage/D93LKIEC/Bonacich - 1972 - Technique for Analyzing Overlapping Memberships.pdf}
}

@article{boyeauDeepGenerativeModeling2025,
  title     = {Deep Generative Modeling of Sample-Level Heterogeneity in Single-Cell Genomics},
  author    = {Boyeau, Pierre and Hong, Justin and Gayoso, Adam and Kim, Martin and {McFaline-Figueroa}, Jos{\'e} L. and Jordan, Michael I. and Azizi, Elham and Ergen, Can and Yosef, Nir},
  year      = 2025,
  month     = nov,
  journal   = {Nature Methods},
  volume    = {22},
  number    = {11},
  pages     = {2264--2274},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-025-02808-x},
  urldate   = {2025-12-03},
  abstract  = {Single-cell genomic studies were recently conducted on hundred of samples exhibiting complex designs. These data have tremendous potential for discovering how sample- or tissue-level phenotypes relate to cellular and molecular composition. However, current analyses are often based on simplified representations of these data by averaging information across cells. We present multi-resolution variational inference (MrVI), a deep generative model designed to realize the potential of cohort studies at the single-cell level. MrVI tackles two fundamental, intertwined problems: stratifying samples into groups and evaluating the cellular and molecular differences between groups, without requiring predefined cell states. Leveraging its single-cell perspective, MrVI detects clinically relevant stratifications of cohorts of people with COVID-19 or inflammatory bowel disease that are manifested in only certain cellular subsets, enabling new discoveries that would otherwise be overlooked. MrVI can de novo identify groups of small molecules with similar biochemical properties and evaluate their effects on cellular composition and gene expression in large-scale perturbation studies. MrVI is an open-source tool at scvi-tools.org.},
  copyright = {2025 The Author(s)},
  langid    = {english},
  keywords  = {Machine learning,Software,Statistical methods,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/QMPAGT2E/Boyeau et al. - 2025 - Deep generative modeling of sample-level heterogeneity in single-cell genomics.pdf}
}

@article{boyeauDeepGenerativeModeling2025a,
  title     = {Deep Generative Modeling of Sample-Level Heterogeneity in Single-Cell Genomics},
  author    = {Boyeau, Pierre and Hong, Justin and Gayoso, Adam and Kim, Martin and {McFaline-Figueroa}, Jos{\'e} L. and Jordan, Michael I. and Azizi, Elham and Ergen, Can and Yosef, Nir},
  year      = 2025,
  month     = nov,
  journal   = {Nature Methods},
  volume    = {22},
  number    = {11},
  pages     = {2264--2274},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-025-02808-x},
  urldate   = {2025-12-12},
  abstract  = {Single-cell genomic studies were recently conducted on hundred of samples exhibiting complex designs. These data have tremendous potential for discovering how sample- or tissue-level phenotypes relate to cellular and molecular composition. However, current analyses are often based on simplified representations of these data by averaging information across cells. We present multi-resolution variational inference (MrVI), a deep generative model designed to realize the potential of cohort studies at the single-cell level. MrVI tackles two fundamental, intertwined problems: stratifying samples into groups and evaluating the cellular and molecular differences between groups, without requiring predefined cell states. Leveraging its single-cell perspective, MrVI detects clinically relevant stratifications of cohorts of people with COVID-19 or inflammatory bowel disease that are manifested in only certain cellular subsets, enabling new discoveries that would otherwise be overlooked. MrVI can de novo identify groups of small molecules with similar biochemical properties and evaluate their effects on cellular composition and gene expression in large-scale perturbation studies. MrVI is an open-source tool at scvi-tools.org.},
  copyright = {2025 The Author(s)},
  langid    = {english},
  keywords  = {Machine learning,Software,Statistical methods,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/7QMJ3ISQ/Boyeau et al. - 2025 - Deep generative modeling of sample-level heterogeneity in single-cell genomics.pdf}
}

@article{boyerCoreTranscriptionalRegulatory2005,
  title        = {Core {{Transcriptional Regulatory Circuitry}} in {{Human Embryonic Stem Cells}}},
  author       = {Boyer, Laurie A. and Lee, Tong Ihn and Cole, Megan F. and Johnstone, Sarah E. and Levine, Stuart S. and Zucker, Jacob P. and Guenther, Matthew G. and Kumar, Roshan M. and Murray, Heather L. and Jenner, Richard G. and Gifford, David K. and Melton, Douglas A. and Jaenisch, Rudolf and Young, Richard A.},
  date         = {2005-09},
  journaltitle = {Cell},
  volume       = {122},
  number       = {6},
  pages        = {947--956},
  issn         = {00928674},
  doi          = {10.1016/j.cell.2005.08.020},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867405008251},
  urldate      = {2019-03-22},
  abstract     = {The transcription factors OCT4, SOX2, and NANOG have essential roles in early development and are required for the propagation of undifferentiated embryonic stem (ES) cells in culture. To gain insights into transcriptional regulation of human ES cells, we have identified OCT4, SOX2, and NANOG target genes using genome-scale location analysis. We found, surprisingly, that OCT4, SOX2, and NANOG co-occupy a substantial portion of their target genes. These target genes frequently encode transcription factors, many of which are developmentally important homeodomain proteins. Our data also indicate that OCT4, SOX2, and NANOG collaborate to form regulatory circuitry consisting of autoregulatory and feedforward loops. These results provide new insights into the transcriptional regulation of stem cells and reveal how OCT4, SOX2, and NANOG contribute to pluripotency and self-renewal.},
  langid       = {english},
  annotation   = {00000}
}

@article{bozaDeepNanoDeepRecurrent2017,
  title        = {{{DeepNano}}: {{Deep}} Recurrent Neural Networks for Base Calling in {{MinION}} Nanopore Reads},
  shorttitle   = {{{DeepNano}}},
  author       = {Boža, Vladimír and Brejová, Broňa and Vinař, Tomáš},
  editor       = {Zhi, Degui},
  date         = {2017-06-05},
  journaltitle = {PLOS ONE},
  volume       = {12},
  number       = {6},
  pages        = {e0178751},
  issn         = {1932-6203},
  doi          = {10.1371/journal.pone.0178751},
  url          = {https://dx.plos.org/10.1371/journal.pone.0178751},
  urldate      = {2019-03-22},
  abstract     = {The MinION device by Oxford Nanopore produces very long reads (reads over 100 kBp were reported); however it suffers from high sequencing error rate. We present an opensource DNA base caller based on deep recurrent neural networks and show that the accuracy of base calling is much dependent on the underlying software and can be improved by considering modern machine learning methods. By employing carefully crafted recurrent neural networks, our tool significantly improves base calling accuracy on data from R7.3 version of the platform compared to the default base caller supplied by the manufacturer. On R9 version, we achieve results comparable to Nanonet base caller provided by Oxford Nanopore. Availability of an open source tool with high base calling accuracy will be useful for development of new applications of the MinION device, including infectious disease detection and custom target enrichment during sequencing.},
  langid       = {english}
}

@article{brachaEngineeringBrainParasites2018,
  title        = {Engineering {{Brain Parasites}} for {{Intracellular Delivery}} of {{Therapeutic Proteins}}},
  author       = {Bracha, Shahar and Hassi, Karoliina and Ross, Paul D. and Cobb, Stuart and Sheiner, Lilach and Rechavi, Oded},
  date         = {2018-12-03},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbgb},
  url          = {http://biorxiv.org/lookup/doi/10.1101/481192},
  urldate      = {2019-03-22},
  abstract     = {Protein therapy has the potential to alleviate many neurological diseases; however, delivery mechanisms for the central nervous system (CNS) are limited, and intracellular delivery poses additional hurdles. To address these challenges, we harnessed the protist parasite Toxoplasma gondii, which can migrate into the CNS and secrete proteins into cells. Using a fusion protein approach, we engineered T. gondii to secrete therapeutic proteins for human neurological disorders. We tested two secretion systems, generated fusion proteins that localized to the secretory organelles of T. gondii and assessed their intracellular targeting in various mammalian cells including neurons. We show that T. gondii expressing GRA16 fused to the Rett syndrome protein MeCP2 deliver a fusion protein that mimics the endogenous MeCP2, binding heterochromatic DNA in neurons. This demonstrates the potential of T. gondii as a therapeutic protein vector, which could provide either transient or chronic, in situ synthesis and delivery of intracellular proteins to the CNS.},
  langid       = {english},
  annotation   = {00000}
}


@article{bravogonzalez-blasSCENICSinglecellMultiomic2023,
  title      = {{{SCENIC}}+: Single-Cell Multiomic Inference of Enhancers and Gene Regulatory Networks},
  shorttitle = {{{SCENIC}}+},
  author     = {{Bravo Gonz{\'a}lez-Blas}, Carmen and De Winter, Seppe and Hulselmans, Gert and Hecker, Nikolai and Matetovici, Irina and Christiaens, Valerie and Poovathingal, Suresh and Wouters, Jasper and Aibar, Sara and Aerts, Stein},
  year       = {2023},
  month      = sep,
  journal    = {Nature Methods},
  volume     = {20},
  number     = {9},
  pages      = {1355--1367},
  publisher  = {Nature Publishing Group},
  issn       = {1548-7105},
  doi        = {10.1038/s41592-023-01938-4},
  urldate    = {2024-07-10},
  abstract   = {Joint profiling of chromatin accessibility and gene expression in individual cells provides an opportunity to decipher enhancer-driven gene regulatory networks (GRNs). Here we present a method for the inference of enhancer-driven GRNs, called SCENIC+. SCENIC+ predicts genomic enhancers along with candidate upstream transcription factors (TFs) and links these enhancers to candidate target genes. To improve both recall and precision of TF identification, we curated and clustered a motif collection with more than 30,000 motifs. We benchmarked SCENIC+ on diverse datasets from different species, including human peripheral blood mononuclear cells, ENCODE cell lines, melanoma cell states and Drosophila retinal development. Next, we exploit SCENIC+ predictions to study conserved TFs, enhancers and GRNs between human and mouse cell types in the cerebral cortex. Finally, we use SCENIC+ to study the dynamics of gene regulation along differentiation trajectories and the effect of TF perturbations on cell state. SCENIC+ is available at scenicplus.readthedocs.io.},
  copyright  = {2023 The Author(s)},
  langid     = {english},
  keywords   = {Epigenomics,Gene regulation,Gene regulatory networks,Software},
  file       = {/Users/jkobject/Zotero/storage/BFKZ48J5/Bravo González-Blas et al. - 2023 - SCENIC+ single-cell multiomic inference of enhanc.pdf}
}


@article{brayCellPaintingHighcontent2016,
  title     = {Cell {{Painting}}, a High-Content Image-Based Assay for Morphological Profiling Using Multiplexed Fluorescent Dyes},
  author    = {Bray, Mark-Anthony and Singh, Shantanu and Han, Han and Davis, Chadwick T. and Borgeson, Blake and Hartland, Cathy and {Kost-Alimova}, Maria and Gustafsdottir, Sigrun M. and Gibson, Christopher C. and Carpenter, Anne E.},
  year      = {2016},
  month     = sep,
  journal   = {Nature Protocols},
  volume    = {11},
  number    = {9},
  pages     = {1757--1774},
  publisher = {Nature Publishing Group},
  issn      = {1750-2799},
  doi       = {10.1038/nprot.2016.105},
  urldate   = {2025-02-25},
  abstract  = {Cell Painting is a high-content screening assay that uses multiplexed fluorescent dyes for image-based profiling of {$\sim$}1,500 morphological features. Image analysis with CellProfiler automatically identifies and extracts data from individual cells.},
  copyright = {2016 Springer Nature Limited},
  langid    = {english},
  keywords  = {Fluorescence imaging,High-throughput screening,Image processing,Phenotypic screening},
  file      = {/Users/jkobject/Zotero/storage/YQEWTVQ4/Bray et al. - 2016 - Cell Painting, a high-content image-based assay fo.pdf}
}

@misc{brixiGenomeModelingDesign2025,
  title         = {Genome Modeling and Design across All Domains of Life with {{Evo}} 2},
  author        = {Brixi, Garyk and Durrant, Matthew G. and Ku, Jerome and Poli, Michael and Brockman, Greg and Chang, Daniel and Gonzalez, Gabriel A. and King, Samuel H. and Li, David B. and Merchant, Aditi T. and Naghipourfar, Mohsen and Nguyen, Eric and {Ricci-Tam}, Chiara and Romero, David W. and Sun, Gwanggyu and Taghibakshi, Ali and Vorontsov, Anton and Yang, Brandon and Deng, Myra and Gorton, Liv and Nguyen, Nam and Wang, Nicholas K. and Adams, Etowah and Baccus, Stephen A. and Dillmann, Steven and Ermon, Stefano and Guo, Daniel and Ilango, Rajesh and Janik, Ken and Lu, Amy X. and Mehta, Reshma and Mofrad, Mohammad R. K. and Ng, Madelena Y. and Pannu, Jaspreet and R{\'e}, Christopher and Schmok, Jonathan C. and John, John St and Sullivan, Jeremy and Zhu, Kevin and Zynda, Greg and Balsam, Daniel and Collison, Patrick and Costa, Anthony B. and {Hernandez-Boussard}, Tina and Ho, Eric and Liu, Ming-Yu and McGrath, Thomas and Powell, Kimberly and Burke, Dave P. and Goodarzi, Hani and Hsu, Patrick D. and Hie, Brian L.},
  year          = {2025},
  month         = feb,
  primaryclass  = {New Results},
  pages         = {2025.02.18.638918},
  publisher     = {bioRxiv},
  doi           = {10.1101/2025.02.18.638918},
  urldate       = {2025-03-09},
  abstract      = {All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation---from noncoding pathogenic mutations to clinically significant BRCA1 variants---without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon--intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/GGPNPWRB/Brixi et al. - 2025 - Genome modeling and design across all domains of l.pdf}
}

@online{brixiGenomeModelingDesign2025a,
  title       = {Genome Modeling and Design across All Domains of Life with {{Evo}} 2},
  author      = {Brixi, Garyk and Durrant, Matthew G. and Ku, Jerome and Poli, Michael and Brockman, Greg and Chang, Daniel and Gonzalez, Gabriel A. and King, Samuel H. and Li, David B. and Merchant, Aditi T. and Naghipourfar, Mohsen and Nguyen, Eric and Ricci-Tam, Chiara and Romero, David W. and Sun, Gwanggyu and Taghibakshi, Ali and Vorontsov, Anton and Yang, Brandon and Deng, Myra and Gorton, Liv and Nguyen, Nam and Wang, Nicholas K. and Adams, Etowah and Baccus, Stephen A. and Dillmann, Steven and Ermon, Stefano and Guo, Daniel and Ilango, Rajesh and Janik, Ken and Lu, Amy X. and Mehta, Reshma and Mofrad, Mohammad R. K. and Ng, Madelena Y. and Pannu, Jaspreet and Ré, Christopher and Schmok, Jonathan C. and John, John St and Sullivan, Jeremy and Zhu, Kevin and Zynda, Greg and Balsam, Daniel and Collison, Patrick and Costa, Anthony B. and Hernandez-Boussard, Tina and Ho, Eric and Liu, Ming-Yu and McGrath, Thomas and Powell, Kimberly and Burke, Dave P. and Goodarzi, Hani and Hsu, Patrick D. and Hie, Brian L.},
  date        = {2025-02-21},
  eprinttype  = {bioRxiv},
  eprintclass = {New Results},
  pages       = {2025.02.18.638918},
  doi         = {10.1101/2025.02.18.638918},
  url         = {https://www.biorxiv.org/content/10.1101/2025.02.18.638918v1},
  urldate     = {2025-03-26},
  abstract    = {All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.},
  langid      = {english},
  pubstate    = {prepublished},
  file        = {/Users/jkobject/Zotero/storage/B9ZJN5TK/Brixi et al. - 2025 - Genome modeling and design across all domains of l.pdf}
}

@article{broughFunctionalViabilityProfiles2011,
  title        = {Functional {{Viability Profiles}} of {{Breast Cancer}}},
  author       = {Brough, R. and Frankum, J. R. and Sims, D. and Mackay, A. and Mendes-Pereira, A. M. and Bajrami, I. and Costa-Cabral, S. and Rafiq, R. and Ahmad, A. S. and Cerone, M. A. and Natrajan, R. and Sharpe, R. and Shiu, K.-K. and Wetterskog, D. and Dedes, K. J. and Lambros, M. B. and Rawjee, T. and Linardopoulos, S. and Reis-Filho, J. S. and Turner, N. C. and Lord, C. J. and Ashworth, A.},
  date         = {2011-08-01},
  journaltitle = {Cancer Discovery},
  volume       = {1},
  number       = {3},
  pages        = {260--273},
  issn         = {2159-8274, 2159-8290},
  doi          = {10/dn87cq},
  url          = {http://cancerdiscovery.aacrjournals.org/cgi/doi/10.1158/2159-8290.CD-11-0107},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@misc{brownLanguageModelsAre2020,
  title         = {Language {{Models}} Are {{Few-Shot Learners}}},
  author        = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year          = 2020,
  month         = jul,
  number        = {arXiv:2005.14165},
  eprint        = {2005.14165},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2005.14165},
  urldate       = {2025-12-02},
  abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/B6DPMDTM/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/jkobject/Zotero/storage/RSEQJCIB/2005.html}
}

@article{brunelDynamicsSparselyConnected,
  title      = {Dynamics of {{Sparsely Connected Networks}} of {{Excitatory}} and {{Inhibitory Spiking Neurons}}},
  author     = {family=BRUNEL, given=NICOLAS, given-i=NICOLAS},
  pages      = {26},
  abstract   = {The dynamics of networks of sparsely connected excitatory and inhibitory integrate-and-fire neurons are studied analytically. The analysis reveals a rich repertoire of states, including synchronous states in which neurons fire regularly; asynchronous states with stationary global activity and very irregular individual cell activity; and states in which the global activity oscillates but individual cells fire irregularly, typically at rates lower than the global oscillation frequency. The network can switch between these states, provided the external frequency, or the balance between excitation and inhibition, is varied. Two types of network oscillations are observed. In the fast oscillatory state, the network frequency is almost fully controlled by the synaptic time scale. In the slow oscillatory state, the network frequency depends mostly on the membrane time constant. Finite size effects in the asynchronous state are also discussed.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Dropbox/Journal Club/BRUNEL - Dynamics of Sparsely Connected Networks of Excitat.pdf}
}

@article{buenrostroATACseqMethodAssaying2015,
  title      = {{{ATAC-seq}}: {{A Method}} for {{Assaying Chromatin Accessibility Genome-Wide}}},
  shorttitle = {{{ATAC-seq}}},
  author     = {Buenrostro, Jason and Wu, Beijing and Chang, Howard and Greenleaf, William},
  year       = 2015,
  month      = jan,
  journal    = {Current protocols in molecular biology / edited by Frederick M. Ausubel ... [et al.]},
  volume     = {109},
  pages      = {21.29.1-21.29.9},
  issn       = {1934-3639},
  doi        = {10.1002/0471142727.mb2129s109},
  urldate    = {2025-12-02},
  abstract   = {This unit describes Assay for Transposase Accessible Chromatin with high-throughput sequencing (ATAC-seq), a method for mapping chromatin accessibility genome-wide. This method probes DNA accessibility with hyperactive Tn5 transposase, which inserts sequencing adapters into accessible regions of chromatin. Sequencing reads can then be used to infer regions of increased accessibility, as well as to map regions of transcription factor binding and nucleosome position. The method is a fast and sensitive alternative to DNase-Seq for assaying chromatin accessibility genome-wide, or to MNase for assaying nucleosome positions in accessible regions of the genome.},
  pmcid      = {PMC4374986},
  pmid       = {25559105},
  file       = {/Users/jkobject/Zotero/storage/LKGYFGFC/Buenrostro et al. - 2015 - ATAC-seq A Method for Assaying Chromatin Accessibility Genome-Wide.pdf}
}

@article{bunneHowBuildVirtual2024,
  title      = {How to Build the Virtual Cell with Artificial Intelligence: {{Priorities}} and Opportunities},
  shorttitle = {How to Build the Virtual Cell with Artificial Intelligence},
  author     = {Bunne, Charlotte and Roohani, Yusuf and Rosen, Yanay and Gupta, Ankit and Zhang, Xikun and Roed, Marcel and Alexandrov, Theo and AlQuraishi, Mohammed and Brennan, Patricia and Burkhardt, Daniel B. and Califano, Andrea and Cool, Jonah and Dernburg, Abby F. and Ewing, Kirsty and Fox, Emily B. and Haury, Matthias and Herr, Amy E. and Horvitz, Eric and Hsu, Patrick D. and Jain, Viren and Johnson, Gregory R. and Kalil, Thomas and Kelley, David R. and Kelley, Shana O. and Kreshuk, Anna and Mitchison, Tim and Otte, Stephani and Shendure, Jay and Sofroniew, Nicholas J. and Theis, Fabian and Theodoris, Christina V. and Upadhyayula, Srigokul and Valer, Marc and Wang, Bo and Xing, Eric and {Yeung-Levy}, Serena and Zitnik, Marinka and Karaletsos, Theofanis and Regev, Aviv and Lundberg, Emma and Leskovec, Jure and Quake, Stephen R.},
  year       = {2024},
  month      = dec,
  journal    = {Cell},
  volume     = {187},
  number     = {25},
  pages      = {7045--7063},
  publisher  = {Elsevier},
  issn       = {0092-8674, 1097-4172},
  doi        = {10.1016/j.cell.2024.11.015},
  urldate    = {2025-02-25},
  langid     = {english},
  pmid       = {39672099},
  keywords   = {AI,cell biology,ML,virtual cell},
  file       = {/Users/jkobject/Zotero/storage/LL9R7LRU/Bunne et al. - 2024 - How to build the virtual cell with artificial inte.pdf}
}

@article{burclaffProximaltoDistalSurveyHealthy2022,
  title    = {A {{Proximal-to-Distal Survey}} of {{Healthy Adult Human Small Intestine}} and {{Colon Epithelium}} by {{Single-Cell Transcriptomics}}},
  author   = {Burclaff, Joseph and Bliton, R. Jarrett and Breau, Keith A. and Ok, Meryem T. and {Gomez-Martinez}, Ismael and Ranek, Jolene S. and Bhatt, Aadra P. and Purvis, Jeremy E. and Woosley, John T. and Magness, Scott T.},
  year     = {2022},
  journal  = {Cellular and Molecular Gastroenterology and Hepatology},
  volume   = {13},
  number   = {5},
  pages    = {1554--1589},
  issn     = {2352-345X},
  doi      = {10.1016/j.jcmgh.2022.02.007},
  abstract = {BACKGROUND \& AIMS: Single-cell transcriptomics offer unprecedented resolution of tissue function at the cellular level, yet studies analyzing healthy adult human small intestine and colon are sparse. Here, we present single-cell transcriptomics covering the duodenum, jejunum, ileum, and ascending, transverse, and descending colon from 3 human beings. METHODS: A total of 12,590 single epithelial cells from 3 independently processed organ donors were evaluated for organ-specific lineage biomarkers, differentially regulated genes, receptors, and drug targets. Analyses focused on intrinsic cell properties and their capacity for response to extrinsic signals along the gut axis across different human beings. RESULTS: Cells were assigned to 25 epithelial lineage clusters. Multiple accepted intestinal stem cell markers do not specifically mark all human intestinal stem cells. Lysozyme expression is not unique to human Paneth cells, and Paneth cells lack expression of expected niche factors. Bestrophin 4 (BEST4)+ cells express Neuropeptide Y (NPY) and show maturational differences between the small intestine and colon. Tuft cells possess a broad ability to interact with the innate and adaptive immune systems through previously unreported receptors. Some classes of mucins, hormones, cell junctions, and nutrient absorption genes show unappreciated regional expression differences across lineages. The differential expression of receptors and drug targets across lineages show biological variation and the potential for variegated responses. CONCLUSIONS: Our study identifies novel lineage marker genes, covers regional differences, shows important differences between mouse and human gut epithelium, and reveals insight into how the epithelium responds to the environment and drugs. This comprehensive cell atlas of the healthy adult human intestinal epithelium resolves likely functional differences across anatomic regions along the gastrointestinal tract and advances our understanding of human intestinal physiology.},
  langid   = {english},
  pmcid    = {PMC9043569},
  pmid     = {35176508},
  keywords = {Animals,BEST4,Cell Atlas,Colon,Epithelium,Humans,Intestinal Mucosa,Intestinal Stem Cell,Intestine Small,Mice,Paneth Cell,scRNAseq,Transcriptome},
  file     = {/Users/jkobject/Zotero/storage/L53H7LFG/Burclaff et al. - 2022 - A Proximal-to-Distal Survey of Healthy Adult Human.pdf}
}

@article{buttnerTestMetricAssessing2019,
  title     = {A Test Metric for Assessing Single-Cell {{RNA-seq}} Batch Correction},
  author    = {B{\"u}ttner, Maren and Miao, Zhichao and Wolf, F. Alexander and Teichmann, Sarah A. and Theis, Fabian J.},
  year      = {2019},
  month     = jan,
  journal   = {Nature Methods},
  volume    = {16},
  number    = {1},
  pages     = {43--49},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-018-0254-1},
  urldate   = {2025-02-25},
  abstract  = {Single-cell transcriptomics is a versatile tool for exploring heterogeneous cell populations, but as with all genomics experiments, batch effects can hamper data integration and interpretation. The success of batch-effect correction is often evaluated by visual inspection of low-dimensional embeddings, which are inherently imprecise. Here we present a user-friendly, robust and sensitive k-nearest-neighbor batch-effect test (kBET; https://github.com/theislab/kBET) for quantification of batch effects. We used kBET to assess commonly used batch-regression and normalization approaches, and to quantify the extent to which they remove batch effects while preserving biological variability. We also demonstrate the application of kBET to data from peripheral blood mononuclear cells (PBMCs) from healthy donors to distinguish cell-type-specific inter-individual variability from changes in relative proportions of cell populations. This has important implications for future data-integration efforts, central to projects such as the Human Cell Atlas.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Data integration,Gene expression,Software},
  file      = {/Users/jkobject/Zotero/storage/6R7DF6W9/Büttner et al. - 2019 - A test metric for assessing single-cell RNA-seq ba.pdf}
}

@article{campbellLargeScaleProfilingKinase2016,
  title        = {Large-{{Scale Profiling}} of {{Kinase Dependencies}} in {{Cancer Cell Lines}}},
  author       = {Campbell, James and Ryan, Colm~J. and Brough, Rachel and Bajrami, Ilirjana and Pemberton, Helen~N. and Chong, Irene~Y. and Costa-Cabral, Sara and Frankum, Jessica and Gulati, Aditi and Holme, Harriet and Miller, Rowan and Postel-Vinay, Sophie and Rafiq, Rumana and Wei, Wenbin and Williamson, Chris~T. and Quigley, David~A. and Tym, Joe and Al-Lazikani, Bissan and Fenton, Timothy and Natrajan, Rachael and Strauss, Sandra~J. and Ashworth, Alan and Lord, Christopher~J.},
  date         = {2016-03},
  journaltitle = {Cell Reports},
  volume       = {14},
  number       = {10},
  pages        = {2490--2501},
  issn         = {22111247},
  doi          = {10/f8wbq8},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S2211124716301267},
  urldate      = {2019-03-22},
  abstract     = {One approach to identifying cancer-specific vulnerabilities and therapeutic targets is to profile genetic dependencies in cancer cell lines. Here, we describe data from a series of siRNA screens that identify the kinase genetic dependencies in 117 cancer cell lines from ten cancer types. By integrating the siRNA screen data with molecular profiling data, including exome sequencing data, we show how vulnerabilities/genetic dependencies that are associated with mutations in specific cancer driver genes can be identified. By integrating additional data sets into this analysis, including protein-protein interaction data, we also demonstrate that the genetic dependencies associated with many cancer driver genes form dense connections on functional interaction networks. We demonstrate the utility of this resource by using it to predict the drug sensitivity of genetically or histologically defined subsets of tumor cell lines, including an increased sensitivity of osteosarcoma cell lines to FGFR inhibitors and SMAD4 mutant tumor cells to mitotic inhibitors.},
  langid       = {english},
  annotation   = {00000}
}

@misc{CancerassociatedFibroblastsBasic,
  title        = {Cancer-Associated Fibroblasts: From Basic Science to Anticancer Therapy {\textbar} {{Experimental}} \& {{Molecular Medicine}}},
  urldate      = {2024-07-26},
  howpublished = {https://www.nature.com/articles/s12276-023-01013-0},
  file         = {/Users/jkobject/Zotero/storage/Z4F722LD/s12276-023-01013-0.html}
}

@misc{carreiraHiPHierarchicalPerceiver2022,
  title  = {HiP: Hierarchical Perceiver},
  author = {Carreira, João and others},
  year   = {2022},
  doi    = {10.48550/arXiv.2202.10890}
}


@article{cassaEstimatingSelectiveEffects2017,
  title        = {Estimating the Selective Effects of Heterozygous Protein-Truncating Variants from Human Exome Data},
  author       = {Cassa, Christopher A and Weghorn, Donate and Balick, Daniel J and Jordan, Daniel M and Nusinow, David and Samocha, Kaitlin E and O'Donnell-Luria, Anne and MacArthur, Daniel G and Daly, Mark J and Beier, David R and Sunyaev, Shamil R},
  date         = {2017-05},
  journaltitle = {Nature Genetics},
  volume       = {49},
  number       = {5},
  pages        = {806--810},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/f9x3xp},
  url          = {http://www.nature.com/articles/ng.3831},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@online{CD99CrossroadsPhysiology,
  title   = {{{CD99}} at the Crossroads of Physiology and Pathology - {{PMC}}},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5842202/},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/XCUDNVDB/PMC5842202.html}
}

@article{chalkUnifiedTheoryEfficient,
  title  = {Towards a Unified Theory of Efficient, Predictive and Sparse Coding: {{Supplementary}} Information},
  author = {Chalk, Matthew and Marre, Olivier and Tkacik, Gasper},
  pages  = {13},
  langid = {english},
  file   = {/Users/jeremie/Dropbox/Journal Club/Chalk et al. - Towards a uniﬁed theory of eﬃcient, predictive and.pdf}
}

@article{changMicroRNASignatureAnalysis2011,
  title        = {{{MicroRNA}} Signature Analysis in Colorectal Cancer: Identification of Expression Profiles in Stage {{II}} Tumors Associated with Aggressive Disease},
  shorttitle   = {{{MicroRNA}} Signature Analysis in Colorectal Cancer},
  author       = {Chang, Kah Hoong and Miller, Nicola and Kheirelseid, Elrasheid A. H. and Lemetre, Christophe and Ball, Graham R. and Smith, Myles J. and Regan, Mark and McAnena, Oliver J. and Kerin, Michael J.},
  date         = {2011-11},
  journaltitle = {International Journal of Colorectal Disease},
  volume       = {26},
  number       = {11},
  pages        = {1415--1422},
  issn         = {0179-1958, 1432-1262},
  doi          = {10/b675pp},
  url          = {http://link.springer.com/10.1007/s00384-011-1279-4},
  urldate      = {2018-04-11},
  abstract     = {Purpose Colorectal cancer (CRC) is a clinically diverse disease whose molecular etiology remains poorly understood. The purpose of this study was to identify miRNA expression patterns predictive of CRC tumor status and to investigate associations between microRNA (miRNA) expression and clinicopathological parameters.},
  langid       = {english},
  annotation   = {00000}
}

@article{chaplotACTIVENEURALLOCALIZATION,
  title      = {{{ACTIVE NEURAL LOCALIZATION}}},
  author     = {Chaplot, Devendra Singh and Parisotto, Emilio and Salakhutdinov, Ruslan},
  pages      = {15},
  abstract   = {Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose “Active Neural Localizer”, a fully differentiable neural network that learns to localize accurately and efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to localize accurately while minimizing the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model’s capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {00000}
}

@misc{cheap,
  title         = {Tokenized and {{Continuous Embedding Compressions}} of {{Protein Sequence}} and {{Structure}}},
  author        = {Lu, Amy X. and Yan, Wilson and Yang, Kevin K. and Gligorijevic, Vladimir and Cho, Kyunghyun and Abbeel, Pieter and Bonneau, Richard and Frey, Nathan},
  year          = {2024},
  month         = nov,
  primaryclass  = {New Results},
  pages         = {2024.08.06.606920},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.08.06.606920},
  urldate       = {2025-02-06},
  abstract      = {Existing protein machine learning representations typically model either the sequence or structure distribution, with the other modality implicit. The latent space of sequence-to-structure prediction models such as ESMFold represents the joint distribution of sequence and structure; however, we find these embeddings to exhibit massive activations, whereby some channels have values 3000{\texttimes} higher than others, regardless of the input. Further, on continuous compression schemes, ESMFold embeddings can be reduced by a factor of 128{\texttimes} along the channel and 8{\texttimes} along the length, while retaining structure information at {$<$}2{\AA} scale accuracy, and performing competitively on protein function and localization benchmarks. On discrete compression schemes, we construct a tokenized all-atom structure vocabulary that retains high reconstruction accuracy, thus introducing a tokenized representation of all-atom structure that can be obtained from sequence alone. We term this series of embeddings as CHEAP (Compressed Hourglass Embedding Adaptations of Proteins) embeddings, obtained via the HPCT (Hourglass Protein Compression Transformer) architecture. CHEAP is a compact representation of both protein structure and sequence, sheds light on information content asymmetries between sequence and structure, democratizes representations captured by large models, and is designed to have flexible downstream applications such as generation, search, and prediction.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/RBN53IWK/Lu et al. - 2024 - Tokenized and Continuous Embedding Compressions of.pdf}
}

@article{chechikInformationBottleneckGaussian,
  title    = {Information {{Bottleneck}} for {{Gaussian Variables}}},
  author   = {Chechik, Gal and Globerson, Amir and Tishby, Naftali and Weiss, Yair},
  pages    = {25},
  abstract = {The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another - relevance - variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difficult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Σx|yΣ−x 1, which is also the basis obtained in Canonical Correlation Analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the “information-curve”), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections.},
  langid   = {english},
  file     = {/Users/jeremie/Dropbox/Journal Club/Chechik et al. - Information Bottleneck for Gaussian Variables.pdf}
}

@article{chechikOnlineAlgorithmLarge,
  title      = {An {{Online Algorithm}} for {{Large Scale Image Similarity Learning}}},
  author     = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
  pages      = {9},
  abstract   = {Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning. It stands in the core of classification methods like kernel machines, and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, current approaches for learning similarity do not scale to large datasets, especially when imposing metric constraints on the learned similarity. We describe OASIS, a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features. Scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost. OASIS is accurate at a wide range of scales: on a standard benchmark with thousands of images, it is more precise than state-of-the-art methods, and faster by orders of magnitude. On 2.7 million images collected from the web, OASIS can be trained within 3 days on a single CPU. The nonmetric similarities learned by OASIS can be transformed into metric similarities, achieving higher precisions than similarities that are learned as metrics in the first place. This suggests an approach for learning a metric from data that is larger by orders of magnitude than was handled before.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{cheifetWhereGenomicsGoing2019,
  title        = {Where Is Genomics Going Next?},
  author       = {Cheifet, Barbara},
  date         = {2019-12},
  journaltitle = {Genome Biology},
  volume       = {20},
  number       = {1},
  issn         = {1474-760X},
  doi          = {10/gfxbfx},
  url          = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1626-2},
  urldate      = {2019-03-22},
  abstract     = {We polled the Editorial Board of Genome Biology to ask where they see genomics going in the next few years. Here are some of their responses.},
  langid       = {english},
  annotation   = {00000}
}

@article{chenCoreTranscriptionalRegulatory2020,
  title     = {Core Transcriptional Regulatory Circuitries in Cancer},
  author    = {Chen, Ye and Xu, Liang and Lin, Ruby Yu-Tong and M{\"u}schen, Markus and Koeffler, H. Phillip},
  year      = {2020},
  month     = oct,
  journal   = {Oncogene},
  volume    = {39},
  number    = {43},
  pages     = {6633--6646},
  publisher = {Nature Publishing Group},
  issn      = {1476-5594},
  doi       = {10.1038/s41388-020-01459-w},
  urldate   = {2024-04-19},
  abstract  = {Transcription factors (TFs) coordinate the on-and-off states of gene expression typically in a combinatorial fashion. Studies from embryonic stem cells and other cell types have revealed that a clique of self-regulated core TFs control cell identity and cell state. These core TFs form interconnected feed-forward transcriptional loops to establish and reinforce the cell-type-specific gene-expression program; the ensemble of core TFs and their regulatory loops constitutes core transcriptional regulatory circuitry (CRC). Here, we summarize recent progress in computational reconstitution and biologic exploration of CRCs across various human malignancies, and consolidate the strategy and methodology for CRC discovery. We also discuss the genetic basis and therapeutic vulnerability of CRC, and highlight new frontiers and future efforts for the study of CRC in cancer. Knowledge of CRC in cancer is fundamental to understanding cancer-specific transcriptional addiction, and should provide important insight to both pathobiology and therapeutics.},
  copyright = {2020 The Author(s)},
  langid    = {english},
  keywords  = {Cancer genetics,Transcription},
  file      = {/Users/jkobject/Zotero/storage/EX7HTRRU/Chen et al. - 2020 - Core transcriptional regulatory circuitries in can.pdf}
}

@article{chenCRISPRCas9ScreenReveals2017,
  title        = {{{CRISPR-Cas9}} Screen Reveals a {{MYCN-amplified}} Neuroblastoma Dependency on {{EZH2}}},
  author       = {Chen, Liying and Alexe, Gabriela and Dharia, Neekesh V. and Ross, Linda and Iniguez, Amanda Balboni and Conway, Amy Saur and Wang, Emily Jue and Veschi, Veronica and Lam, Norris and Qi, Jun and Gustafson, W. Clay and Nasholm, Nicole and Vazquez, Francisca and Weir, Barbara A. and Cowley, Glenn S. and Ali, Levi D. and Pantel, Sasha and Jiang, Guozhi and Harrington, William F. and Lee, Yenarae and Goodale, Amy and Lubonja, Rakela and Krill-Burger, John M. and Meyers, Robin M. and Tsherniak, Aviad and Root, David E. and Bradner, James E. and Golub, Todd R. and Roberts, Charles W.M. and Hahn, William C. and Weiss, William A. and Thiele, Carol J. and Stegmaier, Kimberly},
  date         = {2017-12-04},
  journaltitle = {Journal of Clinical Investigation},
  volume       = {128},
  number       = {1},
  pages        = {446--462},
  issn         = {0021-9738, 1558-8238},
  doi          = {10/gfxbf4},
  url          = {https://www.jci.org/articles/view/90793},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{chenGraphAttentionNetwork2022,
  title    = {Graph Attention Network for Link Prediction of Gene Regulations from Single-Cell {{RNA-sequencing}} Data},
  author   = {Chen, Guangyi and Liu, Zhi-Ping},
  year     = {2022},
  month    = sep,
  journal  = {Bioinformatics},
  volume   = {38},
  number   = {19},
  pages    = {4522--4529},
  issn     = {1367-4803},
  doi      = {10.1093/bioinformatics/btac559},
  urldate  = {2024-07-10},
  abstract = {Single-cell RNA sequencing (scRNA-seq) data provides unprecedented opportunities to reconstruct gene regulatory networks (GRNs) at fine-grained resolution. Numerous unsupervised or self-supervised models have been proposed to infer GRN from bulk RNA-seq data, but few of them are appropriate for scRNA-seq data under the circumstance of low signal-to-noise ratio and dropout. Fortunately, the surging of TF-DNA binding data (e.g. ChIP-seq) makes supervised GRN inference possible. We regard supervised GRN inference as a graph-based link prediction problem that expects to learn gene low-dimensional vectorized representations to predict potential regulatory interactions.In this paper, we present GENELink to infer latent interactions between transcription factors (TFs) and target genes in GRN using graph attention network. GENELink projects the single-cell gene expression with observed TF-gene pairs to a low-dimensional space. Then, the specific gene representations are learned to serve for downstream similarity measurement or causal inference of pairwise genes by optimizing the embedding space. Compared to eight existing GRN reconstruction methods, GENELink achieves comparable or better performance on seven scRNA-seq datasets with four types of ground-truth networks. We further apply GENELink on scRNA-seq of human breast cancer metastasis and reveal regulatory heterogeneity of Notch and Wnt signalling pathways between primary tumour and lung metastasis. Moreover, the ontology enrichment results of unique lung metastasis GRN indicate that mitochondrial oxidative phosphorylation (OXPHOS) is functionally important during the seeding step of the cancer metastatic cascade, which is validated by pharmacological assays.The code and data are available at https://github.com/zpliulab/GENELink.Supplementary data are available at Bioinformatics online.},
  file     = {/Users/jkobject/Zotero/storage/BVRNECMC/Chen and Liu - 2022 - Graph attention network for link prediction of gen.pdf;/Users/jkobject/Zotero/storage/6BF4GE2A/6663989.html}
}

@article{chenLearningLearnGradient,
  title      = {Learning to {{Learn}} without {{Gradient Descent}} by {{Gradient Descent}}},
  author     = {Chen, Yutian and Hoffman, Matthew W and Colmenarejo, Sergio Gomez and Denil, Misha and Lillicrap, Timothy P and Botvinick, Matt and family=Freitas, given=Nando, prefix=de, useprefix=true},
  pages      = {9},
  abstract   = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to tradeoff exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@unpublished{chenNeuralOrdinaryDifferential2018,
  title       = {Neural {{Ordinary Differential Equations}}},
  author      = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  date        = {2018-06-19},
  eprint      = {1806.07366},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1806.07366},
  urldate     = {2019-03-22},
  abstract    = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@misc{chenQuantizedMultitaskLearning2024,
  title         = {Quantized Multi-Task Learning for Context-Specific Representations of Gene Network Dynamics},
  author        = {Chen, Han and Venkatesh, Madhavan S. and Ortega, Javier G{\'o}mez and Mahesh, Siddharth V. and Nandi, Tarak N. and Madduri, Ravi K. and Pelka, Karin and Theodoris, Christina V.},
  year          = {2024},
  month         = aug,
  primaryclass  = {New Results},
  pages         = {2024.08.16.608180},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.08.16.608180},
  urldate       = {2024-10-25},
  abstract      = {While often represented as static entities, gene networks are highly context-dependent. Here, we developed a multi-task learning strategy to yield context-specific representations of gene network dynamics. We assembled a corpus comprising {$\sim$}103 million human single-cell transcriptomes from a broad range of tissues and diseases and performed a two stage pretraining, first with non-malignant cells to generate a foundational model and then with continual learning on cancer cells to tune the model to the cancer domain. We performed multi-task learning with the foundational model to learn context-specific representations of a broad range of cell types, tissues, developmental stages, and diseases. We then leveraged the cancer-tuned model to jointly learn cell states and predict tumor-restricting factors within the colorectal tumor microenvironment. Model quantization allowed resource-efficient fine-tuning and inference while preserving biological knowledge. Overall, multi-task learning enables context-specific disease modeling that can yield contextual predictions of candidate therapeutic targets for human disease.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/LFSEFDAP/Chen et al. - 2024 - Quantized multi-task learning for context-specific.pdf}
}

@article{chenSelfcalibratingNeuralNetworks2016,
  title        = {Self-Calibrating {{Neural Networks}} for {{Dimensionality Reduction}}},
  author       = {Chen, Yuansi and Pehlevan, Cengiz and Chklovskii, Dmitri B.},
  date         = {2016-11},
  journaltitle = {2016 50th Asilomar Conference on Signals, Systems and Computers},
  eprint       = {1612.03480},
  eprinttype   = {arXiv},
  pages        = {1488--1495},
  doi          = {10/gfxbf2},
  url          = {http://arxiv.org/abs/1612.03480},
  urldate      = {2019-03-22},
  abstract     = {Recently, a novel family of biologically plausible online algorithms for reducing the dimensionality of streaming data has been derived from the similarity matching principle. In these algorithms, the number of output dimensions can be determined adaptively by thresholding the singular values of the input data matrix. However, setting such threshold requires knowing the magnitude of the desired singular values in advance. Here we propose online algorithms where the threshold is self-calibrating based on the singular values computed from the existing observations. To derive these algorithms from the similarity matching cost function we propose novel regularizers. As before, these online algorithms can be implemented by Hebbian/anti-Hebbian neural networks in which the learning rule depends on the chosen regularizer. We demonstrate both mathematically and via simulation the effectiveness of these online algorithms in various settings.},
  langid       = {english},
  keywords     = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  annotation   = {00000}
}

@inproceedings{chenSelfcalibratingNeuralNetworks2016a,
  title      = {Self-Calibrating Neural Networks for Dimensionality Reduction},
  author     = {Chen, Yuansi and Pehlevan, Cengiz and Chklovskii, Dmitri B.},
  date       = {2016-11},
  pages      = {1488--1495},
  publisher  = {IEEE},
  doi        = {10.1109/ACSSC.2016.7869625},
  url        = {http://ieeexplore.ieee.org/document/7869625/},
  urldate    = {2018-04-11},
  abstract   = {Recently, a novel family of biologically plausible online algorithms for reducing the dimensionality of streaming data has been derived from the similarity matching principle. In these algorithms, the number of output dimensions can be determined adaptively by thresholding the singular values of the input data matrix. However, setting such threshold requires knowing the magnitude of the desired singular values in advance. Here we propose online algorithms where the threshold is self-calibrating based on the singular values computed from the existing observations. To derive these algorithms from the similarity matching cost function we propose novel regularizers. As before, these online algorithms can be implemented by Hebbian/anti-Hebbian neural networks in which the learning rule depends on the chosen regularizer. We demonstrate both mathematically and via simulation the effectiveness of these online algorithms in various settings.},
  isbn       = {978-1-5386-3954-2},
  langid     = {english},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/Chen et al. - 2016 - Self-calibrating neural networks for dimensionalit.pdf}
}

@article{chenSmallLongNoncoding2024,
  title      = {Small and Long Non-Coding {{RNAs}}: {{Past}}, Present, and Future},
  shorttitle = {Small and Long Non-Coding {{RNAs}}},
  author     = {Chen, Ling-Ling and Kim, V. Narry},
  year       = 2024,
  month      = nov,
  journal    = {Cell},
  volume     = {187},
  number     = {23},
  pages      = {6451--6485},
  publisher  = {Elsevier},
  issn       = {0092-8674, 1097-4172},
  doi        = {10.1016/j.cell.2024.10.024},
  urldate    = {2025-12-02},
  langid     = {english},
  pmid       = {39547208},
  keywords   = {long non-coding RNAs,microRNAs,ncRNA biogenesis,ncRNA evolution,ncRNA function,ncRNA mechanism,ncRNAs in diseases,regulatory ncRNAs},
  file       = {/Users/jkobject/Zotero/storage/TRYJAPX5/Chen and Kim - 2024 - Small and long non-coding RNAs Past, present, and future.pdf}
}

@article{chenSTREAMSinglecellTrajectories2018,
  title        = {{{STREAM}}: {{Single-cell Trajectories Reconstruction}}, {{Exploration And Mapping}} of Omics Data},
  shorttitle   = {{{STREAM}}},
  author       = {Chen, Huidong and Albergante, Luca and Hsu, Jonathan Y. and Lareau, Caleb A. and Lo Bosco, Giosuè and Guan, Jihong and Zhou, Shuigeng and Gorban, Alexander N. and Bauer, Daniel E. and Aryee, Martin J. and Langenau, David M. and Zinovyev, Andrei and Buenrostro, Jason D. and Yuan, Guo-Cheng and Pinello, Luca},
  date         = {2018-04-18},
  journaltitle = {bioRxiv},
  doi          = {10.1101/302554},
  url          = {http://biorxiv.org/lookup/doi/10.1101/302554},
  urldate      = {2019-03-22},
  abstract     = {Single-cell transcriptomic assays have enabled the de novo reconstruction of lineage differentiation trajectories, along with the characterization of cellular heterogeneity and state transitions. Several methods have been developed for reconstructing developmental trajectories from single-cell transcriptomic data, but efforts on analyzing single-cell epigenomic data and on trajectory visualization remain limited. Here we present STREAM, an interactive pipeline capable of disentangling and visualizing complex branching trajectories from both single-cell transcriptomic and epigenomic data.},
  langid       = {english},
  annotation   = {00000}
}

@article{chenTransmembraneHelicalTrimers,
  title      = {Transmembrane {{Helical Trimers}}: From {{Sequence}} to {{Geometry}}},
  author     = {Chen, Muyuan},
  pages      = {16},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{chenUNIGeneralpurposeFoundation2024,
  title   = {Towards a general-purpose foundation model for computational pathology},
  author  = {Chen, Richard J. and others},
  journal = {Nature Medicine},
  volume  = {30},
  pages   = {850--862},
  year    = {2024},
  doi     = {10.1038/s41591-024-02857-3}
}

@article{cheungSystematicInvestigationGenetic2011,
  title        = {Systematic Investigation of Genetic Vulnerabilities across Cancer Cell Lines Reveals Lineage-Specific Dependencies in Ovarian Cancer},
  author       = {Cheung, H. W. and Cowley, G. S. and Weir, B. A. and Boehm, J. S. and Rusin, S. and Scott, J. A. and East, A. and Ali, L. D. and Lizotte, P. H. and Wong, T. C. and Jiang, G. and Hsiao, J. and Mermel, C. H. and Getz, G. and Barretina, J. and Gopal, S. and Tamayo, P. and Gould, J. and Tsherniak, A. and Stransky, N. and Luo, B. and Ren, Y. and Drapkin, R. and Bhatia, S. N. and Mesirov, J. P. and Garraway, L. A. and Meyerson, M. and Lander, E. S. and Root, D. E. and Hahn, W. C.},
  date         = {2011-07-26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume       = {108},
  number       = {30},
  pages        = {12372--12377},
  issn         = {0027-8424, 1091-6490},
  doi          = {10/bjfj5s},
  url          = {http://www.pnas.org/cgi/doi/10.1073/pnas.1109363108},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{chief,
  title     = {A Pathology Foundation Model for Cancer Diagnosis and Prognosis Prediction},
  author    = {Wang, Xiyue and Zhao, Junhan and Marostica, Eliana and Yuan, Wei and Jin, Jietian and Zhang, Jiayu and Li, Ruijiang and Tang, Hongping and Wang, Kanran and Li, Yu and Wang, Fang and Peng, Yulong and Zhu, Junyou and Zhang, Jing and Jackson, Christopher R. and Zhang, Jun and Dillon, Deborah and Lin, Nancy U. and Sholl, Lynette and Denize, Thomas and Meredith, David and Ligon, Keith L. and Signoretti, Sabina and Ogino, Shuji and Golden, Jeffrey A. and Nasrallah, MacLean P. and Han, Xiao and Yang, Sen and Yu, Kun-Hsing},
  year      = {2024},
  month     = oct,
  journal   = {Nature},
  volume    = {634},
  number    = {8035},
  pages     = {970--978},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/s41586-024-07894-z},
  urldate   = {2025-02-25},
  abstract  = {Histopathology image evaluation is indispensable for cancer diagnoses and subtype classification. Standard artificial intelligence methods for histopathology image analyses have focused on optimizing specialized models for each diagnostic task1,2. Although such methods have achieved some success, they often have limited generalizability to images generated by different digitization protocols or samples collected from different populations3. Here, to address this challenge, we devised the Clinical Histopathology Imaging Evaluation Foundation (CHIEF) model, a general-purpose weakly supervised machine learning framework to extract pathology imaging features for systematic cancer evaluation. CHIEF leverages two complementary pretraining methods to extract diverse pathology representations: unsupervised pretraining for tile-level feature identification and weakly supervised pretraining for whole-slide pattern recognition. We developed CHIEF using 60,530 whole-slide images spanning 19 anatomical sites. Through pretraining on 44 terabytes of high-resolution pathology imaging datasets, CHIEF extracted microscopic representations useful for cancer cell detection, tumour origin identification, molecular profile characterization and prognostic prediction. We successfully validated CHIEF using 19,491 whole-slide images from 32 independent slide sets collected from 24 hospitals and cohorts internationally. Overall, CHIEF outperformed the state-of-the-art deep learning methods by up to 36.1\%, showing its ability to address domain shifts observed in samples from diverse populations and processed by different slide preparation methods. CHIEF provides a generalizable foundation for efficient digital pathology evaluation for patients with cancer.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
  langid    = {english},
  keywords  = {Cancer,Computational models,Image processing,Machine learning,Predictive medicine},
  file      = {/Users/jkobject/Zotero/storage/B2J4YTI6/Wang et al. - 2024 - A pathology foundation model for cancer diagnosis .pdf}
}

@article{chingOpportunitiesObstaclesDeep2018,
  title        = {Opportunities {{And Obstacles For Deep Learning In Biology And Medicine}}},
  author       = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A and Turaga, Srinivas C and Alexandari, Amr M and Lu, Zhiyong and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Boca, Simina M and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
  date         = {2018-01-19},
  journaltitle = {bioRxiv},
  doi          = {10/gbpvh5},
  url          = {http://biorxiv.org/lookup/doi/10.1101/142760},
  urldate      = {2019-03-22},
  abstract     = {Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems—patient classification, fundamental biological processes, and treatment of patients—and discuss whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at both bench and bedside with the potential to transform several areas of biology and medicine.},
  langid       = {english},
  annotation   = {00000}
}

@article{chongGeneticBasisMendelian2015,
  title        = {The {{Genetic Basis}} of {{Mendelian Phenotypes}}: {{Discoveries}}, {{Challenges}}, and {{Opportunities}}},
  shorttitle   = {The {{Genetic Basis}} of {{Mendelian Phenotypes}}},
  author       = {Chong, Jessica~X. and Buckingham, Kati~J. and Jhangiani, Shalini~N. and Boehm, Corinne and Sobreira, Nara and Smith, Joshua~D. and Harrell, Tanya~M. and McMillin, Margaret~J. and Wiszniewski, Wojciech and Gambin, Tomasz and Coban~Akdemir, Zeynep~H. and Doheny, Kimberly and Scott, Alan~F. and Avramopoulos, Dimitri and Chakravarti, Aravinda and Hoover-Fong, Julie and Mathews, Debra and Witmer, P.~Dane and Ling, Hua and Hetrick, Kurt and Watkins, Lee and Patterson, Karynne~E. and Reinier, Frederic and Blue, Elizabeth and Muzny, Donna and Kircher, Martin and Bilguvar, Kaya and López-Giráldez, Francesc and Sutton, V.~Reid and Tabor, Holly~K. and Leal, Suzanne~M. and Gunel, Murat and Mane, Shrikant and Gibbs, Richard~A. and Boerwinkle, Eric and Hamosh, Ada and Shendure, Jay and Lupski, James~R. and Lifton, Richard~P. and Valle, David and Nickerson, Deborah~A. and Bamshad, Michael~J.},
  date         = {2015-08},
  journaltitle = {The American Journal of Human Genetics},
  volume       = {97},
  number       = {2},
  pages        = {199--215},
  issn         = {00029297},
  doi          = {10/f7nd62},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0002929715002451},
  urldate      = {2019-03-22},
  langid       = {english}
}

@misc{choromanskiRethinkingAttentionPerformers2022,
  title         = {Rethinking {{Attention}} with {{Performers}}},
  author        = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year          = {2022},
  month         = nov,
  number        = {arXiv:2009.14794},
  eprint        = {2009.14794},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2009.14794},
  urldate       = {2024-10-29},
  abstract      = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/5L26W3WY/Choromanski et al. - 2022 - Rethinking Attention with Performers.pdf;/Users/jkobject/Zotero/storage/AJ3X3YYC/2009.html}
}

@article{christiansenSilicoLabelingPredicting2018,
  title        = {In {{Silico Labeling}}: {{Predicting Fluorescent Labels}} in {{Unlabeled Images}}},
  shorttitle   = {In {{Silico Labeling}}},
  author       = {Christiansen, Eric M. and Yang, Samuel J. and Ando, D. Michael and Javaherian, Ashkan and Skibinski, Gaia and Lipnick, Scott and Mount, Elliot and O’Neil, Alison and Shah, Kevan and Lee, Alicia K. and Goyal, Piyush and Fedus, William and Poplin, Ryan and Esteva, Andre and Berndl, Marc and Rubin, Lee L. and Nelson, Philip and Finkbeiner, Steven},
  date         = {2018-04},
  journaltitle = {Cell},
  volume       = {173},
  number       = {3},
  pages        = {792-803.e19},
  issn         = {00928674},
  doi          = {10.1016/j.cell.2018.03.040},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867418303647},
  urldate      = {2019-03-22},
  abstract     = {Microscopy is a central method in life sciences. Many popular methods, such as antibody labeling, are used to add physical fluorescent labels to specific cellular constituents. However, these approaches have significant drawbacks, including inconsistency; limitations in the number of simultaneous labels because of spectral overlap; and necessary perturbations of the experiment, such as fixing the cells, to generate the measurement. Here, we show that a computational machine-learning approach, which we call ‘‘in silico labeling’’ (ISL), reliably predicts some fluorescent labels from transmitted-light images of unlabeled fixed or live biological samples. ISL predicts a range of labels, such as those for nuclei, cell type (e.g., neural), and cell state (e.g., cell death). Because prediction happens in silico, the method is consistent, is not limited by spectral overlap, and does not disturb the experiment. ISL generates biological measurements that would otherwise be problematic or impossible to acquire.},
  langid       = {english},
  annotation   = {00000}
}

@misc{chronopoulouEmbarrassinglySimpleApproach2019,
  title         = {An {{Embarrassingly Simple Approach}} for {{Transfer Learning}} from {{Pretrained Language Models}}},
  author        = {Chronopoulou, Alexandra and Baziotis, Christos and Potamianos, Alexandros},
  year          = {2019},
  month         = may,
  number        = {arXiv:1902.10547},
  eprint        = {1902.10547},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1902.10547},
  urldate       = {2025-02-25},
  abstract      = {A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/P44HXFIX/Chronopoulou et al. - 2019 - An Embarrassingly Simple Approach for Transfer Lea.pdf;/Users/jkobject/Zotero/storage/YAAB8JEM/1902.html}
}

@article{ciofaniValidatedRegulatoryNetwork2012,
  title        = {A {{Validated Regulatory Network}} for {{Th17 Cell Specification}}},
  author       = {Ciofani, Maria and Madar, Aviv and Galan, Carolina and Sellars, MacLean and Mace, Kieran and Pauli, Florencia and Agarwal, Ashish and Huang, Wendy and Parkurst, Christopher~N. and Muratet, Michael and Newberry, Kim~M. and Meadows, Sarah and Greenfield, Alex and Yang, Yi and Jain, Preti and Kirigin, Francis~K. and Birchmeier, Carmen and Wagner, Erwin~F. and Murphy, Kenneth~M. and Myers, Richard~M. and Bonneau, Richard and Littman, Dan~R.},
  date         = {2012-10},
  journaltitle = {Cell},
  volume       = {151},
  number       = {2},
  pages        = {289--303},
  issn         = {00928674},
  doi          = {10/gfsjbd},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0092867412011233},
  urldate      = {2018-04-11},
  abstract     = {Th17 cells have critical roles in mucosal defense and are major contributors to inflammatory disease. Their differentiation requires the nuclear hormone receptor RORgt working with multiple other essential transcription factors (TFs). We have used an iterative systems approach, combining genome-wide TF occupancy, expression profiling of TF mutants, and expression time series to delineate the Th17 global transcriptional regulatory network. We find that cooperatively bound BATF and IRF4 contribute to initial chromatin accessibility and, with STAT3, initiate a transcriptional program that is then globally tuned by the lineage-specifying TF RORgt, which plays a focal deterministic role at key loci. Integration of multiple data sets allowed inference of an accurate predictive model that we computationally and experimentally validated, identifying multiple new Th17 regulators, including Fosl2, a key determinant of cellular plasticity. This interconnected network can be used to investigate new therapeutic approaches to manipulate Th17 functions in the setting of inflammatory disease.},
  langid       = {english},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Ciofani et al. - 2012 - A Validated Regulatory Network for Th17 Cell Speci.pdf}
}

@article{clarkMetaanalysisDiagnosticClinical2018,
  title        = {Meta-Analysis of the Diagnostic and Clinical Utility of Genome and Exome Sequencing and Chromosomal Microarray in Children with Suspected Genetic Diseases},
  author       = {Clark, Michelle M. and Stark, Zornitza and Farnaes, Lauge and Tan, Tiong Y. and White, Susan M. and Dimmock, David and Kingsmore, Stephen F.},
  date         = {2018-12},
  journaltitle = {npj Genomic Medicine},
  volume       = {3},
  number       = {1},
  issn         = {2056-7944},
  doi          = {10/gdv3s4},
  url          = {http://www.nature.com/articles/s41525-018-0053-8},
  urldate      = {2019-03-22},
  langid       = {english}
}

@misc{clarkWhatDoesBERT2019a,
  title         = {What {{Does BERT Look At}}? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle    = {What {{Does BERT Look At}}?},
  author        = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year          = {2019},
  month         = jun,
  number        = {arXiv:1906.04341},
  eprint        = {1906.04341},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1906.04341},
  urldate       = {2024-07-15},
  abstract      = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/2LKUR6JR/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Atte.pdf;/Users/jkobject/Zotero/storage/Z3L8D5ZL/1906.html}
}

@article{coatesImportanceEncodingTraining,
  title      = {The {{Importance}} of {{Encoding Versus Training}} with {{Sparse Coding}} and {{Vector Quantization}}},
  author     = {Coates, Adam and Ng, Andrew Y},
  pages      = {8},
  abstract   = {While vector quantization (VQ) has been applied widely to generate features for visual recognition problems, much recent work has focused on more powerful methods. In particular, sparse coding has emerged as a strong alternative to traditional VQ approaches and has been shown to achieve consistently higher performance on benchmark datasets. Both approaches can be split into a training phase, where the system learns a dictionary of basis functions, and an encoding phase, where the dictionary is used to extract features from new inputs. In this work, we investigate the reasons for the success of sparse coding over VQ by decoupling these phases, allowing us to separate out the contributions of training and encoding in a controlled way. Through extensive experiments on CIFAR, NORB and Caltech 101 datasets, we compare several training and encoding schemes, including sparse coding and a form of VQ with a soft threshold activation function. Our results show not only that we can use fast VQ algorithms for training, but that we can just as well use randomly chosen exemplars from the training set. Rather than spend resources on training, we find it is more important to choose a good encoder—which can often be a simple feed forward non-linearity. Our results include state-of-the-art performance on both CIFAR and NORB.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/autoEncoder/Coates et Ng - The Importance of Encoding Versus Training with Sp.pdf}
}


@misc{cola,
  title         = {{{COLA}}: {{A Benchmark}} for {{Compositional Text-to-image Retrieval}}},
  shorttitle    = {{{COLA}}},
  author        = {Ray, Arijit and Radenovic, Filip and Dubey, Abhimanyu and Plummer, Bryan A. and Krishna, Ranjay and Saenko, Kate},
  year          = {2023},
  month         = nov,
  number        = {arXiv:2305.03689},
  eprint        = {2305.03689},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2305.03689},
  urldate       = {2025-02-06},
  abstract      = {Compositional reasoning is a hallmark of human visual intelligence. Yet, despite the size of large vision-language models, they struggle to represent simple compositions by combining objects with their attributes. To measure this lack of compositional capability, we design Cola, a text-to-image retrieval benchmark to Compose Objects Localized with Attributes. To solve Cola, a model must retrieve images with the correct configuration of attributes and objects and avoid choosing a distractor image with the same objects and attributes but in the wrong configuration. Cola contains about 1.2k composed queries of 168 objects and 197 attributes on around 30K images. Our human evaluation finds that Cola is 83.33\% accurate, similar to contemporary compositionality benchmarks. Using Cola as a testbed, we explore empirical modeling designs to adapt pre-trained vision-language models to reason compositionally. We explore 6 adaptation strategies on 2 seminal vision-language models, using compositionality-centric test benchmarks - Cola and CREPE. We find the optimal adaptation strategy is to train a multi-modal attention layer that jointly attends over the frozen pre-trained image and language features. Surprisingly, training multimodal layers on CLIP performs better than tuning a larger FLAVA model with already pre-trained multimodal layers. Furthermore, our adaptation strategy improves CLIP and FLAVA to comparable levels, suggesting that training multimodal layers using contrastive attribute-object data is key, as opposed to using them pre-trained. Lastly, we show that Cola is harder than a closely related contemporary benchmark, CREPE, since simpler fine-tuning strategies without multimodal layers suffice on CREPE but not on Cola. However, we still see a significant gap between our best adaptation and human accuracy, suggesting considerable room for further research.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {/Users/jkobject/Zotero/storage/AFT4TELR/Ray et al. - 2023 - COLA A Benchmark for Compositional Text-to-image .pdf;/Users/jkobject/Zotero/storage/2FE8IBZ2/2305.html}
}

@article{colottaCancerrelatedInflammationSeventh2009,
  title        = {Cancer-Related Inflammation, the Seventh Hallmark of Cancer: Links to Genetic Instability},
  shorttitle   = {Cancer-Related Inflammation, the Seventh Hallmark of Cancer},
  author       = {Colotta, Francesco and Allavena, Paola and Sica, Antonio and Garlanda, Cecilia and Mantovani, Alberto},
  date         = {2009-07-01},
  journaltitle = {Carcinogenesis},
  shortjournal = {Carcinogenesis},
  volume       = {30},
  number       = {7},
  pages        = {1073--1081},
  issn         = {0143-3334},
  doi          = {10.1093/carcin/bgp127},
  url          = {https://doi.org/10.1093/carcin/bgp127},
  urldate      = {2024-07-26},
  abstract     = {Inflammatory conditions in selected organs increase the risk of cancer. An inflammatory component is present also in the microenvironment of tumors that are not epidemiologically related to inflammation. Recent studies have begun to unravel molecular pathways linking inflammation and cancer. In the tumor microenvironment, smoldering inflammation contributes to proliferation and survival of malignant cells, angiogenesis, metastasis, subversion of adaptive immunity, reduced response to hormones and chemotherapeutic agents. Recent data suggest that an additional mechanism involved in cancer-related inflammation (CRI) is induction of genetic instability by inflammatory mediators, leading to accumulation of random genetic alterations in cancer cells. In a seminal contribution, Hanahan and Weinberg [(2000) Cell, 100, 57–70] identified the six hallmarks of cancer. We surmise that CRI represents the seventh hallmark.},
  file         = {/Users/jkobject/Zotero/storage/4L8AVMI2/Colotta et al. - 2009 - Cancer-related inflammation, the seventh hallmark .pdf;/Users/jkobject/Zotero/storage/3DUJ3J78/2477107.html}
}

@online{comprehensiveSurveyDataAugmentation2024,
  title = {A Comprehensive Survey on Data Augmentation},
  year  = {2024},
  url   = {https://arxiv.org/html/2405.09591v3}
}

@article{condeCrosstissueImmuneCell2022,
  title    = {Cross-Tissue Immune Cell Analysis Reveals Tissue-Specific Features in Humans},
  author   = {Conde, C. Dom{\'i}nguez and Xu, C. and Jarvis, L. B. and Rainbow, D. B. and Wells, S. B. and Gomes, T. and Howlett, S. K. and Suchanek, O. and Polanski, K. and King, H. W. and Mamanova, L. and Huang, N. and Szabo, P. A. and Richardson, L. and Bolt, L. and Fasouli, E. S. and Mahbubani, K. T. and Prete, M. and Tuck, L. and Richoz, N. and Tuong, Z. K. and Campos, L. and Mousa, H. S. and Needham, E. J. and Pritchard, S. and Li, T. and Elmentaite, R. and Park, J. and Rahmani, E. and Chen, D. and Menon, D. K. and Bayraktar, O. A. and James, L. K. and Meyer, K. B. and Yosef, N. and Clatworthy, M. R. and Sims, P. A. and Farber, D. L. and {Saeb-Parsy}, K. and Jones, J. L. and Teichmann, S. A.},
  year     = {2022},
  month    = may,
  journal  = {Science (New York, N.Y.)},
  volume   = {376},
  number   = {6594},
  pages    = {eabl5197},
  doi      = {10.1126/science.abl5197},
  urldate  = {2024-10-23},
  abstract = {Despite their crucial role in health and disease, our knowledge of immune cells within human tissues remains limited. Here, we surveyed the immune compartment of 16 tissues from 12 adult donors by single-cell RNA sequencing and VDJ sequencing ...},
  langid   = {english},
  pmid     = {35549406},
  file     = {/Users/jkobject/Zotero/storage/FK2YTIQ7/Conde et al. - 2022 - Cross-tissue immune cell analysis reveals tissue-s.pdf}
}

@article{constantinescuOrganizingConceptualKnowledge2016,
  title        = {Organizing Conceptual Knowledge in Humans with a Gridlike Code},
  author       = {Constantinescu, A. O. and OReilly, J. X. and Behrens, T. E. J.},
  date         = {2016-06-17},
  journaltitle = {Science},
  volume       = {352},
  number       = {6292},
  pages        = {1464--1468},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/f8rt9r},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaf0941},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@article{coodyAdvancingAgingBiology2018,
  title        = {Advancing the Aging Biology Toolkit},
  author       = {Coody, Troy K and Hughes, Adam L},
  date         = {2018-11-28},
  journaltitle = {eLife},
  volume       = {7},
  issn         = {2050-084X},
  doi          = {10.7554/eLife.42976},
  url          = {https://elifesciences.org/articles/42976},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@inproceedings{cookComplexityTheoremprovingProcedures1971,
  title      = {The Complexity of Theorem-Proving Procedures},
  booktitle  = {Proceedings of the Third Annual {{ACM}} Symposium on {{Theory}} of Computing  - {{STOC}} '71},
  author     = {Cook, Stephen A.},
  date       = {1971},
  pages      = {151--158},
  publisher  = {ACM Press},
  location   = {Shaker Heights, Ohio, United States},
  doi        = {10.1145/800157.805047},
  url        = {http://portal.acm.org/citation.cfm?doid=800157.805047},
  urldate    = {2019-03-22},
  abstract   = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
  eventtitle = {The Third Annual {{ACM}} Symposium},
  langid     = {english}
}

@article{copeQuantifyingCodonUsage2018,
  title        = {Quantifying Codon Usage in Signal Peptides: {{Gene}} Expression and Amino Acid Usage Explain Apparent Selection for Inefficient Codons},
  shorttitle   = {Quantifying Codon Usage in Signal Peptides},
  author       = {Cope, Alexander L. and Hettich, Robert L. and Gilchrist, Michael A.},
  date         = {2018-12},
  journaltitle = {Biochimica et Biophysica Acta (BBA) - Biomembranes},
  volume       = {1860},
  number       = {12},
  pages        = {2479--2485},
  issn         = {00052736},
  doi          = {10/gfnspj},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0005273618302748},
  urldate      = {2019-03-22},
  abstract     = {The Sec secretion pathway is found across all domains of life. A critical feature of Sec secreted proteins is the signal peptide, a short peptide with distinct physicochemical properties located at the N-terminus of the protein. Previous work indicates signal peptides are biased towards translationally inefficient codons, which is hypothesized to be an adaptation driven by selection to improve the efficacy and efficiency of the protein secretion mechanisms. We investigate codon usage in the signal peptides of E. coli using the Codon Adaptation Index (CAI), the tRNA Adaptation Index (tAI), and the ribosomal overhead cost formulation of the stochastic evolutionary model of protein production rates (ROC-SEMPPR). Comparisons between signal peptides and 5′-end of cytoplasmic proteins using CAI and tAI are consistent with a preference for inefficient codons in signal peptides. Simulations reveal these differences are due to amino acid usage and gene expression – we find these differences disappear when accounting for both factors. In contrast, ROC-SEMPPR, a mechanistic population genetics model capable of separating the effects of selection and mutation bias, shows codon usage bias (CUB) of the signal peptides is indistinguishable from the 5′-ends of cytoplasmic proteins. Additionally, we find CUB at the 5′-ends is weaker than later segments of the gene. Results illustrate the value in using models grounded in population genetics to interpret genetic data. We show failure to account for mutation bias and the effects of gene expression on the efficacy of selection against translation inefficiency can lead to a misinterpretation of codon usage patterns.},
  langid       = {english},
  annotation   = {00000}
}

@online{cornmanOMGDatasetOpen2024,
  title       = {The {{OMG}} Dataset: {{An Open MetaGenomic}} Corpus for Mixed-Modality Genomic Language Modeling},
  shorttitle  = {The {{OMG}} Dataset},
  author      = {Cornman, Andre and West-Roberts, Jacob and Camargo, Antonio Pedro and Roux, Simon and Beracochea, Martin and Mirdita, Milot and Ovchinnikov, Sergey and Hwang, Yunha},
  date        = {2024-08-17},
  eprinttype  = {bioRxiv},
  eprintclass = {New Results},
  pages       = {2024.08.14.607850},
  doi         = {10.1101/2024.08.14.607850},
  url         = {https://www.biorxiv.org/content/10.1101/2024.08.14.607850v1},
  urldate     = {2025-03-09},
  abstract    = {Biological language model performance depends heavily on pretraining data quality, diversity, and size. While metagenomic datasets feature enormous biological diversity, their utilization as pretraining data has been limited due to challenges in data accessibility, quality filtering and deduplication. Here, we present the Open MetaGenomic (OMG) corpus, a genomic pretraining dataset totalling 3.1T base pairs and 3.3B protein coding sequences, obtained by combining two largest metagenomic dataset repositories (JGI’s IMG and EMBL’s MGnify). We first document the composition of the dataset and describe the quality filtering steps taken to remove poor quality data. We make the OMG corpus available as a mixed-modality genomic sequence dataset that represents multi-gene encoding genomic sequences with translated amino acids for protein coding sequences, and nucleic acids for intergenic sequences. We train the first mixed-modality genomic language model (gLM2) that leverages genomic context information to learn robust functional representations and coevolutionary signals in protein-protein interfaces. Furthermore, we show that deduplication in embedding space can be used to balance the corpus, demonstrating improved performance on downstream tasks. The OMG dataset is publicly hosted on the Hugging Face Hub at https://huggingface.co/datasets/tattabio/OMG and gLM2 is available at https://huggingface.co/tattabio/gLM2\_650M.},
  langid      = {english},
  pubstate    = {prepublished},
  file        = {/Users/jkobject/Zotero/storage/E58FZMCP/Cornman et al. - 2024 - The OMG dataset An Open MetaGenomic corpus for mi.pdf}
}

@misc{cornmanOMGDatasetOpen2024a,
  title         = {The {{OMG}} Dataset: {{An Open MetaGenomic}} Corpus for Mixed-Modality Genomic Language Modeling},
  shorttitle    = {The {{OMG}} Dataset},
  author        = {Cornman, Andre and {West-Roberts}, Jacob and Camargo, Antonio Pedro and Roux, Simon and Beracochea, Martin and Mirdita, Milot and Ovchinnikov, Sergey and Hwang, Yunha},
  year          = {2024},
  month         = aug,
  primaryclass  = {New Results},
  pages         = {2024.08.14.607850},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.08.14.607850},
  urldate       = {2025-03-09},
  abstract      = {Biological language model performance depends heavily on pretraining data quality, diversity, and size. While metagenomic datasets feature enormous biological diversity, their utilization as pretraining data has been limited due to challenges in data accessibility, quality filtering and deduplication. Here, we present the Open MetaGenomic (OMG) corpus, a genomic pretraining dataset totalling 3.1T base pairs and 3.3B protein coding sequences, obtained by combining two largest metagenomic dataset repositories (JGI's IMG and EMBL's MGnify). We first document the composition of the dataset and describe the quality filtering steps taken to remove poor quality data. We make the OMG corpus available as a mixed-modality genomic sequence dataset that represents multi-gene encoding genomic sequences with translated amino acids for protein coding sequences, and nucleic acids for intergenic sequences. We train the first mixed-modality genomic language model (gLM2) that leverages genomic context information to learn robust functional representations and coevolutionary signals in protein-protein interfaces. Furthermore, we show that deduplication in embedding space can be used to balance the corpus, demonstrating improved performance on downstream tasks. The OMG dataset is publicly hosted on the Hugging Face Hub at https://huggingface.co/datasets/tattabio/OMG and gLM2 is available at https://huggingface.co/tattabio/gLM2\_650M.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/PP83N47Z/Cornman et al. - 2024 - The OMG dataset An Open MetaGenomic corpus for mi.pdf}
}

@article{corsoGraphNeuralNetworks2024,
  title     = {Graph Neural Networks},
  author    = {Corso, Gabriele and Stark, Hannes and Jegelka, Stefanie and Jaakkola, Tommi and Barzilay, Regina},
  year      = 2024,
  month     = mar,
  journal   = {Nature Reviews Methods Primers},
  volume    = {4},
  number    = {1},
  pages     = {17},
  publisher = {Nature Publishing Group},
  issn      = {2662-8449},
  doi       = {10.1038/s43586-024-00294-7},
  urldate   = {2025-12-12},
  abstract  = {Graphs are flexible mathematical objects that can represent many entities and knowledge from different domains, including in the life sciences. Graph neural networks (GNNs) are mathematical models that can learn functions over graphs and are a leading approach for building predictive models on graph-structured data. This combination has enabled GNNs to advance the state of the art in many disciplines, from discovering new antibiotics and identifying drug-repurposing candidates to modelling physical systems and generating new molecules. This Primer provides a practical and accessible introduction to GNNs, describing their properties and applications to the life and physical sciences. Emphasis is placed on the practical implications of key theoretical limitations, new ideas to solve these challenges and important considerations when using GNNs on a new task.},
  copyright = {2024 Springer Nature Limited},
  langid    = {english},
  keywords  = {Computational biology and bioinformatics,Statistics},
  file      = {/Users/jkobject/Zotero/storage/YI979CG9/Corso et al. - 2024 - Graph neural networks.pdf}
}

@article{costanzoGeneticLandscapeCell2010,
  title        = {The {{Genetic Landscape}} of a {{Cell}}},
  author       = {Costanzo, M. and Baryshnikova, A. and Bellay, J. and Kim, Y. and Spear, E. D. and Sevier, C. S. and Ding, H. and Koh, J. L.Y. and Toufighi, K. and Mostafavi, S. and Prinz, J. and St. Onge, R. P. and VanderSluis, B. and Makhnevych, T. and Vizeacoumar, F. J. and Alizadeh, S. and Bahr, S. and Brost, R. L. and Chen, Y. and Cokol, M. and Deshpande, R. and Li, Z. and Lin, Z.-Y. and Liang, W. and Marback, M. and Paw, J. and San Luis, B.-J. and Shuteriqi, E. and Tong, A. H. Y. and family=Dyk, given=N., prefix=van, useprefix=true and Wallace, I. M. and Whitney, J. A. and Weirauch, M. T. and Zhong, G. and Zhu, H. and Houry, W. A. and Brudno, M. and Ragibizadeh, S. and Papp, B. and Pal, C. and Roth, F. P. and Giaever, G. and Nislow, C. and Troyanskaya, O. G. and Bussey, H. and Bader, G. D. and Gingras, A.-C. and Morris, Q. D. and Kim, P. M. and Kaiser, C. A. and Myers, C. L. and Andrews, B. J. and Boone, C.},
  date         = {2010-01-22},
  journaltitle = {Science},
  volume       = {327},
  number       = {5964},
  pages        = {425--431},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/bvs256},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.1180823},
  urldate      = {2019-03-22},
  abstract     = {A genome-scale genetic interaction map was constructed by examining 5.4 million gene-gene pairs for synthetic genetic interactions, generating quantitative genetic interaction profiles for \textasciitilde 75\% of all genes in the budding yeast, Saccharomyces cerevisiae. A network based on genetic interaction profiles reveals a functional map of the cell in which genes of similar biological processes cluster together in coherent subsets, and highly correlated profiles delineate specific pathways to define gene function. The global network identifies functional cross-connections between all bioprocesses, mapping a cellular wiring diagram of pleiotropy. Genetic interaction degree correlated with a number of different gene attributes, which may be informative about genetic network hubs in other organisms. We also demonstrate that extensive and unbiased mapping of the genetic landscape provides a key for interpretation of chemical-genetic interactions and drug target identification.},
  langid       = {english},
  annotation   = {00000}
}

@article{cowleyParallelGenomescaleLoss2014,
  title        = {Parallel Genome-Scale Loss of Function Screens in 216 Cancer Cell Lines for the Identification of Context-Specific Genetic Dependencies},
  author       = {Cowley, Glenn S and Weir, Barbara A and Vazquez, Francisca and Tamayo, Pablo and Scott, Justine A and Rusin, Scott and East-Seletsky, Alexandra and Ali, Levi D and Gerath, William FJ and Pantel, Sarah E and Lizotte, Patrick H and Jiang, Guozhi and Hsiao, Jessica and Tsherniak, Aviad and Dwinell, Elizabeth and Aoyama, Simon and Okamoto, Michael and Harrington, William and Gelfand, Ellen and Green, Thomas M and Tomko, Mark J and Gopal, Shuba and Wong, Terrence C and Li, Hubo and Howell, Sara and Stransky, Nicolas and Liefeld, Ted and Jang, Dongkeun and Bistline, Jonathan and Hill Meyers, Barbara and Armstrong, Scott A and Anderson, Ken C and Stegmaier, Kimberly and Reich, Michael and Pellman, David and Boehm, Jesse S and Mesirov, Jill P and Golub, Todd R and Root, David E and Hahn, William C},
  date         = {2014-09-30},
  journaltitle = {Scientific Data},
  volume       = {1},
  pages        = {140035},
  issn         = {2052-4463},
  doi          = {10/gfxbgr},
  url          = {http://www.nature.com/articles/sdata201435},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{creutzigPredictiveCodingSlowness2008,
  title        = {Predictive {{Coding}} and the {{Slowness Principle}}: {{An Information-Theoretic Approach}}},
  shorttitle   = {Predictive {{Coding}} and the {{Slowness Principle}}},
  author       = {Creutzig, Felix and Sprekeler, Henning},
  date         = {2008-04},
  journaltitle = {Neural Computation},
  volume       = {20},
  number       = {4},
  pages        = {1026--1041},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/dkh49s},
  url          = {http://www.mitpressjournals.org/doi/10.1162/neco.2008.01-07-455},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Creutzig et Sprekeler - 2008 - Predictive Coding and the Slowness Principle An I.pdf;/Users/jeremie/Dropbox/Journal Club/Creutzig et Sprekeler - 2008 - Predictive Coding and the Slowness Principle An I.pdf}
}

@article{crookBayesianMixtureModelling,
  title      = {A {{Bayesian}} Mixture Modelling Approach for Spatial Proteomics},
  author     = {Crook, Oliver M and Mulvey, Claire M and Kirk, Paul D W and Lilley, Kathryn S and Gatto, Laurent},
  pages      = {29},
  langid     = {english},
  keywords   = {❓ Multiple DOI},
  annotation = {00000}
}

@article{crowleyBenchmarkingCellTypeGene2025,
  title   = {Benchmarking cell type and gene set annotation by large language models with AnnDictionary},
  author  = {Crowley, Grant and Quake, Stephen R.},
  journal = {Nature Communications},
  volume  = {16},
  pages   = {9511},
  year    = {2025},
  doi     = {10.1038/s41467-025-64840-6}
}

@article{cuiScGPTBuildingFoundation2024,
  title      = {{{scGPT}}: Toward Building a Foundation Model for Single-Cell Multi-Omics Using Generative {{AI}}},
  shorttitle = {{{scGPT}}},
  author     = {Cui, Haotian and Wang, Chloe and Maan, Hassaan and Pang, Kuan and Luo, Fengning and Duan, Nan and Wang, Bo},
  year       = {2024},
  month      = feb,
  journal    = {Nature Methods},
  pages      = {1--11},
  publisher  = {Nature Publishing Group},
  issn       = {1548-7105},
  doi        = {10.1038/s41592-024-02201-0},
  urldate    = {2024-04-18},
  abstract   = {Generative pretrained models have achieved remarkable success in various domains such as language and computer vision. Specifically, the combination of large-scale diverse datasets and pretrained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between language and cellular biology (in which texts comprise words; similarly, cells are defined by genes), our study probes the applicability of foundation models to advance cellular biology and genetic research. Using burgeoning single-cell sequencing data, we have constructed a foundation model for single-cell biology, scGPT, based on a generative pretrained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT effectively distills critical biological insights concerning genes and cells. Through further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell type annotation, multi-batch integration, multi-omic integration, perturbation response prediction and gene network inference.},
  copyright  = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid     = {english},
  keywords   = {Computational models,Machine learning,Software,Transcriptomics},
  file       = {/Users/jkobject/Zotero/storage/IPKD7VJD/Cui et al. - 2024 - scGPT toward building a foundation model for sing.pdf}
}

@article{cummingsImprovingGeneticDiagnosis2017,
  title        = {Improving Genetic Diagnosis in {{Mendelian}} Disease with Transcriptome Sequencing},
  author       = {Cummings, Beryl B and Marshall, Jamie L and Tukiainen, Taru and Lek, Monkol and Donkervoort, Sandra and Foley, A Reghan and Bolduc, Veronique and Waddell, Leigh B and Sandaradura, Sarah A and Weisburd, Ben and Karczewski, Konrad J and Sarkozy, Anna and Hu, Ying and Gonorazky, Hernan and Claeys, Kristl and Joshi, Himanshu and Bournazos, Adam and Oates, Emily C and Ghaoui, Roula and Davis, Mark R and Laing, Nigel G and Topf, Ana and Straub, Volker and Dowling, James J and Muntoni, Francesco and Clarke, Nigel F and Cooper, Sandra T and Bönnemann, Carsten G and MacArthur, Daniel G},
  date         = {2017},
  journaltitle = {SCIENCE TRANSLATIONAL MEDICINE},
  pages        = {13},
  doi          = {10/gfvm3m},
  langid       = {english},
  annotation   = {00000}
}

@misc{cuomoImpactRareCommon2025,
  title         = {Impact of {{Rare}} and {{Common Genetic Variation}} on {{Cell Type-Specific Gene Expression}}},
  author        = {Cuomo, Anna S. E. and Spenceley, Eleanor and Tanudisastro, Hope A. and Bowen, Blake and Henry, Albert and Huang, Hao Lawrence and Xue, Angli and Zhou, Wei and Welland, Matthew J. and Lee, Arthur S. and Wing, Kristof and Tang, Owen and Gray, Michael P. and Franklin, Michael and Harper, Michael and Silk, Michael and Bobowik, Katalina and Stuckey, Alexander and Marshall, John and Bakiris, Vivian and Uren, Caitlin and Madala, Bindu Swapna and Miniter, Amy and Bartie, Caitlin and Neavin, Drew R. and Qiao, Zhen and {Ben-David}, Eyal and Chen, Ling and Farh, Kyle Kai-How and Grieve, Stuart M. and Nguyen, Tung and Piscionere, Jennifer and Siggs, Owen M. and Nicholas, Hannah and de Lange, Katrina M. and Hewitt, Alex H. and Figtree, Gemma A. and MacArthur, Daniel G. and Powell, Joseph E.},
  year          = 2025,
  month         = mar,
  pages         = {2025.03.20.25324352},
  publisher     = {medRxiv},
  doi           = {10.1101/2025.03.20.25324352},
  urldate       = {2025-12-13},
  abstract      = {Understanding the genetic basis of gene expression can shed light on the regulatory mechanisms underlying complex traits and diseases. Single-cell resolved measures of RNA levels and single-cell expression quantitative trait loci (sc-eQTLs) have revealed genetic regulation that drives sub-tissue cell states and types across diverse human tissues. Here, we describe the first phase of TenK10K, the largest- to-date dataset of matched whole-genome sequencing (WGS) and single-cell RNA-sequencing (scRNA-seq). We leverage scRNA-seq data from over 5 million cells across 28 immune cell types and matched WGS from 1,925 individuals. This provides power to detect associations between rare and low-frequency genetic variants that have largely been uncharacterised in their impact on cell-specific gene expression. We map the effects of both common and rare variants in a cell type specific manner using SAIGE-QTL. This newly developed method increases power by modelling single cells directly using a Poisson model rather than relying on aggregated `pseudobulk' counts. We identify putative common regulatory variants for 83\% of all 21,404 genes tested and cumulative rare variant signals for 47\% of genes. We explore how genetic effects vary across cell type and state spectra, develop a framework to determine the degree to which sc-eQTLs are cell type specific, and show that about half of the effects are observed only in one or a few cell types. By integrating our results with functional annotations and disease information, we further characterise the likely molecular modes of action for many disease-associated variants. Finally, we explore the effects of genetic variants on gene expression across different cell states and functions, as well as effects that directly vary cell state abundance.},
  archiveprefix = {medRxiv},
  copyright     = {\copyright{} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/I2EG6RRX/Cuomo et al. - 2025 - Impact of Rare and Common Genetic Variation on Cell Type-Specific Gene Expression.pdf}
}

@article{curtoWhatCanTopology2016,
  title        = {What Can Topology Tell Us about the Neural Code?},
  author       = {Curto, Carina},
  date         = {2016-09-27},
  journaltitle = {Bulletin of the American Mathematical Society},
  volume       = {54},
  number       = {1},
  pages        = {63--78},
  issn         = {0273-0979, 1088-9485},
  doi          = {10/f9sj4j},
  url          = {http://www.ams.org/bull/2017-54-01/S0273-0979-2016-01554-0/},
  urldate      = {2018-04-11},
  abstract     = {Neuroscience is undergoing a period of rapid experimental progress and expansion. New mathematical tools, previously unknown in the neuroscience community, are now being used to tackle fundamental questions and analyze emerging data sets. Consistent with this trend, the last decade has seen an uptick in the use of topological ideas and methods in neuroscience. In this paper I will survey recent applications of topology in neuroscience, and explain why topology is an especially natural tool for understanding neural codes.},
  langid       = {english},
  annotation   = {00000}
}

@misc{daiWhyCanGPT2023,
  title         = {Why {{Can GPT Learn In-Context}}? {{Language Models Implicitly Perform Gradient Descent}} as {{Meta-Optimizers}}},
  shorttitle    = {Why {{Can GPT Learn In-Context}}?},
  author        = {Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  year          = {2023},
  month         = may,
  number        = {arXiv:2212.10559},
  eprint        = {2212.10559},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2212.10559},
  urldate       = {2024-04-19},
  abstract      = {Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at {\textbackslash}url\{https://aka.ms/icl\}.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/ENE2L49Q/Dai et al. - 2023 - Why Can GPT Learn In-Context Language Models Impl.pdf;/Users/jkobject/Zotero/storage/P7REY2Z5/2212.html}
}

@article{dalinAbstract2710Associations2022,
  title      = {Abstract 2710: {{Associations}} between Structural Variant Signatures and Drug Sensitivity in Cell Lines},
  shorttitle = {Abstract 2710},
  author     = {Dalin, Simona and Dubois, Frank and Zhang, Shu and Crane, Alexander and Kalfon, Jeremie and Noorbakhsh, Javad and Beroukhim, Rameen},
  year       = 2022,
  month      = jun,
  journal    = {Cancer Research},
  volume     = {82},
  number     = {12\_Supplement},
  pages      = {2710},
  issn       = {0008-5472},
  doi        = {10.1158/1538-7445.AM2022-2710},
  urldate    = {2025-12-01},
  abstract   = {Structural variants (SVs) within the cancer genome reflect defects in DNA damage repair, impact gene regulation, and alter the genomic landscape. SVs impact a larger fraction of the cancer genome than any other genetic aberration, yet their effects on modulating therapeutic response have been largely uncharacterized. Patterns of rearrangements, otherwise known as signatures, vary across tumor and tissue type, and can indicate underlying biological mechanisms responsible for cancer etiology. We have extracted SV signatures from whole genome sequencing data from hundreds of cancer cell lines. Using datasets indicating the cell lines' response to over a thousand small molecule inhibitors, we have identified associations between SV signatures and response to therapeutics. These results will enhance our understanding of mechanisms underlying rearrangement signatures and nominate new biomarkers of drug response.Citation Format: Simona Dalin, Frank Dubois, Shu Zhang, Alexander Crane, Jeremie Kalfon, Javad Noorbakhsh, Rameen Beroukhim. Associations between structural variant signatures and drug sensitivity in cell lines [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2022; 2022 Apr 8-13. Philadelphia (PA): AACR; Cancer Res 2022;82(12\_Suppl):Abstract nr 2710.},
  file       = {/Users/jkobject/Zotero/storage/J2L8YR2E/1538-7445.html}
}

@misc{dalla-torreNucleotideTransformerBuilding2024,
  title         = {The {{Nucleotide Transformer}}: {{Building}} and {{Evaluating Robust Foundation Models}} for {{Human Genomics}}},
  shorttitle    = {The {{Nucleotide Transformer}}},
  author        = {{Dalla-Torre}, Hugo and Gonzalez, Liam and {Mendoza-Revilla}, Javier and Carranza, Nicolas Lopez and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and de Almeida, Bernardo P. and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
  year          = {2024},
  month         = oct,
  primaryclass  = {New Results},
  pages         = {2023.01.11.523679},
  publisher     = {bioRxiv},
  doi           = {10.1101/2023.01.11.523679},
  urldate       = {2025-02-25},
  abstract      = {Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learning between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, ranging from 50M up to 2.5B parameters and integrating information from 3,202 diverse human genomes, as well as 850 genomes selected across diverse phyla, including both model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the developed models can be fine-tuned at low cost and despite low available data regime to solve a variety of genomics applications. Despite no supervision, the transformer models learned to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence. Code and weights available on GitHub in Jax and HuggingFace in Pytorch. Example notebooks to apply these models to any downstream task are available on HuggingFace.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/PMF9VGG3/Dalla-Torre et al. - 2024 - The Nucleotide Transformer Building and Evaluatin.pdf}
}

@misc{daoFlashAttention2FasterAttention2023,
  title         = {{{FlashAttention-2}}: {{Faster Attention}} with {{Better Parallelism}} and {{Work Partitioning}}},
  shorttitle    = {{{FlashAttention-2}}},
  author        = {Dao, Tri},
  year          = {2023},
  month         = jul,
  number        = {arXiv:2307.08691},
  eprint        = {2307.08691},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2307.08691},
  urldate       = {2024-07-15},
  abstract      = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4\${\textbackslash}times\$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40{\textbackslash}\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2\${\textbackslash}times\$ speedup compared to FlashAttention, reaching 50-73{\textbackslash}\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72{\textbackslash}\% model FLOPs utilization).},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/2BCCWPP3/Dao - 2023 - FlashAttention-2 Faster Attention with Better Par.pdf;/Users/jkobject/Zotero/storage/PY2WMX86/2307.html}
}

@book{daviesSyntheticBiologyVery2018,
  title      = {Synthetic {{Biology}}: {{A Very Short Introduction}}},
  shorttitle = {Synthetic {{Biology}}},
  author     = {Davies, Jamie A.},
  year       = 2018,
  month      = jul,
  publisher  = {Oxford University Press},
  doi        = {10.1093/actrade/9780198803492.001.0001},
  urldate    = {2025-12-02},
  abstract   = {Synthetic biology is one of the 21st century's fastest growing fields of research. Building on traditional genetic engineering, synthetic biology uses multi-gene modules and pathways to make very significant changes to what cells can do. Synthetic Biology: A Very Short Introduction provides a concise explanation of what synthetic biology is, and how it is beginning to affect many fields of technology, such as new drug manufacture, biofuel production, tackling pollution, and medical diagnostics. There is also the possibility of creating new life from non-living starting materials. The considerable controversies surrounding synthetic biology are also discussed, including fears that the dangers of engineering life are worse than its benefits.},
  isbn       = {978-0-19-880349-2}
}

@unpublished{decaoMolGANImplicitGenerative2018,
  title       = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle  = {{{MolGAN}}},
  author      = {De Cao, Nicola and Kipf, Thomas},
  date        = {2018-05-30},
  eprint      = {1805.11973},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1805.11973},
  urldate     = {2019-03-22},
  abstract    = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihoodfree generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihoodbased method that directly generates graphs, albeit being susceptible to mode collapse.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{dedonnoPopulationlevelIntegrationSinglecell2023,
  title   = {Population-level integration of single-cell datasets enables multi-scale analysis across samples},
  author  = {De Donno, Carlo and others},
  journal = {Nature Methods},
  volume  = {20},
  pages   = {1683--1692},
  year    = {2023},
  doi     = {10.1038/s41592-023-02035-2}
}

@online{deeperEvaluationSinglecellFoundation2024,
  title   = {Deeper evaluation of a single-cell foundation model},
  journal = {Nature Machine Intelligence},
  year    = {2024},
  url     = {https://www.nature.com/articles/s42256-024-00949-w}
}

@article{DeepLearningGenomics2019,
  title        = {Deep Learning for Genomics},
  date         = {2019-01},
  journaltitle = {Nature Genetics},
  volume       = {51},
  number       = {1},
  pages        = {1--1},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gfsq4r},
  url          = {http://www.nature.com/articles/s41588-018-0328-0},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{defreitasCirculating70KDa2022,
  title        = {The Circulating 70~{{kDa}} Heat Shock Protein ({{HSPA1A}}) Level Is a Potential Biomarker for Breast Carcinoma and Its Progression},
  author       = {family=Freitas, given=Gabriela Boufelli, prefix=de, useprefix=true and Penteado, Laura and Miranda, Mila Meneguelli and Filassi, José Roberto and Baracat, Edmund Chada and Linhares, Iara Moreno},
  date         = {2022-07-29},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume       = {12},
  eprint       = {35906272},
  eprinttype   = {pmid},
  pages        = {13012},
  issn         = {2045-2322},
  doi          = {10.1038/s41598-022-17414-6},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9338230/},
  urldate      = {2024-07-26},
  abstract     = {The early diagnosis of breast cancer can improve treatment and prognosis. We sought to evaluate whether the serum concentration of the 70~kDa heat shock protein (HSPA1A) was elevated in Brazilian women with breast cancer, and if levels correlated with tumor characteristics. This was a cross-sectional, analytical, case–control exploratory study performed at The University of São Paulo School of Medicine. From September 2017 to December 2018, 68 women with breast cancer and 59 controls were recruited. The HSPA1A concentration in serum samples was determined by ELISA by individuals blinded to the clinical data. The mean ages in the study and control groups were 54.9 and 52.0~years, respectively. The median serum levels of HSPA1A were elevated in women with breast cancer (1037~pg/ml) compared with controls (300~pg/ml) (p\,{$<$}\,0.001). Elevated HSPA1A levels were associated with advanced histological tumor grade (p\,{$<$}\,0.001) and with the cell proliferation index (KI67) (p\,=\,0.0418). The HSPA1A concentration was similar in women with different histological subtypes, nuclear grade, hormone receptor expression, HER2 status and the presence or absence of angiolymphatic invasion. Elevated serum HSPA1A in Brazilian women with advanced histological grade and proliferation index breast cancer supports the potential value of additional investigation on larger and more varied populations to verify the value of HSPA1A detection as a component of breast cancer diagnosis and progression.},
  pmcid        = {PMC9338230},
  file         = {/Users/jkobject/Zotero/storage/PRW3JA8K/de Freitas et al. - 2022 - The circulating 70 kDa heat shock protein (HSPA1A).pdf}
}

@online{delineatingEffectiveUseSelfSupervisedLearning2024,
  title   = {Delineating the effective use of self-supervised learning in single-cell genomics},
  journal = {Nature Machine Intelligence},
  year    = {2024},
  url     = {https://www.nature.com/articles/s42256-024-00934-3}
}

@article{dengQuantumEntanglementNeural2017,
  title        = {Quantum {{Entanglement}} in {{Neural Network States}}},
  author       = {Deng, Dong-Ling and Li, Xiaopeng and Das Sarma, S.},
  date         = {2017-05-11},
  journaltitle = {Physical Review X},
  volume       = {7},
  number       = {2},
  issn         = {2160-3308},
  doi          = {10/f98bg3},
  url          = {http://link.aps.org/doi/10.1103/PhysRevX.7.021021},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/Deng et al. - 2017 - Quantum Entanglement in Neural Network States.pdf}
}

@article{deplanckeGeneCenteredElegansProteinDNA2006,
  title        = {A {{Gene-Centered C}}. Elegans {{Protein-DNA Interaction Network}}},
  author       = {Deplancke, Bart and Mukhopadhyay, Arnab and Ao, Wanyuan and Elewa, Ahmed M. and Grove, Christian A. and Martinez, Natalia J. and Sequerra, Reynaldo and Doucette-Stamm, Lynn and Reece-Hoyes, John S. and Hope, Ian A. and Tissenbaum, Heidi A. and Mango, Susan E. and Walhout, Albertha J.M.},
  date         = {2006-06},
  journaltitle = {Cell},
  volume       = {125},
  number       = {6},
  pages        = {1193--1205},
  issn         = {00928674},
  doi          = {10/d993tk},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0092867406006246},
  urldate      = {2018-04-11},
  abstract     = {Transcription regulatory networks consist of physical and functional interactions between transcription factors (TFs) and their target genes. The systematic mapping of TF-target gene interactions has been pioneered in unicellular systems, using ‘‘TF-centered’’ methods (e.g., chromatin immunoprecipitation). However, metazoan systems are less amenable to such methods. Here, we used ‘‘gene-centered’’ high-throughput yeast one-hybrid (Y1H) assays to identify 283 interactions between 72 C. elegans digestive tract gene promoters and 117 proteins. The resulting protein-DNA interaction (PDI) network is highly connected and enriched for TFs that are expressed in the digestive tract. We provide functional annotations for \$10\% of all worm TFs, many of which were previously uncharacterized, and find ten novel putative TFs, illustrating the power of a gene-centered approach. We provide additional in vivo evidence for multiple PDIs and illustrate how the PDI network provides insights into metazoan differential gene expression at a systems level.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Deplancke et al. - 2006 - A Gene-Centered C. elegans Protein-DNA Interaction.pdf}
}

@article{deplanckeGeneticsTranscriptionFactor2016,
  title        = {The {{Genetics}} of {{Transcription Factor DNA Binding Variation}}},
  author       = {Deplancke, Bart and Alpern, Daniel and Gardeux, Vincent},
  date         = {2016-07},
  journaltitle = {Cell},
  volume       = {166},
  number       = {3},
  pages        = {538--554},
  issn         = {00928674},
  doi          = {10.1016/j.cell.2016.07.012},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0092867416309187},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@online{DepMapPanCancerBiomarker,
  title        = {{{DepMap}}: Pan-{{Cancer}} Biomarker Discovery},
  shorttitle   = {{{DepMap}}},
  url          = {https://docs.google.com/presentation/u/2/d/1Uz6ZW13MwWot8yChssegNPPmknPvb0HJcLXyGHOqSIo/edit?fromCopy=true&usp=embed_facebook},
  urldate      = {2023-01-19},
  abstract     = {DepMap: pan-Cancer biomarker \& target discovery Jérémie Kalfon - Aqemia DepMap (Omics) - Link biomarkers to targets - Celligner},
  langid       = {english},
  organization = {Google Docs},
  file         = {/Users/jkobject/Zotero/storage/5H4HW5WF/DepMap pan-Cancer biomarker discovery.html}
}

@article{desaiImprovingGeneRegulatory2017,
  title        = {Improving {{Gene Regulatory Network Inference}} by {{Incorporating Rates}} of {{Transcriptional Changes}}},
  author       = {Desai, Jigar S. and Sartor, Ryan C. and Lawas, Lovely Mae and Jagadish, S. V. Krishna and Doherty, Colleen J.},
  date         = {2017-12},
  journaltitle = {Scientific Reports},
  volume       = {7},
  number       = {1},
  issn         = {2045-2322},
  doi          = {10/gfxbdt},
  url          = {http://www.nature.com/articles/s41598-017-17143-1},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Desai et al. - 2017 - Improving Gene Regulatory Network Inference by Inc.pdf}
}

@article{desilvaHomologicalSensorNetworks2007,
  title      = {Homological {{Sensor Networks}}},
  author     = {family=Silva, given=Vin, prefix=de, useprefix=true and Ghrist, Robert},
  date       = {2007},
  volume     = {54},
  number     = {1},
  pages      = {8},
  langid     = {english},
  annotation = {00000}
}

@misc{devlinBERTPretrainingDeep2019,
  title         = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle    = {{{BERT}}},
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year          = {2019},
  month         = may,
  number        = {arXiv:1810.04805},
  eprint        = {1810.04805},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1810.04805},
  urldate       = {2024-04-19},
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/ISPFBRRW/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/jkobject/Zotero/storage/MG3VAX9F/1810.html}
}

@misc{deWaeleSystematicAssessment2025,
  title        = {A Systematic Assessment of Single-Cell Language Model Configurations},
  author       = {De Waele, Gilles and Menschaert, Gerben and Waegeman, Willem},
  year         = {2025},
  howpublished = {bioRxiv},
  doi          = {10.1101/2025.04.02.646825},
  url          = {https://doi.org/10.1101/2025.04.02.646825},
  note         = {Preprint}
}

@article{dhaeseleerGeneticNetworkInference2000,
  title        = {Genetic Network Inference: From Co-Expression Clustering to Reverse Engineering},
  shorttitle   = {Genetic Network Inference},
  author       = {D'haeseleer, P. and Liang, S. and Somogyi, R.},
  date         = {2000-08-01},
  journaltitle = {Bioinformatics},
  volume       = {16},
  number       = {8},
  pages        = {707--726},
  issn         = {1367-4803, 1460-2059},
  doi          = {10/fnhp28},
  url          = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/16.8.707},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/D'haeseleer et al. - 2000 - Genetic network inference from co-expression clus.pdf}
}

@misc{dhuliawalaVariationalClassification2024,
  title         = {Variational {{Classification}}},
  author        = {Dhuliawala, Shehzaad and Sachan, Mrinmaya and Allen, Carl},
  year          = {2024},
  month         = jan,
  number        = {arXiv:2305.10406},
  eprint        = {2305.10406},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2305.10406},
  urldate       = {2024-07-19},
  abstract      = {We present a latent variable model for classification that provides a novel probabilistic interpretation of neural network softmax classifiers. We derive a variational objective to train the model, analogous to the evidence lower bound (ELBO) used to train variational auto-encoders, that generalises the softmax cross-entropy loss. Treating inputs to the softmax layer as samples of a latent variable, our abstracted perspective reveals a potential inconsistency between their anticipated distribution, required for accurate label predictions, and their empirical distribution found in practice. We augment the variational objective to mitigate such inconsistency and induce a chosen latent distribution, instead of the implicit assumption found in a standard softmax layer. Overall, we provide new theoretical insight into the inner workings of widely-used softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates that our proposed approach, variational classification, maintains classification accuracy while the reshaped latent space improves other desirable properties of a classifier, such as calibration, adversarial robustness, robustness to distribution shift and sample efficiency useful in low data settings.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/GRZ7XPJA/Dhuliawala et al. - 2024 - Variational Classification.pdf;/Users/jkobject/Zotero/storage/Y2I9YH35/2305.html}
}

@article{diasDownregulationMetallothionein2A2022,
  title        = {Downregulation of Metallothionein {{2A}} Reduces Migration, Invasion and Proliferation Activities in Human Squamous Cell Carcinoma Cells},
  author       = {Dias, Aline Marques and family=Mendonça, given=Raíssa Pinheiro, prefix=de, useprefix=true and family=Silva Kataoka, given=Maria Sueli, prefix=da, useprefix=true and Jaeger, Ruy G. and family=Jesus Viana Pinheiro, given=João, prefix=de, useprefix=true and family=Melo Alves Junior, given=Sérgio, prefix=de, useprefix=true},
  date         = {2022-05},
  journaltitle = {Molecular Biology Reports},
  shortjournal = {Mol Biol Rep},
  volume       = {49},
  number       = {5},
  eprint       = {35107738},
  eprinttype   = {pmid},
  pages        = {3665--3674},
  issn         = {1573-4978},
  doi          = {10.1007/s11033-022-07206-6},
  abstract     = {BACKGROUND: The invasive behaviour of squamous cell carcinoma (SCC), a common malignant tumour of the mouth, is a process mediated by cell proliferation, extracellular matrix proteolysis and other factors. Studies have shown a potential relationship between growth factors, metallothionein 2A (MT2A) and matrix metalloproteinase (MMP) activation in malignant tumours. The aim of this study was to downregulate MT2A in cells (Cal27) derived from human squamous cell carcinoma. METHODS: Cal27 cells with reduced MT2A were subjected to proliferation, migration and invasion assays. Immunofluorescence and western blot confirmed MT2A depletion by siRNA. Growth curve assays assessed cell proliferation. Indirect immunofluorescence analysed the expression of MT2A, MMP-2, MMP-9, epidermal growth factor (EGF), transforming growth factor alpha (TGF-α), tumour necrosis factor alpha (TNF-α) and Ki67. Zymography evaluated the effects of MT2A silencing on MMP-2 and -9 expression. Migration and invasion activities were evaluated using migration and invasion assays. RESULTS: CAL27 cells displayed MT2A, MMP-2, MMP-9, EGF, TGF-α, TNF-α and Ki67. MT2A depletion decreased MMP-9, EGF, TGF-α and Ki67 protein levels, while increasing TNF-α. CONCLUSIONS: MT2A downregulation reduced cell proliferation, migration and invasion activities. Therefore, MT2A has an important role in cell proliferation, migration and invasion in human oral SCC cells.},
  langid       = {english},
  keywords     = {Carcinoma Squamous Cell,Cell Line Tumor,Cell Movement,Cell Proliferation,Down-Regulation,Epidermal Growth Factor,Humans,Ki-67 Antigen,Matrix Metalloproteinase 2,Matrix Metalloproteinase 9,Metallothionein,Mouth neoplasms,Squamous cell carcinoma,Transforming Growth Factor alpha,Tumor Necrosis Factor-alpha,Tumour}
}

@article{dibaeiniaSERGIOSingleCellExpression2020,
  title      = {{{SERGIO}}: {{A Single-Cell Expression Simulator Guided}} by {{Gene Regulatory Networks}}},
  shorttitle = {{{SERGIO}}},
  author     = {Dibaeinia, Payam and Sinha, Saurabh},
  year       = {2020},
  month      = sep,
  journal    = {Cell Systems},
  volume     = {11},
  number     = {3},
  pages      = {252-271.e11},
  issn       = {2405-4712},
  doi        = {10.1016/j.cels.2020.08.003},
  urldate    = {2025-02-25},
  abstract   = {A common approach to benchmarking of single-cell transcriptomics tools is to generate synthetic datasets that statistically resemble experimental data. However, most existing single-cell simulators do not incorporate transcription factor-gene regulatory interactions that underlie expression dynamics. Here, we present SERGIO, a simulator of single-cell gene expression data that models the stochastic nature of transcription as well as regulation of genes by multiple transcription factors according to a user-provided gene regulatory network. SERGIO can simulate any number of cell types in steady state or cells differentiating to multiple fates. We show that datasets generated by SERGIO are statistically comparable to experimental data generated by Illumina HiSeq2000, Drop-seq, Illumina 10X chromium, and Smart-seq. We use SERGIO to benchmark several single-cell analysis tools, including GRN inference methods, and identify Tcf7, Gata3, and Bcl11b as key drivers of T~cell differentiation by performing in silico knockout experiments. SERGIO is freely available for download here: https://github.com/PayamDiba/SERGIO.},
  keywords   = {benchmarking single-cell analysis tools,differentiation trajectories,gene regulatory networks,RNA velocity,simulations,single-cell RNA-seq},
  file       = {/Users/jkobject/Zotero/storage/PKLKBHRI/Dibaeinia and Sinha - 2020 - SERGIO A Single-Cell Expression Simulator Guided .pdf;/Users/jkobject/Zotero/storage/VEZFXFK8/S2405471220302878.html}
}

@article{dicarloHowDoesBrain2012,
  title        = {How {{Does}} the {{Brain Solve Visual Object Recognition}}?},
  author       = {DiCarlo, James~J. and Zoccolan, Davide and Rust, Nicole~C.},
  date         = {2012-02},
  journaltitle = {Neuron},
  volume       = {73},
  number       = {3},
  pages        = {415--434},
  issn         = {08966273},
  doi          = {10/gcsgvm},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S089662731200092X},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Dropbox/Journal Club/DiCarlo et al. - 2012 - How Does the Brain Solve Visual Object Recognition.pdf}
}


@article{dicarloUntanglingInvariantObject2007,
  title        = {Untangling Invariant Object Recognition},
  author       = {DiCarlo, James J. and Cox, David D.},
  date         = {2007-08},
  journaltitle = {Trends in Cognitive Sciences},
  volume       = {11},
  number       = {8},
  pages        = {333--341},
  issn         = {13646613},
  doi          = {10/bb6268},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S1364661307001593},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Dropbox/Journal Club/DiCarlo et Cox - 2007 - Untangling invariant object recognition.pdf}
}

@article{diehlCellOntology20162016,
  title      = {The {{Cell Ontology}} 2016: Enhanced Content, Modularization, and Ontology Interoperability},
  shorttitle = {The {{Cell Ontology}} 2016},
  author     = {Diehl, Alexander D. and Meehan, Terrence F. and Bradford, Yvonne M. and Brush, Matthew H. and Dahdul, Wasila M. and Dougall, David S. and He, Yongqun and {Osumi-Sutherland}, David and Ruttenberg, Alan and Sarntivijai, Sirarat and Van Slyke, Ceri E. and Vasilevsky, Nicole A. and Haendel, Melissa A. and Blake, Judith A. and Mungall, Christopher J.},
  year       = {2016},
  month      = jul,
  journal    = {Journal of Biomedical Semantics},
  volume     = {7},
  number     = {1},
  pages      = {44},
  issn       = {2041-1480},
  doi        = {10.1186/s13326-016-0088-7},
  urldate    = {2024-07-19},
  abstract   = {The Cell Ontology (CL) is an OBO Foundry candidate ontology covering the domain of canonical, natural biological cell types. Since its inception in 2005, the CL has undergone multiple rounds of revision and expansion, most notably in its representation of hematopoietic cells. For in vivo cells, the CL focuses on vertebrates but provides general classes that can be used for other metazoans, which can be subtyped in species-specific ontologies.},
  keywords   = {Anatomy Ontology,Cell Line Cell,Gene Ontology,Logical Definition,Neuroscience Information Framework},
  file       = {/Users/jkobject/Zotero/storage/PAQ24VZV/Diehl et al. - 2016 - The Cell Ontology 2016 enhanced content, modulari.pdf;/Users/jkobject/Zotero/storage/RPSEVUHZ/s13326-016-0088-7.html}
}

@article{dijkRecoveringGeneInteractions2018,
  title     = {Recovering {{Gene Interactions}} from {{Single-Cell Data Using Data Diffusion}}},
  author    = {van Dijk, David and Sharma, Roshan and Nainys, Juozas and Yim, Kristina and Kathail, Pooja and Carr, Ambrose J. and Burdziak, Cassandra and Moon, Kevin R. and Chaffer, Christine L. and Pattabiraman, Diwakar and Bierie, Brian and Mazutis, Linas and Wolf, Guy and Krishnaswamy, Smita and Pe'er, Dana},
  year      = {2018},
  month     = jul,
  journal   = {Cell},
  volume    = {174},
  number    = {3},
  pages     = {716-729.e27},
  publisher = {Elsevier},
  issn      = {0092-8674, 1097-4172},
  doi       = {10.1016/j.cell.2018.05.061},
  urldate   = {2024-07-15},
  abstract  = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}p{$>$}Single-cell RNA sequencing technologies suffer from many sources of technical noise, including under-sampling of mRNA molecules, often termed "dropout," which can severely obscure important gene-gene relationships. To address this, we developed MAGIC (Markov affinity-based graph imputation of cells), a method that shares information across similar cells, via data diffusion, to denoise the cell count matrix and fill in missing transcripts. We validate MAGIC on several biological systems and find it effective at recovering gene-gene relationships and additional structures. Applied to the epithilial to mesenchymal transition, MAGIC reveals a phenotypic continuum, with the majority of cells residing in intermediate states that display stem-like signatures, and infers known and previously uncharacterized regulatory interactions, demonstrating that our approach can successfully uncover regulatory relations without perturbations.{$<$}/p{$>$}},
  langid    = {english},
  pmid      = {29961576},
  file      = {/Users/jkobject/Zotero/storage/NRVNC93B/Dijk et al. - 2018 - Recovering Gene Interactions from Single-Cell Data.pdf}
}

@misc{dingPrivacypreservingPredictiveFoundation2025,
  title  = {Toward a privacy-preserving predictive foundation model of single-cell transcriptomics with federated learning and tabular modeling},
  author = {Ding, Jiarui and others},
  year   = {2025},
  eprint = {2025.01.06.631427},
  doi    = {10.1101/2025.01.06.631427}
}

@article{dixitPerturbseqDissectingMolecular2016,
  title      = {Perturb-Seq: {{Dissecting}} Molecular Circuits with Scalable Single Cell {{RNA}} Profiling of Pooled Genetic Screens},
  shorttitle = {Perturb-Seq},
  author     = {Dixit, Atray and Parnas, Oren and Li, Biyu and Chen, Jenny and Fulco, Charles P. and {Jerby-Arnon}, Livnat and Marjanovic, Nemanja D. and Dionne, Danielle and Burks, Tyler and Raychndhury, Raktima and Adamson, Britt and Norman, Thomas M. and Lander, Eric S. and Weissman, Jonathan S. and Friedman, Nir and Regev, Aviv},
  year       = {2016},
  month      = dec,
  journal    = {Cell},
  volume     = {167},
  number     = {7},
  pages      = {1853-1866.e17},
  issn       = {0092-8674},
  doi        = {10.1016/j.cell.2016.11.038},
  urldate    = {2024-07-19},
  abstract   = {Genetic screens help infer gene function in mammalian cells, but it has remained difficult to assay complex phenotypes -- such as transcriptional profiles -- at scale. Here, we develop Perturb-seq, combining single cell RNA-seq and CRISPR based perturbations to perform many such assays in a pool. We demonstrate Perturb-seq by analyzing 200,000 cells in immune cells and cell lines, focusing on transcription factors regulating the response of dendritic cells to lipopolysaccharide (LPS). Perturb-seq accurately identifies individual gene targets, gene signatures, and cell states affected by individual perturbations and their genetic interactions. We posit new functions for regulators of differentiation, the anti-viral response, and mitochondrial function during immune activation. By decomposing many high content measurements into the effects of perturbations, their interactions, and diverse cell metadata, Perturb-seq dramatically increases the scope of pooled genomic assays.,},
  pmcid      = {PMC5181115},
  pmid       = {27984732},
  file       = {/Users/jkobject/Zotero/storage/QJHMNZM6/Dixit et al. - 2016 - Perturb-seq Dissecting molecular circuits with sc.pdf}
}

@article{dobinSTARUltrafastUniversal2013,
  title      = {{{STAR}}: Ultrafast Universal {{RNA-seq}} Aligner},
  shorttitle = {{{STAR}}},
  author     = {Dobin, Alexander and Davis, Carrie A. and Schlesinger, Felix and Drenkow, Jorg and Zaleski, Chris and Jha, Sonali and Batut, Philippe and Chaisson, Mark and Gingeras, Thomas R.},
  year       = 2013,
  month      = jan,
  journal    = {Bioinformatics},
  volume     = {29},
  number     = {1},
  pages      = {15--21},
  issn       = {1367-4803},
  doi        = {10.1093/bioinformatics/bts635},
  urldate    = {2025-12-02},
  abstract   = {Motivation: Accurate alignment of high-throughput RNA-seq data is a challenging and yet unsolved problem because of the non-contiguous transcript structure, relatively short read lengths and constantly increasing throughput of the sequencing technologies. Currently available RNA-seq aligners suffer from high mapping error rates, low mapping speed, read length limitation and mapping biases.Results: To align our large (\&gt;80 billon reads) ENCODE Transcriptome RNA-seq dataset, we developed the Spliced Transcripts Alignment to a Reference (STAR) software based on a previously undescribed RNA-seq alignment algorithm that uses sequential maximum mappable seed search in uncompressed suffix arrays followed by seed clustering and stitching procedure. STAR outperforms other aligners by a factor of \&gt;50 in mapping speed, aligning to the human genome 550 million 2 \texttimes{} 76 bp paired-end reads per hour on a modest 12-core server, while at the same time improving alignment sensitivity and precision. In addition to unbiased de novo detection of canonical junctions, STAR can discover non-canonical splices and chimeric (fusion) transcripts, and is also capable of mapping full-length RNA sequences. Using Roche 454 sequencing of reverse transcription polymerase chain reaction amplicons, we experimentally validated 1960 novel intergenic splice junctions with an 80--90\% success rate, corroborating the high precision of the STAR mapping strategy.Availability and implementation: STAR is implemented as a standalone C++ code. STAR is free open source software distributed under GPLv3 license and can be downloaded from http://code.google.com/p/rna-star/.Contact: ~dobin@cshl.edu.},
  file       = {/Users/jkobject/Zotero/storage/ZU44ZW8E/Dobin et al. - 2013 - STAR ultrafast universal RNA-seq aligner.pdf;/Users/jkobject/Zotero/storage/X59J8NFR/bts635.html}
}

@article{doWhatExpectationMaximization2008,
  title        = {What Is the Expectation Maximization Algorithm?},
  author       = {Do, Chuong B and Batzoglou, Serafim},
  date         = {2008-08},
  journaltitle = {Nature Biotechnology},
  volume       = {26},
  number       = {8},
  pages        = {897--899},
  issn         = {1087-0156, 1546-1696},
  doi          = {10/gt6},
  url          = {http://www.nature.com/articles/nbt1406},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{duanOneShotImitationLearning,
  title      = {One-{{Shot Imitation Learning}}},
  author     = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  pages      = {23},
  abstract   = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a metalearning framework for achieving such capability, which we call one-shot imitation learning.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Duan et al. - One-Shot Imitation Learning.pdf}
}

@article{duanRL2FASTREINFORCEMENT2017,
  title      = {{{RL2}}: {{FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING}}},
  author     = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter and family=Berkeley, given=UC, given-i=UC},
  date       = {2017},
  pages      = {18},
  abstract   = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a “fast” reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL2, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose (“slow”) RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the “fast” RL algorithm on the current (previously unseen) MDP. We evaluate RL2 experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-armed bandit problems and finite MDPs. After RL2 is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the largescale side, we test RL2 on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/Duan et al. - 2017 - RL2 FAST REINFORCEMENT LEARNING VIA SLOW REINFORC.pdf}
}


@article{dunhamIntegratedEncyclopedia,
  title     = {An Integrated Encyclopedia of {{DNA}} Elements in the Human Genome},
  author    = {Dunham, Ian and Kundaje, Anshul and Aldred, Shelley F. and Collins, Patrick J. and Davis, Carrie A. and Doyle, Francis and Epstein, Charles B. and Frietze, Seth and Harrow, Jennifer and Kaul, Rajinder and Khatun, Jainab and Lajoie, Bryan R. and Landt, Stephen G. and Lee, Bum-Kyu and Pauli, Florencia and Rosenbloom, Kate R. and Sabo, Peter and Safi, Alexias and Sanyal, Amartya and Shoresh, Noam and Simon, Jeremy M. and Song, Lingyun and Trinklein, Nathan D. and Altshuler, Robert C. and Birney, Ewan and Brown, James B. and Cheng, Chao and Djebali, Sarah and Dong, Xianjun and Dunham, Ian and Ernst, Jason and Furey, Terrence S. and Gerstein, Mark and Giardine, Belinda and Greven, Melissa and Hardison, Ross C. and Harris, Robert S. and Herrero, Javier and Hoffman, Michael M. and Iyer, Sowmya and Kellis, Manolis and Khatun, Jainab and Kheradpour, Pouya and Kundaje, Anshul and Lassmann, Timo and Li, Qunhua and Lin, Xinying and Marinov, Georgi K. and Merkel, Angelika and Mortazavi, Ali and Parker, Stephen C. J. and Reddy, Timothy E. and Rozowsky, Joel and Schlesinger, Felix and Thurman, Robert E. and Wang, Jie and Ward, Lucas D. and Whitfield, Troy W. and Wilder, Steven P. and Wu, Weisheng and Xi, Hualin S. and Yip, Kevin Y. and Zhuang, Jiali and Bernstein, Bradley E. and Birney, Ewan and Dunham, Ian and Green, Eric D. and Gunter, Chris and Snyder, Michael and Pazin, Michael J. and Lowdon, Rebecca F. and Dillon, Laura A. L. and Adams, Leslie B. and Kelly, Caroline J. and Zhang, Julia and Wexler, Judith R. and Green, Eric D. and Good, Peter J. and Feingold, Elise A. and Bernstein, Bradley E. and Birney, Ewan and Crawford, Gregory E. and Dekker, Job and Elnitski, Laura and Farnham, Peggy J. and Gerstein, Mark and Giddings, Morgan C. and Gingeras, Thomas R. and Green, Eric D. and Guig{\'o}, Roderic and Hardison, Ross C. and Hubbard, Timothy J. and Kellis, Manolis and Kent, W. James and Lieb, Jason D. and Margulies, Elliott H. and Myers, Richard M. and Snyder, Michael and Stamatoyannopoulos, John A. and Tenenbaum, Scott A. and Weng, Zhiping and White, Kevin P. and Wold, Barbara and Khatun, Jainab and Yu, Yanbao and Wrobel, John and Risk, Brian A. and Gunawardena, Harsha P. and Kuiper, Heather C. and Maier, Christopher W. and Xie, Ling and Chen, Xian and Giddings, Morgan C. and Bernstein, Bradley E. and Epstein, Charles B. and Shoresh, Noam and Ernst, Jason and Kheradpour, Pouya and Mikkelsen, Tarjei S. and Gillespie, Shawn and Goren, Alon and Ram, Oren and Zhang, Xiaolan and Wang, Li and Issner, Robbyn and Coyne, Michael J. and Durham, Timothy and Ku, Manching and Truong, Thanh and Ward, Lucas D. and Altshuler, Robert C. and Eaton, Matthew L. and Kellis, Manolis and Djebali, Sarah and Davis, Carrie A. and Merkel, Angelika and Dobin, Alex and Lassmann, Timo and Mortazavi, Ali and Tanzer, Andrea and Lagarde, Julien and Lin, Wei and Schlesinger, Felix and Xue, Chenghai and Marinov, Georgi K. and Khatun, Jainab and Williams, Brian A. and Zaleski, Chris and Rozowsky, Joel and R{\"o}der, Maik and Kokocinski, Felix and Abdelhamid, Rehab F. and Alioto, Tyler and Antoshechkin, Igor and Baer, Michael T. and Batut, Philippe and Bell, Ian and Bell, Kimberly and Chakrabortty, Sudipto and Chen, Xian and Chrast, Jacqueline and Curado, Joao and Derrien, Thomas and Drenkow, Jorg and Dumais, Erica and Dumais, Jackie and Duttagupta, Radha and Fastuca, Megan and {Fejes-Toth}, Kata and Ferreira, Pedro and Foissac, Sylvain and Fullwood, Melissa J. and Gao, Hui and Gonzalez, David and Gordon, Assaf and Gunawardena, Harsha P. and Howald, C{\'e}dric and Jha, Sonali and Johnson, Rory and Kapranov, Philipp and King, Brandon and Kingswood, Colin and Li, Guoliang and Luo, Oscar J. and Park, Eddie and Preall, Jonathan B. and Presaud, Kimberly and Ribeca, Paolo and Risk, Brian A. and Robyr, Daniel and Ruan, Xiaoan and Sammeth, Michael and Sandhu, Kuljeet Singh and Schaeffer, Lorain and See, Lei-Hoon and Shahab, Atif and Skancke, Jorgen and Suzuki, Ana Maria and Takahashi, Hazuki and Tilgner, Hagen and Trout, Diane and Walters, Nathalie and Wang, Huaien and Wrobel, John and Yu, Yanbao and Hayashizaki, Yoshihide and Harrow, Jennifer and Gerstein, Mark and Hubbard, Timothy J. and Reymond, Alexandre and Antonarakis, Stylianos E. and Hannon, Gregory J. and Giddings, Morgan C. and Ruan, Yijun and Wold, Barbara and Carninci, Piero and Guig{\'o}, Roderic and Gingeras, Thomas R. and Rosenbloom, Kate R. and Sloan, Cricket A. and Learned, Katrina and Malladi, Venkat S. and Wong, Matthew C. and Barber, Galt P. and Cline, Melissa S. and Dreszer, Timothy R. and Heitner, Steven G. and Karolchik, Donna and Kent, W. James and Kirkup, Vanessa M. and Meyer, Laurence R. and Long, Jeffrey C. and Maddren, Morgan and Raney, Brian J. and Furey, Terrence S. and Song, Lingyun and Grasfeder, Linda L. and Giresi, Paul G. and Lee, Bum-Kyu and Battenhouse, Anna and Sheffield, Nathan C. and Simon, Jeremy M. and Showers, Kimberly A. and Safi, Alexias and London, Darin and Bhinge, Akshay A. and Shestak, Christopher and Schaner, Matthew R. and Ki Kim, Seul and Zhang, Zhuzhu Z. and Mieczkowski, Piotr A. and Mieczkowska, Joanna O. and Liu, Zheng and McDaniell, Ryan M. and Ni, Yunyun and Rashid, Naim U. and Kim, Min Jae and Adar, Sheera and Zhang, Zhancheng and Wang, Tianyuan and Winter, Deborah and Keefe, Damian and Birney, Ewan and Iyer, Vishwanath R. and Lieb, Jason D. and Crawford, Gregory E. and Li, Guoliang and Sandhu, Kuljeet Singh and Zheng, Meizhen and Wang, Ping and Luo, Oscar J. and Shahab, Atif and Fullwood, Melissa J. and Ruan, Xiaoan and Ruan, Yijun and Myers, Richard M. and Pauli, Florencia and Williams, Brian A. and Gertz, Jason and Marinov, Georgi K. and Reddy, Timothy E. and Vielmetter, Jost and Partridge, E. and Trout, Diane and Varley, Katherine E. and Gasper, Clarke and {The ENCODE Project Consortium} and {Overall coordination (data analysis coordination)} and {Data production leads (data production)} and {Lead analysts (data analysis)} and {Writing group} and {NHGRI project management (scientific management)} and {Principal investigators (steering committee)} and {Boise State University and University of North Carolina at Chapel Hill Proteomics groups (data production and analysis)} and {Broad Institute Group (data production and analysis)} and Cold Spring Harbor, Center for Genomic Regulation, Barcelona, RIKEN, Sanger Institute, University of Lausanne, Genome Institute of Singapore group (data production {and} analysis), University of Geneva and {Data coordination center at UC Santa Cruz (production data coordination)} and Duke University, University of Texas, Austin, University of North Carolina-Chapel Hill group (data production {and} analysis), {\relax EBI} and {Genome Institute of Singapore group (data production and analysis)} and HudsonAlpha Institute, UC Irvine, Stanford group (data production {and} analysis), Caltech},
  year      = {2012},
  month     = sep,
  journal   = {Nature},
  volume    = {489},
  number    = {7414},
  pages     = {57--74},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/nature11247},
  urldate   = {2024-07-23},
  abstract  = {The human genome encodes the blueprint of life, but the function of the vast majority of its nearly three billion bases is unknown. The Encyclopedia of DNA Elements (ENCODE) project has systematically mapped regions of transcription, transcription factor association, chromatin structure and histone modification. These data enabled us to assign biochemical functions for 80\% of the genome, in particular outside of the well-studied protein-coding regions. Many discovered candidate regulatory elements are physically associated with one another and with expressed genes, providing new insights into the mechanisms of gene regulation. The newly identified elements also show a statistical correspondence to sequence variants linked to human disease, and can thereby guide interpretation of this variation. Overall, the project provides new insights into the organization and regulation of our genes and genome, and is an expansive resource of functional annotations for biomedical research.},
  copyright = {2012 The Author(s)},
  langid    = {english},
  keywords  = {Functional genomics,Genetic variation,Genome-wide association studies,Molecular biology},
  file      = {/Users/jkobject/Zotero/storage/WX68TTL7/Dunham et al. - 2012 - An integrated encyclopedia of DNA elements in the .pdf}
}

@article{durbinSelectiveGeneDependencies2018,
  title        = {Selective Gene Dependencies in {{MYCN-amplified}} Neuroblastoma Include the Core Transcriptional Regulatory Circuitry},
  author       = {Durbin, Adam D. and Zimmerman, Mark W. and Dharia, Neekesh V. and Abraham, Brian J. and Iniguez, Amanda Balboni and Weichert-Leahey, Nina and He, Shuning and Krill-Burger, John M. and Root, David E. and Vazquez, Francisca and Tsherniak, Aviad and Hahn, William C. and Golub, Todd R. and Young, Richard A. and Look, A. Thomas and Stegmaier, Kimberly},
  date         = {2018-09},
  journaltitle = {Nature Genetics},
  volume       = {50},
  number       = {9},
  pages        = {1240--1246},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gd2zq3},
  url          = {http://www.nature.com/articles/s41588-018-0191-z},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{duReservoirComputingUsing2017,
  title        = {Reservoir Computing Using Dynamic Memristors for Temporal Information Processing},
  author       = {Du, Chao and Cai, Fuxi and Zidan, Mohammed A. and Ma, Wen and Lee, Seung Hwan and Lu, Wei D.},
  date         = {2017-12},
  journaltitle = {Nature Communications},
  volume       = {8},
  number       = {1},
  issn         = {2041-1723},
  doi          = {10/gcrktb},
  url          = {http://www.nature.com/articles/s41467-017-02337-y},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/spiking/Du et al. - 2017 - Reservoir computing using dynamic memristors for t.pdf}
}

@misc{dwivediGeneralizationTransformerNetworks2021,
  title         = {A {{Generalization}} of {{Transformer Networks}} to {{Graphs}}},
  author        = {Dwivedi, Vijay Prakash and Bresson, Xavier},
  year          = 2021,
  month         = jan,
  number        = {arXiv:2012.09699},
  eprint        = {2012.09699},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2012.09699},
  urldate       = {2025-12-12},
  abstract      = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/HFA4ZYSY/Dwivedi and Bresson - 2021 - A Generalization of Transformer Networks to Graphs.pdf;/Users/jkobject/Zotero/storage/K8ZTT29X/2012.html}
}

@misc{EBISPOTEfo2024,
  title        = {{{EBISPOT}}/Efo},
  year         = {2024},
  month        = jul,
  urldate      = {2024-07-19},
  abstract     = {Github repo for the Experimental Factor Ontology (EFO)},
  howpublished = {EBISPOT}
}

@online{EffectPHExtracellular,
  title   = {The {{Effect}} of {{pH}} on the {{Extracellular Matrix}} and {{Biofilms}} - {{PubMed}}},
  url     = {https://pubmed.ncbi.nlm.nih.gov/26155386/},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/52792XNS/26155386.html}
}

@misc{efficientft,
  title         = {Parameter-{{Efficient Transfer Learning}} for {{NLP}}},
  author        = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  year          = {2019},
  month         = jun,
  number        = {arXiv:1902.00751},
  eprint        = {1902.00751},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1902.00751},
  urldate       = {2025-02-25},
  abstract      = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/899BTETK/Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf;/Users/jkobject/Zotero/storage/MNRET6YQ/1902.html}
}

@online{EnrichrComprehensiveGene,
  title   = {Enrichr: A Comprehensive Gene Set Enrichment Analysis Web Server 2016 Update - {{PMC}}},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4987924/},
  urldate = {2024-07-25},
  file    = {/Users/jkobject/Zotero/storage/HUGDSW3F/PMC4987924.html}
}

@misc{EpidemiologyClinicalBenign,
  title        = {Epidemiology of Clinical Benign Prostatic Hyperplasia - {{PMC}}},
  urldate      = {2024-07-26},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5717991/},
  file         = {/Users/jkobject/Zotero/storage/QYP5UICM/PMC5717991.html}
}

@article{eraslanSinglecellRNAseqDenoising2019,
  title     = {Single-Cell {{RNA-seq}} Denoising Using a Deep Count Autoencoder},
  author    = {Eraslan, G{\"o}kcen and Simon, Lukas M. and Mircea, Maria and Mueller, Nikola S. and Theis, Fabian J.},
  year      = {2019},
  month     = jan,
  journal   = {Nature Communications},
  volume    = {10},
  number    = {1},
  pages     = {390},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-018-07931-2},
  urldate   = {2024-07-15},
  abstract  = {Single-cell RNA sequencing (scRNA-seq) has enabled researchers to study gene expression at a cellular resolution. However, noise due to amplification and dropout may obstruct analyses, so scalable denoising methods for increasingly large but sparse scRNA-seq data are needed. We propose a deep count autoencoder network (DCA) to denoise scRNA-seq datasets. DCA takes the count distribution, overdispersion and sparsity of the data into account using a negative binomial noise model with or without zero-inflation, and nonlinear gene-gene dependencies are captured. Our method scales linearly with the number of cells and can, therefore, be applied to datasets of millions of cells. We demonstrate that DCA denoising improves a diverse set of typical scRNA-seq data analyses using simulated and real datasets. DCA outperforms existing methods for data imputation in quality and speed, enhancing biological discovery.},
  copyright = {2019 The Author(s)},
  langid    = {english},
  keywords  = {Computational models,Machine learning,Statistical methods},
  file      = {/Users/jkobject/Zotero/storage/TVJQUYXC/Eraslan et al. - 2019 - Single-cell RNA-seq denoising using a deep count a.pdf}
}

@misc{esm2,
  title         = {Transformer Protein Language Models Are Unsupervised Structure Learners},
  author        = {Rao, Roshan and Meier, Joshua and Sercu, Tom and Ovchinnikov, Sergey and Rives, Alexander},
  year          = {2020},
  month         = dec,
  primaryclass  = {New Results},
  pages         = {2020.12.15.422761},
  publisher     = {bioRxiv},
  doi           = {10.1101/2020.12.15.422761},
  urldate       = {2025-02-25},
  abstract      = {Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.1},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/QKXZAEJI/Rao et al. - 2020 - Transformer protein language models are unsupervis.pdf}
}

@misc{esmfold,
  title         = {Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction},
  author        = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Costa, Allan dos Santos and {Fazel-Zarandi}, Maryam and Sercu, Tom and Candido, Sal and Rives, Alexander},
  year          = {2022},
  month         = jul,
  primaryclass  = {New Results},
  pages         = {2022.07.20.500902},
  publisher     = {bioRxiv},
  doi           = {10.1101/2022.07.20.500902},
  urldate       = {2024-07-15},
  abstract      = {Large language models have recently been shown to develop emergent capabilities with scale, going beyond simple pattern matching to perform higher level reasoning and generate lifelike images and text. While language models trained on protein sequences have been studied at a smaller scale, little is known about what they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters, the largest language models of proteins to be evaluated to date. We find that as models are scaled they learn information enabling the prediction of the three-dimensional structure of a protein at the resolution of individual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly from the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for sequences with low perplexity that are well understood by the language model. ESMFold inference is an order of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic proteins in practical timescales.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/4TJ7U3I2/Lin et al. - 2022 - Language models of protein sequences at the scale .pdf}
}

@unpublished{espeholtIMPALAScalableDistributed2018,
  title       = {{{IMPALA}}: {{Scalable Distributed Deep-RL}} with {{Importance Weighted Actor-Learner Architectures}}},
  shorttitle  = {{{IMPALA}}},
  author      = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  date        = {2018-02-05},
  eprint      = {1802.01561},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1802.01561},
  urldate     = {2019-03-22},
  abstract    = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation  = {00000}
}

@article{estevaGuideDeepLearning2019,
  title        = {A Guide to Deep Learning in Healthcare},
  author       = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
  date         = {2019-01},
  journaltitle = {Nature Medicine},
  volume       = {25},
  number       = {1},
  pages        = {24--29},
  issn         = {1078-8956, 1546-170X},
  doi          = {10.1038/s41591-018-0316-z},
  url          = {http://www.nature.com/articles/s41591-018-0316-z},
  urldate      = {2019-03-22},
  langid       = {english}
}

@misc{evansGraphStructuredNeural2024,
  title         = {Graph {{Structured Neural Networks}} for {{Perturbation Biology}}},
  author        = {Evans, Nathaniel J. and Mills, Gordon B. and Wu, Guanming and Song, Xubo and McWeeney, Shannon},
  year          = {2024},
  month         = feb,
  primaryclass  = {New Results},
  pages         = {2024.02.28.582164},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.02.28.582164},
  urldate       = {2024-04-19},
  abstract      = {1 Abstract Computational modeling of perturbation biology identifies relationships between molecular elements and cellular response, and an accurate understanding of these systems will support the full realization of precision medicine. Traditional deep learning, while often accurate in predicting response, is unlikely to capture the true sequence of involved molecular interactions. Our work is motivated by two assumptions: 1) Methods that encourage mechanistic prediction logic are likely to be more trustworthy, and 2) problem-specific algorithms are likely to outperform generic algorithms. We present an alternative to Graph Neural Networks (GNNs) termed Graph Structured Neural Networks (GSNN), which uses cell signaling knowledge, encoded as a graph data structure, to add inductive biases to deep learning. We apply our method to perturbation biology using the LINCS L1000 dataset and literature-curated molecular interactions. We demonstrate that GSNNs outperform baseline algorithms in several prediction tasks, including 1) perturbed expression, 2) cell viability of drug combinations, and 3) disease-specific drug prioritization. We also present a method called GSNNExplainer to explain GSNN predictions in a biologically interpretable form. This work has broad application in basic biological research and pre-clincal drug repurposing. Further refinement of these methods may produce trustworthy models of drug response suitable for use as clinical decision aids. Availability and implementation Our implementation of the GSNN method is available at https://github.com/nathanieljevans/GSNN. All data used in this work is publicly available.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/QT9BPYTS/Evans et al. - 2024 - Graph Structured Neural Networks for Perturbation .pdf}
}

@article{exomeaggregationconsortiumAnalysisProteincodingGenetic2016,
  title        = {Analysis of Protein-Coding Genetic Variation in 60,706 Humans},
  author       = {{Exome Aggregation Consortium} and Lek, Monkol and Karczewski, Konrad J. and Minikel, Eric V. and Samocha, Kaitlin E. and Banks, Eric and Fennell, Timothy and O’Donnell-Luria, Anne H. and Ware, James S. and Hill, Andrew J. and Cummings, Beryl B. and Tukiainen, Taru and Birnbaum, Daniel P. and Kosmicki, Jack A. and Duncan, Laramie E. and Estrada, Karol and Zhao, Fengmei and Zou, James and Pierce-Hoffman, Emma and Berghout, Joanne and Cooper, David N. and Deflaux, Nicole and DePristo, Mark and Do, Ron and Flannick, Jason and Fromer, Menachem and Gauthier, Laura and Goldstein, Jackie and Gupta, Namrata and Howrigan, Daniel and Kiezun, Adam and Kurki, Mitja I. and Moonshine, Ami Levy and Natarajan, Pradeep and Orozco, Lorena and Peloso, Gina M. and Poplin, Ryan and Rivas, Manuel A. and Ruano-Rubio, Valentin and Rose, Samuel A. and Ruderfer, Douglas M. and Shakir, Khalid and Stenson, Peter D. and Stevens, Christine and Thomas, Brett P. and Tiao, Grace and Tusie-Luna, Maria T. and Weisburd, Ben and Won, Hong-Hee and Yu, Dongmei and Altshuler, David M. and Ardissino, Diego and Boehnke, Michael and Danesh, John and Donnelly, Stacey and Elosua, Roberto and Florez, Jose C. and Gabriel, Stacey B. and Getz, Gad and Glatt, Stephen J. and Hultman, Christina M. and Kathiresan, Sekar and Laakso, Markku and McCarroll, Steven and McCarthy, Mark I. and McGovern, Dermot and McPherson, Ruth and Neale, Benjamin M. and Palotie, Aarno and Purcell, Shaun M. and Saleheen, Danish and Scharf, Jeremiah M. and Sklar, Pamela and Sullivan, Patrick F. and Tuomilehto, Jaakko and Tsuang, Ming T. and Watkins, Hugh C. and Wilson, James G. and Daly, Mark J. and MacArthur, Daniel G.},
  date         = {2016-08},
  journaltitle = {Nature},
  volume       = {536},
  number       = {7616},
  pages        = {285--291},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/bsf3},
  url          = {http://www.nature.com/articles/nature19057},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@article{fangGSEApyComprehensivePackage2023,
  title      = {{{GSEApy}}: A Comprehensive Package for Performing Gene Set Enrichment Analysis in {{Python}}},
  shorttitle = {{{GSEApy}}},
  author     = {Fang, Zhuoqing and Liu, Xinyuan and Peltz, Gary},
  year       = 2023,
  month      = jan,
  journal    = {Bioinformatics},
  volume     = {39},
  number     = {1},
  pages      = {btac757},
  issn       = {1367-4811},
  doi        = {10.1093/bioinformatics/btac757},
  urldate    = {2025-11-30},
  abstract   = {Gene set enrichment analysis (GSEA) is a commonly used algorithm for characterizing gene expression changes. However, the currently available tools used to perform GSEA have a limited ability to analyze large datasets, which is particularly problematic for the analysis of single-cell data. To overcome this limitation, we developed a GSEA package in Python (GSEApy), which could efficiently analyze large single-cell datasets.We present a package (GSEApy) that performs GSEA in either the command line or Python environment. GSEApy uses a Rust implementation to enable it to calculate the same enrichment statistic as GSEA for a collection of pathways. The Rust implementation of GSEApy is 3-fold faster than the Numpy version of GSEApy (v0.10.8) and uses \&gt;4-fold less memory. GSEApy also provides an interface between Python and Enrichr web services, as well as for BioMart. The Enrichr application programming interface enables GSEApy to perform over-representation analysis for an input gene list. Furthermore, GSEApy consists of several tools, each designed to facilitate a particular type of enrichment analysis.The new GSEApy with Rust extension is deposited in PyPI: https://pypi.org/project/gseapy/. The GSEApy source code is freely available at https://github.com/zqfang/GSEApy. Also, the documentation website is available at https://gseapy.rtfd.io/.Supplementary data are available at Bioinformatics online.},
  file       = {/Users/jkobject/Zotero/storage/H3FQ6EBI/Fang et al. - 2023 - GSEApy a comprehensive package for performing gene set enrichment analysis in Python.pdf;/Users/jkobject/Zotero/storage/M7XC6BIS/btac757.html}
}

@article{samaranScConfluenceSinglecellDiagonal2024,
  title = {{{scConfluence}}: Single-Cell Diagonal Integration with Regularized {{Inverse Optimal Transport}} on Weakly Connected Features},
  shorttitle = {{{scConfluence}}},
  author = {Samaran, Jules and Peyr{\'e}, Gabriel and Cantini, Laura},
  year = 2024,
  month = sep,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {7762},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-51382-x},
  urldate = {2026-01-15},
  abstract = {The abundance of unpaired multimodal single-cell data has motivated a growing body of research into the development of diagonal integration methods. However, the state-of-the-art suffers from the loss of biological information due to feature conversion and struggles with modality-specific populations. To overcome these crucial limitations, we here introduce scConfluence, a method for single-cell diagonal integration. scConfluence combines uncoupled autoencoders on the complete set of features with regularized Inverse Optimal Transport on weakly connected features. We extensively benchmark scConfluence in several single-cell integration scenarios proving that it outperforms the state-of-the-art. We then demonstrate the biological relevance of scConfluence in three applications. We predict spatial patterns for Scgn, Synpr and Olah in scRNA-smFISH integration. We improve the classification of B cells and Monocytes in highly heterogeneous scRNA-scATAC-CyTOF integration. Finally, we reveal the joint contribution of Fezf2 and apical dendrite morphology in Intra Telencephalic neurons, based on morphological images and scRNA.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational models,Data integration,Machine learning,RNA sequencing,Sequencing},
  file = {/Users/jkobject/Zotero/storage/HVS5ZR2T/Samaran et al. - 2024 - scConfluence single-cell diagonal integration with regularized Inverse Optimal Transport on weakly.pdf}
}


@misc{fayRxRx3PhenomicsMap2023,
  title         = {{{RxRx3}}: {{Phenomics Map}} of {{Biology}}},
  shorttitle    = {{{RxRx3}}},
  author        = {Fay, Marta M. and Kraus, Oren and Victors, Mason and Arumugam, Lakshmanan and Vuggumudi, Kamal and Urbanik, John and Hansen, Kyle and Celik, Safiye and Cernek, Nico and Jagannathan, Ganesh and Christensen, Jordan and Earnshaw, Berton A. and Haque, Imran S. and Mabey, Ben},
  year          = 2023,
  month         = feb,
  primaryclass  = {New Results},
  pages         = {2023.02.07.527350},
  publisher     = {bioRxiv},
  doi           = {10.1101/2023.02.07.527350},
  urldate       = {2025-12-13},
  abstract      = {The combination of modern genetic perturbation techniques with high content screening has enabled genome-scale cell microscopy experiments that can be leveraged to construct maps of biology. These are built by processing microscopy images to produce readouts in unified and relatable representation space to capture known biological relationships and discover new ones. To further enable the scientific community to develop methods and insights from map-scale data, here we release RxRx3, the first ever public high-content screening dataset combining genome-scale CRISPR knockouts with multiple-concentration screening of small molecules (a set of FDA approved and commercially available bioactive compounds). The dataset contains 6-channel fluorescent microscopy images and associated deep learning embeddings from over 2.2 million wells that span 17,063 CRISPR knockouts and 1,674 compounds at 8 doses each. RxRx3 is one of the largest collections of cellular screening data, and as far as we know, the largest generated consistently via a common experimental protocol within a single laboratory. Our goal in releasing RxRx3 is to demonstrate the benefits of generating consistent data, enable the development of the machine learning methods on this scale of data and to foster research, methods development, and collaboration. For more information about RxRx3 please visit RxRx.ai/rxrx3},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {\copyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/BB8V7A3C/Fay et al. - 2023 - RxRx3 Phenomics Map of Biology.pdf}
}

@misc{fedusSwitchTransformersScaling2022,
  title         = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle    = {Switch {{Transformers}}},
  author        = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year          = {2022},
  month         = jun,
  number        = {arXiv:2101.03961},
  eprint        = {2101.03961},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2101.03961},
  urldate       = {2024-07-23},
  abstract      = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/ATEW59MU/Fedus et al. - 2022 - Switch Transformers Scaling to Trillion Parameter.pdf;/Users/jkobject/Zotero/storage/5NX7AV9X/2101.html}
}

@article{feldmanPooledOpticalScreens2018,
  title        = {Pooled Optical Screens in Human Cells},
  author       = {Feldman, David and Singh, Avtar and Schmid-Burgk, Jonathan L and Mezger, Anja and Garrity, Anthony J and Carlson, Rebecca J and Zhang, Feng and Blainey, Paul},
  date         = {2018-08-02},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbgv},
  url          = {http://biorxiv.org/lookup/doi/10.1101/383943},
  urldate      = {2019-03-22},
  abstract     = {Large-scale genetic screens play a key role in the systematic discovery of genes underlying cellular phenotypes. Pooling of genetic perturbations greatly increases screening throughput, but has so far been limited to screens of enrichments defined by cell fitness and flow cytometry, or to comparatively low-throughput single cell gene expression profiles. Although microscopy is a rich source of spatial and temporal information about mammalian cells, high-content imaging screens have been restricted to much less efficient arrayed formats. Here, we introduce an optical method to link perturbations and their phenotypic outcomes at the single-cell level in a pooled setting. Barcoded perturbations are read out by targeted in situ sequencing following image-based phenotyping. We apply this technology to screen a focused set of 952 genes across \&gt;3 million cells for involvement in NF-KB activation by imaging the translocation of RelA (p65) to the nucleus, recovering 20 known pathway components and 3 novel candidate positive regulators of IL-1β and TNFα-stimulated immune responses.},
  langid       = {english}
}
@article{fengTopologicalConformationalStability2016,
  title        = {A Topological and Conformational Stability Alphabet for Multipass Membrane Proteins},
  author       = {Feng, Xiang and Barth, Patrick},
  date         = {2016-03},
  journaltitle = {Nature Chemical Biology},
  volume       = {12},
  number       = {3},
  pages        = {167--173},
  issn         = {1552-4450, 1552-4469},
  doi          = {10/f8cnmm},
  url          = {http://www.nature.com/articles/nchembio.2001},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}
@article{feurerEfficientRobustAutomated,
  title      = {Efficient and {{Robust Automated Machine Learning}}},
  author     = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  pages      = {9},
  abstract   = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Feurer et al. - Efficient and Robust Automated Machine Learning.pdf}
}
@misc{FibroblastHeterogeneityProstate,
  title        = {Fibroblast Heterogeneity in Prostate Carcinogenesis - {{PMC}}},
  urldate      = {2024-07-26},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8788937/},
  file         = {/Users/jkobject/Zotero/storage/5ULUU4KG/PMC8788937.html}
}
@article{finneganMaximumEntropyMethods,
  title      = {Maximum Entropy Methods for Extracting the Learned Features of Deep Neural Networks},
  author     = {Finnegan, Alex and Song, Jun S},
  pages      = {20},
  abstract   = {New architectures of multilayer artificial neural networks and new methods for training them are rapidly revolutionizing the application of machine learning in diverse fields, including business, social science, physical sciences, and biology. Interpreting deep neural networks, however, currently remains elusive, and a critical challenge lies in understanding which meaningful features a network is actually learning. We present a general method for interpreting deep neural networks and extracting network-learned features from input data. We describe our algorithm in the context of biological sequence analysis. Our approach, based on ideas from statistical physics, samples from the maximum entropy distribution over possible sequences, anchored at an input sequence and subject to constraints implied by the empirical function learned by a network. Using our framework, we demonstrate that local transcription factor binding motifs can be identified from a network trained on ChIP-seq data and that nucleosome positioning signals are indeed learned by a network trained on chemical cleavage nucleosome maps. Imposing a further constraint on the maximum entropy distribution also allows us to probe whether a network is learning global sequence features, such as the high GC content in nucleosome-rich regions. This work thus provides valuable mathematical tools for interpreting and extracting learned features from feed-forward neural networks.},
  langid     = {english},
  keywords   = {❓ Multiple DOI},
  annotation = {00000}
}
@article{flamaryPOTPythonOptimal2021,
  title   = {POT: Python Optimal Transport},
  author  = {Flamary, Rémi and others},
  journal = {Journal of Machine Learning Research},
  volume  = {22},
  pages   = {1--8},
  year    = {2021}
}
@article{flamaryPOTPythonOptimal2021a,
  title      = {{{POT}}: {{Python Optimal Transport}}},
  shorttitle = {{{POT}}},
  author     = {Flamary, R{\'e}mi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z. and Boisbunon, Aur{\'e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and Gautheron, L{\'e}o and Gayraud, Nathalie T. H. and Janati, Hicham and Rakotomamonjy, Alain and Redko, Ievgen and Rolet, Antoine and Schutz, Antony and Seguy, Vivien and Sutherland, Danica J. and Tavenard, Romain and Tong, Alexander and Vayer, Titouan},
  year       = 2021,
  journal    = {Journal of Machine Learning Research},
  volume     = {22},
  number     = {78},
  pages      = {1--8},
  issn       = {1533-7928},
  urldate    = {2025-12-04},
  abstract   = {Optimal  transport  has  recently  been  reintroduced  to  the  machine  learning  community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications.  We propose a Python toolbox that implements several key optimal transport ideas  for  the  machine  learning  community.   The  toolbox  contains  implementations  of  a number  of  founding  works  of  OT  for  machine  learning  such  as  Sinkhorn  algorithm  and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research.  This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.},
  file       = {/Users/jkobject/Zotero/storage/JQHBIFW6/Flamary et al. - 2021 - POT Python Optimal Transport.pdf;/Users/jkobject/Zotero/storage/LN6NXF3J/20-451.html}
}
@article{flaxmanMachineLearningPopulation2018,
  title        = {Machine Learning in Population Health: {{Opportunities}} and Threats},
  shorttitle   = {Machine Learning in Population Health},
  author       = {Flaxman, Abraham D. and Vos, Theo},
  date         = {2018-11-27},
  journaltitle = {PLOS Medicine},
  volume       = {15},
  number       = {11},
  pages        = {e1002702},
  issn         = {1549-1676},
  doi          = {10/gfxbgm},
  url          = {http://dx.plos.org/10.1371/journal.pmed.1002702},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}
@article{forbesCOSMICExploringWorlds2015,
  title        = {{{COSMIC}}: Exploring the World's Knowledge of Somatic Mutations in Human Cancer},
  shorttitle   = {{{COSMIC}}},
  author       = {Forbes, Simon A. and Beare, David and Gunasekaran, Prasad and Leung, Kenric and Bindal, Nidhi and Boutselakis, Harry and Ding, Minjie and Bamford, Sally and Cole, Charlotte and Ward, Sari and Kok, Chai Yin and Jia, Mingming and De, Tisham and Teague, Jon W. and Stratton, Michael R. and McDermott, Ultan and Campbell, Peter J.},
  date         = {2015-01-28},
  journaltitle = {Nucleic Acids Research},
  volume       = {43},
  number       = {D1},
  pages        = {D805-D811},
  issn         = {1362-4962, 0305-1048},
  doi          = {10/f64ng8},
  url          = {http://academic.oup.com/nar/article/43/D1/D805/2437384/COSMIC-exploring-the-worlds-knowledge-of-somatic},
  urldate      = {2019-03-22},
  abstract     = {COSMIC, the Catalogue Of Somatic Mutations In Cancer (http://cancer.sanger.ac.uk) is the world’s largest and most comprehensive resource for exploring the impact of somatic mutations in human cancer. Our latest release (v70; Aug 2014) describes 2 002 811 coding point mutations in over one million tumor samples and across most human genes. To emphasize depth of knowledge on known cancer genes, mutation information is curated manually from the scientific literature, allowing very precise definitions of disease types and patient details. Combination of almost 20 000 published studies gives substantial resolution of how mutations and phenotypes relate in human cancer, providing insights into the stratification of mutations and biomarkers across cancer patient populations. Conversely, our curation of cancer genomes (over 12 000) emphasizes knowledge breadth, driving discovery of unrecognized cancerdriving hotspots and molecular targets. Our highresolution curation approach is globally unique, giving substantial insight into molecular biomarkers in human oncology. In addition, COSMIC also details more than six million noncoding mutations, 10 534 gene fusions, 61 299 genome rearrangements, 695 504 abnormal copy number segments and 60 119 787 abnormal expression variants. All these types of somatic mutation are annotated to both the human genome and each affected coding gene, then correlated across disease and mutation types.},
  langid       = {english},
  annotation   = {00000}
}
@article{fortunatoNoisyNetworksExploration,
  title      = {Noisy {{Networks}} for {{Exploration}}},
  author     = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Osband, Jacob Menick Ian and Mnih, Alex Graves Vlad and Hassabis, Remi Munos Demis and Blundell, Olivier Pietquin Charles and Legg, Shane},
  pages      = {17},
  abstract   = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000}
}
@misc{fradkinOrthrusEvolutionaryFunctional2024,
  title         = {Orthrus: {{Towards Evolutionary}} and {{Functional RNA Foundation Models}}},
  shorttitle    = {Orthrus},
  author        = {Fradkin, Philip and Shi, Ruian and Isaev, Keren and Frey, Brendan J. and Morris, Quaid and Lee, Leo J. and Wang, Bo},
  year          = {2024},
  month         = oct,
  primaryclass  = {New Results},
  pages         = {2024.10.10.617658},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.10.10.617658},
  urldate       = {2025-02-25},
  abstract      = {In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Pre-trained genomic foundation models offer an avenue to adapt learned RNA representations to biological prediction tasks. However, existing genomic foundation models are trained using strategies borrowed from textual or visual domains, such as masked language modelling or next token prediction, that do not leverage biological domain knowledge. Here, we introduce Orthrus, a Mamba-based RNA foundation model pre-trained using a novel self-supervised contrastive learning objective with biological augmentations. Orthrus is trained by maximizing embedding similarity between curated pairs of RNA transcripts, where pairs are formed from splice isoforms of 10 model organisms and transcripts from orthologous genes in 400+ mammalian species from the Zoonomia Project. This training objective results in a latent representation that clusters RNA sequences with functional and evolutionary similarities. We find that the generalized mature RNA isoform representations learned by Orthrus significantly outperform existing genomic foundation models on five mRNA property prediction tasks, and requires only a fraction of fine-tuning data to do so.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/B47QEKMU/Fradkin et al. - 2024 - Orthrus Towards Evolutionary and Functional RNA F.pdf}
}
@article{franzenPanglaoDBWebServer2019,
  title      = {{{PanglaoDB}}: A Web Server for Exploration of Mouse and Human Single-Cell {{RNA}} Sequencing Data},
  shorttitle = {{{PanglaoDB}}},
  author     = {Franz{\'e}n, Oscar and Gan, Li-Ming and Bj{\"o}rkegren, Johan L M},
  year       = {2019},
  month      = apr,
  journal    = {Database: The Journal of Biological Databases and Curation},
  volume     = {2019},
  pages      = {baz046},
  issn       = {1758-0463},
  doi        = {10.1093/database/baz046},
  urldate    = {2024-07-23},
  abstract   = {Single-cell RNA sequencing is an increasingly used method to measure gene expression at the single cell level and build cell-type atlases of tissues. Hundreds of single-cell sequencing datasets have already been published. However, studies are frequently deposited as raw data, a format difficult to access for biological researchers due to the need for data processing using complex computational pipelines. We have implemented an online database, PanglaoDB, accessible through a user-friendly interface that can be used to explore published mouse and human single cell RNA sequencing studies. PanglaoDB contains pre-processed and pre-computed analyses from more than 1054 single-cell experiments covering most major single cell platforms and protocols, based on more than 4 million cells from a wide range of tissues and organs. The online interface allows users to query and explore cell types, genetic pathways and regulatory networks. In addition, we have established a community-curated cell-type marker compendium, containing more than 6000 gene-cell-type associations, as a resource for automatic annotation of cell types.},
  pmcid      = {PMC6450036},
  pmid       = {30951143},
  file       = {/Users/jkobject/Zotero/storage/H2QUEIUF/Franzén et al. - 2019 - PanglaoDB a web server for exploration of mouse a.pdf}
}

@article{fristonActiveInferenceCuriosity2017,
  title        = {Active {{Inference}}, {{Curiosity}} and {{Insight}}},
  author       = {Friston, Karl J. and Lin, Marco and Frith, Christopher D. and Pezzulo, Giovanni and Hobson, J. Allan and Ondobaka, Sasha},
  date         = {2017-10},
  journaltitle = {Neural Computation},
  volume       = {29},
  number       = {10},
  pages        = {2633--2683},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/gfsvjq},
  url          = {http://www.mitpressjournals.org/doi/abs/10.1162/neco_a_00999},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Friston et al. - 2017 - Active Inference, Curiosity and Insight.pdf}
}

@incollection{FrontMatter2003,
  title     = {Front {{Matter}}},
  booktitle = {Origins of {{Molecular Biology}}},
  year      = 2003,
  pages     = {i-xxii},
  publisher = {John Wiley \& Sons, Ltd},
  doi       = {10.1128/9781555817763.fmatter},
  urldate   = {2025-12-02},
  abstract  = {The prelims comprise: Half-Title Page Title Page Copyright Page Contents Preface to the New Edition Preface to the First Edition Foreword to the New Edition},
  isbn      = {978-1-68367-216-6},
  langid    = {english},
  file      = {/Users/jkobject/Zotero/storage/BED8YTL7/2003 - Front Matter.pdf}
}

@article{fuFoundationModelTranscription2025,
  title   = {A foundation model of transcription across human cell types},
  author  = {Fu, Xikun and others},
  journal = {Nature},
  volume  = {637},
  pages   = {965--973},
  year    = {2025},
  doi     = {10.1038/s41586-024-08391-z}
}

@article{fukumizuKernelBayesRule,
  title      = {Kernel {{Bayes}}’ {{Rule}}: {{Bayesian Inference}} with {{Positive Deﬁnite Kernels}}},
  author     = {Fukumizu, Kenji},
  pages      = {25},
  abstract   = {A kernel method for realizing Bayes’ rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes’ rule are presented, including Baysian computation without likelihood and filtering with a nonparametric state-space model.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{fuStromalepithelialInteractionsProstate2020,
  title        = {Stromal-Epithelial Interactions in Prostate Cancer: {{Overexpression}} of {{PAGE4}} in Stromal Cells Inhibits the Invasive Ability of Epithelial Cells},
  shorttitle   = {Stromal-Epithelial Interactions in Prostate Cancer},
  author       = {Fu, Shui and Liu, Tao and Lv, Chengcheng and Fu, Cheng and Zeng, Ruoheng and Kakehi, Yoshiyuki and Kulkarni, Prakash and Getzenberg, Robert H. and Zeng, Yu},
  date         = {2020-11},
  journaltitle = {Journal of Cellular Biochemistry},
  shortjournal = {J Cell Biochem},
  volume       = {121},
  number       = {11},
  eprint       = {32003504},
  eprinttype   = {pmid},
  pages        = {4406--4418},
  issn         = {1097-4644},
  doi          = {10.1002/jcb.29664},
  abstract     = {It is now widely recognized that carcinoma-associated fibroblasts which are believed to be myofibroblasts, promote the transformation of prostate epithelial cells to cancer cells, enhance their proliferation and invasiveness, and induce the acquisition of resistance to cancer therapy and immune evasiveness. Prostate-associated gene 4 (PAGE4) is an intrinsically disordered protein that is remarkably prostate-specific. PAGE4 is also a stress-response protein that functions as a transcriptional regulator and is upregulated in early-stage prostate cancer (PCa) and its precursor lesions. However, PAGE4 is downregulated in high-grade PCa and metastatic disease. Here, we show that PAGE4 is highly expressed in the stromal cells surrounding the cancer-adjacent "normal" glands and low-grade PCa lesions but not in lesions proximal to high-grade PCa. Overexpression of PAGE4 in a stromal cell line inhibits the migration and invasion of PCa epithelial cells in multiple coculture systems. PAGE4 overexpression also inhibits the downregulation of E-cadherin in PCa epithelial cells when cocultured with stromal cells. Furthermore, signaling via tumor necrosis factor-α and transforming growth factor-β pathways is decreased in the stromal cells overexpressing PAGE4 suggesting that PAGE4 appears to play a protective role against disease progression by perturbing interactions between epithelial cells and stromal cells in PCa. Taken together, these findings support previous observations that upregulation of PAGE4 in PCa correlates with a better prognosis and highlight PAGE4 as a novel therapeutic target for early-stage "low-risk" disease.},
  langid       = {english},
  keywords     = {Antigens Neoplasm,Apoptosis,Biomarkers Tumor,Case-Control Studies,cell movement,Cell Movement,Cell Proliferation,Epithelial Cells,Gene Expression Regulation Neoplastic,Humans,Male,Neoplasm Invasiveness,PAGE4,Phosphorylation,Prognosis,prostatic neoplasms,Prostatic Neoplasms,Stromal Cells,Survival Rate,Tumor Cells Cultured,tumor microenvironment}
}

@article{ganguliEfficientSensoryEncoding2014,
  title        = {Efficient {{Sensory Encoding}} and {{Bayesian Inference}} with {{Heterogeneous Neural Populations}}},
  author       = {Ganguli, Deep and Simoncelli, Eero P.},
  date         = {2014-10},
  journaltitle = {Neural Computation},
  volume       = {26},
  number       = {10},
  pages        = {2103--2134},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/f6ggzz},
  url          = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00638},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Dropbox/Journal Club/Ganguli et Simoncelli - 2014 - Efficient Sensory Encoding and Bayesian Inference .pdf}
}

@article{ganguliNeuralPerceptualSignatures,
  title  = {Neural and Perceptual Signatures of Efficient Sensory Coding},
  author = {Ganguli, Deep and Simoncelli, Eero P},
  pages  = {24},
  langid = {english},
  file   = {/Users/jeremie/Dropbox/Journal Club/Ganguli et Simoncelli - Neural and perceptual signatures of eﬃcient sensor.pdf}
}

@article{ganInferringGeneRegulatory2024,
  title     = {Inferring Gene Regulatory Networks from Single-Cell Transcriptomics Based on Graph Embedding},
  author    = {Gan, Yanglan and Yu, Jiacheng and Xu, Guangwei and Yan, Cairong and Zou, Guobing},
  year      = {2024},
  month     = may,
  journal   = {Bioinformatics},
  volume    = {40},
  number    = {5},
  publisher = {Oxford Academic},
  doi       = {10.1093/bioinformatics/btae291},
  urldate   = {2024-07-10},
  abstract  = {AbstractMotivation. Gene regulatory networks (GRNs) encode gene regulation in living organisms, and have become a critical tool to understand complex biolo},
  langid    = {english},
  file      = {/Users/jkobject/Zotero/storage/I8BEEB47/Gan et al. - 2024 - Inferring gene regulatory networks from single-cel.pdf}
}

@unpublished{gargNeurogenesisInspiredDictionaryLearning2017,
  title       = {Neurogenesis-{{Inspired Dictionary Learning}}: {{Online Model Adaption}} in a {{Changing World}}},
  shorttitle  = {Neurogenesis-{{Inspired Dictionary Learning}}},
  author      = {Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Lozano, Aurelie},
  date        = {2017-01-21},
  eprint      = {1701.06106},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1701.06106},
  urldate     = {2019-03-22},
  abstract    = {In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model’s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the “neuronal birth” is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. “Neuronal death” is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{gargNeurogenesisInspiredDictionaryLearning2017a,
  title      = {Neurogenesis-{{Inspired Dictionary Learning}}: {{Online Model Adaption}} in a {{Changing World}}},
  shorttitle = {Neurogenesis-{{Inspired Dictionary Learning}}},
  author     = {Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Lozano, Aurelie},
  date       = {2017-08},
  pages      = {1696--1702},
  publisher  = {International Joint Conferences on Artificial Intelligence Organization},
  doi        = {10.24963/ijcai.2017/235},
  url        = {https://www.ijcai.org/proceedings/2017/235},
  urldate    = {2018-04-11},
  abstract   = {In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model’s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the “neuronal birth” is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. “Neuronal death” is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.},
  isbn       = {978-0-9992411-0-3},
  langid     = {english},
  annotation = {00000}
}

@article{garnettSystematicIdentificationGenomic2012,
  title        = {Systematic Identification of Genomic Markers of Drug Sensitivity in Cancer Cells},
  author       = {Garnett, Mathew J. and Edelman, Elena J. and Heidorn, Sonja J. and Greenman, Chris D. and Dastur, Anahita and Lau, King Wai and Greninger, Patricia and Thompson, I. Richard and Luo, Xi and Soares, Jorge and Liu, Qingsong and Iorio, Francesco and Surdez, Didier and Chen, Li and Milano, Randy J. and Bignell, Graham R. and Tam, Ah T. and Davies, Helen and Stevenson, Jesse A. and Barthorpe, Syd and Lutz, Stephen R. and Kogera, Fiona and Lawrence, Karl and McLaren-Douglas, Anne and Mitropoulos, Xeni and Mironenko, Tatiana and Thi, Helen and Richardson, Laura and Zhou, Wenjun and Jewitt, Frances and Zhang, Tinghu and O’Brien, Patrick and Boisvert, Jessica L. and Price, Stacey and Hur, Wooyoung and Yang, Wanjuan and Deng, Xianming and Butler, Adam and Choi, Hwan Geun and Chang, Jae Won and Baselga, Jose and Stamenkovic, Ivan and Engelman, Jeffrey A. and Sharma, Sreenath V. and Delattre, Olivier and Saez-Rodriguez, Julio and Gray, Nathanael S. and Settleman, Jeffrey and Futreal, P. Andrew and Haber, Daniel A. and Stratton, Michael R. and Ramaswamy, Sridhar and McDermott, Ultan and Benes, Cyril H.},
  date         = {2012-03},
  journaltitle = {Nature},
  volume       = {483},
  number       = {7391},
  pages        = {570--575},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/m4w},
  url          = {http://www.nature.com/articles/nature11005},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{garrawayLessonsCancerGenome2013,
  title        = {Lessons from the {{Cancer Genome}}},
  author       = {Garraway, Levi~A. and Lander, Eric~S.},
  date         = {2013-03},
  journaltitle = {Cell},
  volume       = {153},
  number       = {1},
  pages        = {17--37},
  issn         = {00928674},
  doi          = {10/f4txb5},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867413002882},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@online{GeneOntologyKnowledgebase,
  title   = {Gene {{Ontology}} Knowledgebase in 2023 | {{Genetics}} | {{Oxford Academic}}},
  url     = {https://academic.oup.com/genetics/article/224/1/iyad031/7068118?login=true},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/6XCYMK95/7068118.html}
}

@article{gersteinArchitectureHumanRegulatory2012,
  title        = {Architecture of the Human Regulatory Network Derived from {{ENCODE}} Data},
  author       = {Gerstein, Mark B. and Kundaje, Anshul and Hariharan, Manoj and Landt, Stephen G. and Yan, Koon-Kiu and Cheng, Chao and Mu, Xinmeng Jasmine and Khurana, Ekta and Rozowsky, Joel and Alexander, Roger and Min, Renqiang and Alves, Pedro and Abyzov, Alexej and Addleman, Nick and Bhardwaj, Nitin and Boyle, Alan P. and Cayting, Philip and Charos, Alexandra and Chen, David Z. and Cheng, Yong and Clarke, Declan and Eastman, Catharine and Euskirchen, Ghia and Frietze, Seth and Fu, Yao and Gertz, Jason and Grubert, Fabian and Harmanci, Arif and Jain, Preti and Kasowski, Maya and Lacroute, Phil and Leng, Jing and Lian, Jin and Monahan, Hannah and O’Geen, Henriette and Ouyang, Zhengqing and Partridge, E. Christopher and Patacsil, Dorrelyn and Pauli, Florencia and Raha, Debasish and Ramirez, Lucia and Reddy, Timothy E. and Reed, Brian and Shi, Minyi and Slifer, Teri and Wang, Jing and Wu, Linfeng and Yang, Xinqiong and Yip, Kevin Y. and Zilberman-Schapira, Gili and Batzoglou, Serafim and Sidow, Arend and Farnham, Peggy J. and Myers, Richard M. and Weissman, Sherman M. and Snyder, Michael},
  date         = {2012-09},
  journaltitle = {Nature},
  volume       = {489},
  number       = {7414},
  pages        = {91--100},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/f36w7m},
  url          = {http://www.nature.com/articles/nature11245},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{gilchristCombiningModelsProtein2007,
  title        = {Combining {{Models}} of {{Protein Translation}} and {{Population Genetics}} to {{Predict Protein Production Rates}} from {{Codon Usage Patterns}}},
  author       = {Gilchrist, M. A.},
  date         = {2007-08-16},
  journaltitle = {Molecular Biology and Evolution},
  volume       = {24},
  number       = {11},
  pages        = {2362--2372},
  issn         = {0737-4038, 1537-1719},
  doi          = {10.1093/molbev/msm169},
  url          = {https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msm169},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{gilchristEstimatingGeneExpression2015,
  title        = {Estimating {{Gene Expression}} and {{Codon-Specific Translational Efficiencies}}, {{Mutation Biases}}, and {{Selection Coefficients}} from {{Genomic Data Alonez}}},
  author       = {Gilchrist, Michael A and Chen, Wei-Chen and Shah, Premal and Landerer, Cedric L and Zaretzki, Russell},
  date         = {2015},
  journaltitle = {Genome Biol. Evol.},
  pages        = {21},
  doi          = {10/f7mj7s},
  abstract     = {Extracting biologically meaningful information from the continuing flood of genomic data is a major challenge in the life sciences. Codon usage bias (CUB) is a general feature of most genomes and is thought to reflect the effects of both natural selection for efficient translation and mutation bias. Here we present a mechanistically interpretable, Bayesian model (ribosome overhead costs Stochastic Evolutionary Model of Protein Production Rate [ROC SEMPPR]) to extract meaningful information from patterns of CUB within a genome. ROC SEMPPR is grounded in population genetics and allows us to separate the contributions of mutational biases and natural selection against translational inefficiency on a gene-by-gene and codon-by-codon basis. Until now, the primary disadvantage of similar approaches was the need for genome scale measurements of gene expression. Here, we demonstrate that it is possible to both extract accurate estimates of codon-specific mutation biases and translational efficiencies while simultaneously generating accurate estimates of gene expression, rather than requiring such information. We demonstrate the utility of ROC SEMPPR using the Saccharomyces cerevisiae S288c genome. When we compare our model fits with previous approaches we observe an exceptionally high agreement between estimates of both codon-specific parameters and gene expression levels (r {$>$} 0:99 in all cases). We also observe strong agreement between our parameter estimates and those derived from alternative data sets. For example, our estimates of mutation bias and those from mutational accumulation experiments are highly correlated (r ¼ 0:95). Our estimates of codonspecific translational inefficiencies and tRNA copy number-based estimates of ribosome pausing time (r ¼ 0:64), and mRNA and ribosome profiling footprint-based estimates of gene expression (r ¼ 0:53 À 0:74) are also highly correlated, thus supporting the hypothesis that selection against translational inefficiency is an important force driving the evolution of CUB. Surprisingly, we find that for particular amino acids, codon usage in highly expressed genes can still be largely driven by mutation bias and that failing to take mutation bias into account can lead to the misidentification of an amino acid’s “optimal” codon. In conclusion, our method demonstrates that an enormous amount of biologically important information is encoded within genome scale patterns of codon usage, accessing this information does not require gene expression measurements, but instead carefully formulated biologically interpretable models.},
  langid       = {english},
  annotation   = {00000}
}

@article{giovannucciAutomatedGestureTracking2018,
  title        = {Automated Gesture Tracking in Head-Fixed Mice},
  author       = {Giovannucci, A. and Pnevmatikakis, E.A. and Deverett, B. and Pereira, T. and Fondriest, J. and Brady, M.J. and Wang, S.S.-H. and Abbas, W. and Parés, P. and Masip, D.},
  date         = {2018-04},
  journaltitle = {Journal of Neuroscience Methods},
  volume       = {300},
  pages        = {184--195},
  issn         = {01650270},
  doi          = {10/gds8q9},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0165027017302509},
  urldate      = {2018-04-11},
  abstract     = {Background. The preparation consisting of a head-fixed mouse on a spherical or cylindrical treadmill offers unique advantages in a variety of experimental contexts. Head fixation provides the mechanical stability necessary for optical and electrophysiological recordings and stimulation. Additionally, it can be combined with virtual environments such as T-mazes, enabling these types of recording during diverse behaviors.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/friends/Giovannucci et al. - 2018 - Automated gesture tracking in head-fixed mice.pdf}
}

@article{giovannucciCerebellarGranuleCells2017,
  title        = {Cerebellar Granule Cells Acquire a Widespread Predictive Feedback Signal during Motor Learning},
  author       = {Giovannucci, Andrea and Badura, Aleksandra and Deverett, Ben and Najafi, Farzaneh and Pereira, Talmo D and Gao, Zhenyu and Ozden, Ilker and Kloth, Alexander D and Pnevmatikakis, Eftychios and Paninski, Liam and De Zeeuw, Chris I and Medina, Javier F and Wang, Samuel S-H},
  date         = {2017-05},
  journaltitle = {Nature Neuroscience},
  volume       = {20},
  number       = {5},
  pages        = {727--734},
  issn         = {1097-6256, 1546-1726},
  doi          = {10/f9vrt6},
  url          = {http://www.nature.com/articles/nn.4531},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{gonzalezCombinatorialPredictionTherapeutic2024,
  title    = {Combinatorial Prediction of Therapeutic Perturbations Using Causally-Inspired Neural Networks},
  author   = {Gonzalez, Guadalupe and Herath, Isuru and Veselkov, Kirill and Bronstein, Michael and Zitnik, Marinka},
  year     = {2024},
  month    = jan,
  journal  = {bioRxiv},
  pages    = {2024.01.03.573985},
  doi      = {10.1101/2024.01.03.573985},
  urldate  = {2024-07-10},
  abstract = {As an alternative to target-driven drug discovery, phenotype-driven approaches identify compounds that counteract the overall disease effects by analyzing phenotypic signatures. Our study introduces a novel approach to this field, aiming to expand the search space for new therapeutic agents. We introduce PDGrapher, a causally-inspired graph neural network model designed to predict arbitrary perturbagens -- sets of therapeutic targets -- capable of reversing disease effects. Unlike existing methods that learn responses to perturbations, PDGrapher solves the inverse problem, which is to infer the perturbagens necessary to achieve a specific response -- i.e., directly predicting perturbagens by learning which perturbations elicit a desired response. Experiments across eight datasets of genetic and chemical perturbations show that PDGrapher successfully predicted effective perturbagens in up to 9\% additional test samples and ranked therapeutic targets up to 35\% higher than competing methods. A key innovation of PDGrapher is its direct prediction capability, which contrasts with the indirect, computationally intensive models traditionally used in phenotypedriven drug discovery that only predict changes in phenotypes due to perturbations. The direct approach enables PDGrapher to train up to 30 times faster, representing a significant leap in efficiency. Our results suggest that PDGrapher can advance phenotype-driven drug discovery, offering a fast and comprehensive approach to identifying therapeutically useful perturbations.},
  pmcid    = {PMC10802439},
  pmid     = {38260532},
  file     = {/Users/jkobject/Zotero/storage/ALN4YYGT/Gonzalez et al. - 2024 - Combinatorial prediction of therapeutic perturbati.pdf}
}

@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}

@unpublished{goodfellowGenerativeAdversarialNetworks2014,
  title       = {Generative {{Adversarial Networks}}},
  author      = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date        = {2014-06-10},
  eprint      = {1406.2661},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1406.2661},
  urldate     = {2019-03-22},
  abstract    = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{goodwinComingAgeTen2016,
  title        = {Coming of Age: Ten Years of next-Generation Sequencing Technologies},
  shorttitle   = {Coming of Age},
  author       = {Goodwin, Sara and McPherson, John D. and McCombie, W. Richard},
  date         = {2016-06},
  journaltitle = {Nature Reviews Genetics},
  volume       = {17},
  number       = {6},
  pages        = {333--351},
  issn         = {1471-0056, 1471-0064},
  doi          = {10/f8nnv3},
  url          = {http://www.nature.com/articles/nrg.2016.49},
  urldate      = {2019-03-22},
  abstract     = {Since the completion of the human genome project in 2003, extraordinary progress has been made in genome sequencing technologies, which has led to a decreased cost per megabase and an increase in the number and diversity of sequenced genomes. An astonishing complexity of genome architecture has been revealed, bringing these sequencing technologies to even greater advancements. Some approaches maximize the number of bases sequenced in the least amount of time, generating a wealth of data that can be used to understand increasingly complex phenotypes. Alternatively, other approaches now aim to sequence longer contiguous pieces of DNA, which are essential for resolving structurally complex regions. These and other strategies are providing researchers and clinicians a variety of tools to probe genomes in greater depth, leading to an enhanced understanding of how genome sequence variants underlie phenotype and disease.},
  langid       = {english},
  annotation   = {00000}
}

@article{granmoTsetlinMachineGame,
  title    = {The {{Tsetlin Machine}} - {{A Game Theoretic Bandit Driven Approach}} to {{Optimal Pattern Recognition}} with {{Propositional Logic}}∗},
  author   = {Granmo, Ole-Christoffer},
  pages    = {28},
  abstract = {Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks. Unknown to many, there exists an arguably even simpler and more versatile learning mechanism, namely, the Tsetlin Automaton. Merely by means of a single integer as memory, it learns the optimal action in stochastic environments through increment and decrement operations. In this paper, we introduce the Tsetlin Machine, which solves complex pattern recognition problems with easy-to-interpret propositional formulas, composed by a collective of Tsetlin Automata. To eliminate the longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our theoretical analysis establishes that the Nash equilibria of the game are aligned with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones. We argue that the Tsetlin Machine finds the propositional formula that provides optimal accuracy, with probability arbitrarily close to unity. In four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks, SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It further turns out that the accuracy advantage of the Tsetlin Machine increases with lack of data. The Tsetlin Machine has a significant computational performance advantage since both inputs, patterns, and outputs are expressed as bits, while recognition of patterns relies on bit manipulation. The combination of accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains, including safety-critical medicine. Being the first of its kind, we believe the Tsetlin Machine will kick-start completely new paths of research, with a potentially significant impact on the AI field and the applications of AI.},
  langid   = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file     = {/Users/jeremie/Documents/science/ML/Granmo - The Tsetlin Machine - A Game Theoretic Bandit Driv.pdf}
}

@unpublished{gravesNeuralTuringMachines2014,
  title       = {Neural {{Turing Machines}}},
  author      = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  date        = {2014-10-20},
  eprint      = {1410.5401},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1410.5401},
  urldate     = {2019-03-22},
  abstract    = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Neural and Evolutionary Computing}
}

@article{grayGPUKernelsBlockSparse,
  title      = {{{GPU Kernels}} for {{Block-Sparse Weights}}},
  author     = {Gray, Scott and Radford, Alec and Kingma, Diederik P},
  pages      = {12},
  abstract   = {We’re releasing highly optimized GPU kernels for an underexplored class of neural network architectures: networks with block-sparse weights. The kernels allow for efficient evaluation and differentiation of linear layers, including convolutional layers, with flexibly configurable block-sparsity patterns in the weight matrix. We find that depending on the sparsity, these kernels can run orders of magnitude faster than the best available alternatives such as cuBLAS. Using the kernels we improve upon the state-of-the-art in text sentiment analysis and generative modeling of text and images. By releasing our kernels in the open we aim to spur further advancement in model and algorithm design.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{greensideDiscoveringEpistaticFeature2018,
  title        = {Discovering Epistatic Feature Interactions from Neural Network Models of Regulatory {{DNA}} Sequences},
  author       = {Greenside, Peyton G and Shimko, Tyler and Fordyce, Polly and Kundaje, Anshul},
  date         = {2018-07-26},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbfj},
  url          = {http://biorxiv.org/lookup/doi/10.1101/302711},
  urldate      = {2019-03-22},
  abstract     = {Transcription factors bind complex regulatory DNA sequence patterns in a combinatorial manner to modulate gene expression. Deep neural networks (DNNs) can learn these cis-regulatory grammars encoded in regulatory DNA sequences associated with transcription factor binding and chromatin accessibility. Several feature attribution methods have been developed for estimating the predictive importance of individual features (nucleotides or motifs) in any input DNA sequence to its associated output prediction from a DNN model. However, these methods do not reveal higher-order, epistatic feature interactions encoded by the models. We present a new method called Deep Feature Interaction Maps (DFIM) to efficiently estimate interactions between all pairs of features in any input DNA sequence. DFIM accurately identifies ground truth motif interactions embedded in simulated regulatory DNA sequences. DFIM identifies synergistic interactions between GATA1 and TAL1 motifs from in vivo TF binding models. DFIM reveals epistatic interactions involving nucleotides flanking the core motif of the Cbf1 TF in yeast from in vitro TF binding models. We also apply DFIM to regulatory sequence models of in vivo chromatin accessibility to reveal interactions between regulatory genetic variants and proximal motifs of target TFs as validated by TF binding quantitative trait loci. Our approach makes significant strides in improving the interpretability of deep learning models for genomics.},
  langid       = {english},
  annotation   = {00000}
}

@misc{gretafriarItTakesThree2023,
  title        = {It Takes Three to Tango: Transcription Factors Bind {{DNA}}, Protein, and {{RNA}} {\textbar} {{Whitehead Institute}}},
  shorttitle   = {It Takes Three to Tango},
  author       = {GRETA FRIAR},
  year         = {2023},
  month        = jun,
  journal      = {Whitehead Institute of MIT},
  urldate      = {2024-04-19},
  abstract     = {Transcription factors bind DNA and protein. Now, researchers in Whitehead Institute Member Richard Young's lab find that many transcription factors can also bind RNA, which fine tunes their regulation of gene expression, suggesting new therapeutic opportunities.},
  howpublished = {https://wi.mit.edu/news/it-takes-three-tango-transcription-factors-bind-dna-protein-and-rna},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/8WZL9ANC/it-takes-three-tango-transcription-factors-bind-dna-protein-and-rna.html}
}

@unpublished{groverNode2vecScalableFeature2016,
  title       = {Node2vec: {{Scalable Feature Learning}} for {{Networks}}},
  shorttitle  = {Node2vec},
  author      = {Grover, Aditya and Leskovec, Jure},
  date        = {2016-07-03},
  eprint      = {1607.00653},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1607.00653},
  urldate     = {2019-03-22},
  abstract    = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  annotation  = {00000}
}

@inproceedings{groverNode2vecScalableFeature2016a,
  title      = {Node2vec: {{Scalable Feature Learning}} for {{Networks}}},
  shorttitle = {Node2vec},
  author     = {Grover, Aditya and Leskovec, Jure},
  date       = {2016},
  pages      = {855--864},
  publisher  = {ACM Press},
  doi        = {10.1145/2939672.2939754},
  url        = {http://dl.acm.org/citation.cfm?doid=2939672.2939754},
  urldate    = {2018-04-11},
  abstract   = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks.},
  isbn       = {978-1-4503-4232-2},
  langid     = {english},
  file       = {/Users/jeremie/Documents/science/ML/NN/GAN/Grover et Leskovec - 2016 - node2vec Scalable Feature Learning for Networks.pdf}
}

@article{gruningSpikingNeuralNetworks2014,
  title        = {Spiking {{Neural Networks}}: {{Principles}} and {{Challenges}}},
  author       = {Gruning, Andre and Bohte, Sander M},
  date         = {2014},
  journaltitle = {Computational Intelligence},
  pages        = {10},
  abstract     = {Over the last decade, various spiking neural network models have been proposed, along with a similarly increasing interest in spiking models of computation in computational neuroscience. The aim of this tutorial paper is to outline some of the common ground in state-of-the-art spiking neural networks as well as open challenges.},
  langid       = {english},
  keywords     = {⛔ No DOI found}
}

@article{guentherCombinationCDK462019,
  title        = {A {{Combination CDK4}}/6 and {{IGF1R Inhibitor Strategy}} for {{Ewing Sarcoma}}},
  author       = {Guenther, Lillian M. and Dharia, Neekesh V. and Ross, Linda and Conway, Amy and Robichaud, Amanda L. and Catlett, Jerrel L. and Wechsler, Caroline S. and Frank, Elizabeth S. and Goodale, Amy and Church, Alanna J. and Tseng, Yuen-Yi and Guha, Rajarshi and McKnight, Crystal G. and Janeway, Katherine A. and Boehm, Jesse S. and Mora, Jaume and Davis, Mindy I. and Alexe, Gabriela and Piccioni, Federica and Stegmaier, Kimberly},
  date         = {2019-02-15},
  journaltitle = {Clinical Cancer Research},
  volume       = {25},
  number       = {4},
  pages        = {1343--1357},
  issn         = {1078-0432, 1557-3265},
  doi          = {10/gfxbfp},
  url          = {http://clincancerres.aacrjournals.org/lookup/doi/10.1158/1078-0432.CCR-18-0372},
  urldate      = {2019-03-22},
  abstract     = {Introduction: Novel targeted therapeutics have transformed the care of subsets of patients with cancer. In pediatric malignancies, however, with simple tumor genomes and infrequent targetable mutations, there have been few new FDA-approved targeted drugs. The cyclin-dependent kinase (CDK)4/6 pathway recently emerged as a dependency in Ewing sarcoma. Given the heightened efficacy of this class with targeted drug combinations in other cancers, as well as the propensity of resistance to emerge with single agents, we aimed to identify genes mediating resistance to CDK4/6 inhibitors and biologically relevant combinations for use with CDK4/6 inhibitors in Ewing.},
  langid       = {english},
  annotation   = {00000}
}

@article{guerraPrecisionTargetingBFL12018,
  title        = {Precision {{Targeting}} of {{BFL-1}}/{{A1}} and an {{ATM Co-dependency}} in {{Human Cancer}}},
  author       = {Guerra, Rachel M. and Bird, Gregory H. and Harvey, Edward P. and Dharia, Neekesh V. and Korshavn, Kyle J. and Prew, Michelle S. and Stegmaier, Kimberly and Walensky, Loren D.},
  date         = {2018-09},
  journaltitle = {Cell Reports},
  volume       = {24},
  number       = {13},
  pages        = {3393-3403.e5},
  issn         = {22111247},
  doi          = {10/gd8gh5},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S2211124718314086},
  urldate      = {2019-03-22},
  abstract     = {Cancer cells overexpress a diversity of antiapoptotic BCL-2 family proteins, such as BCL-2, MCL-1, and BFL-1/A1, to enforce cellular immortality. Thus, intensive drug development efforts have focused on targeting this class of oncogenic proteins to overcome treatment resistance. Whereas a selective BCL-2 inhibitor has been FDA approved and several small molecule inhibitors of MCL-1 have recently entered phase I clinical testing, BFL1/A1 remains undrugged. Here, we developed a series of stapled peptide design principles to engineer a functionally selective and cell-permeable BFL-1/A1 inhibitor that is specifically cytotoxic to BFL-1/A1-dependent human cancer cells. Because cancers harbor a diversity of resistance mechanisms and typically require multi-agent treatment, we further investigated BFL-1/A1 co-dependencies by mining a genome-scale CRISPR-Cas9 screen. We identified ataxia-telangiectasia-mutated (ATM) kinase as a BFL-1/A1 co-dependency in acute myeloid leukemia (AML), which informed the validation of BFL-1/A1 and ATM inhibitor co-treatment as a synergistic approach to subverting apoptotic resistance in cancer.},
  langid       = {english},
  annotation   = {00000}
}

@article{gunawanIntroductionRepresentationLearning2023,
  title    = {An Introduction to Representation Learning for Single-Cell Data Analysis},
  author   = {Gunawan, Ihuan and Vafaee, Fatemeh and Meijering, Erik and Lock, John George},
  year     = {2023},
  month    = aug,
  journal  = {Cell Reports Methods},
  volume   = {3},
  number   = {8},
  pages    = {100547},
  issn     = {2667-2375},
  doi      = {10.1016/j.crmeth.2023.100547},
  urldate  = {2025-03-27},
  abstract = {Single-cell-resolved systems biology methods, including omics- and imaging-based measurement modalities, generate a wealth of high-dimensional data characterizing the heterogeneity of cell populations. Representation learning methods are routinely used to analyze these complex, high-dimensional data by projecting them into lower-dimensional embeddings. This facilitates the interpretation and interrogation of the structures, dynamics, and regulation of cell heterogeneity. Reflecting their central role in analyzing diverse single-cell data types, a myriad of representation learning methods exist, with new approaches continually emerging. Here, we contrast general features of representation learning methods spanning statistical, manifold learning, and neural network approaches. We consider key steps involved in representation learning with single-cell data, including data pre-processing, hyperparameter optimization, downstream analysis, and biological validation. Interdependencies and contingencies linking these steps are also highlighted. This overview is intended to guide researchers in the selection, application, and optimization of representation learning strategies for current and future single-cell research applications., High-dimensional data generated by single-cell systems biology (omics) methods require powerful representation learning approaches to enable interpretation and interrogation of the structures, dynamics, and regulation of cell heterogeneity. In this perspective, Gunawan et~al. elucidate key steps involved in representation learning to guide optimized method application by researchers analyzing diverse single-cell data modalities.},
  pmcid    = {PMC10475795},
  pmid     = {37671013},
  file     = {/Users/jkobject/Zotero/storage/3FAH7QGL/Gunawan et al. - 2023 - An introduction to representation learning for sin.pdf}
}

@article{gutigSpikingNeuronsCan2016,
  title        = {Spiking Neurons Can Discover Predictive Features by Aggregate-Label Learning},
  author       = {Gutig, R.},
  date         = {2016-03-04},
  journaltitle = {Science},
  volume       = {351},
  number       = {6277},
  pages        = {aab4113-aab4113},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/f8cs6c},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.aab4113},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/spiking/Gutig - 2016 - Spiking neurons can discover predictive features b.pdf}
}

@article{haberEvolvingWarCancer2011,
  title        = {The {{Evolving War}} on {{Cancer}}},
  author       = {Haber, Daniel~A. and Gray, Nathanael~S. and Baselga, Jose},
  date         = {2011-04},
  journaltitle = {Cell},
  volume       = {145},
  number       = {1},
  pages        = {19--24},
  issn         = {00928674},
  doi          = {10/bkcx5v},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867411003035},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{hagemann-jensenSinglecellRNACounting2020,
  title     = {Single-Cell {{RNA}} Counting at Allele and Isoform Resolution Using {{Smart-seq3}}},
  author    = {{Hagemann-Jensen}, Michael and Ziegenhain, Christoph and Chen, Ping and Ramsk{\"o}ld, Daniel and Hendriks, Gert-Jan and Larsson, Anton J. M. and Faridani, Omid R. and Sandberg, Rickard},
  year      = 2020,
  month     = jun,
  journal   = {Nature Biotechnology},
  volume    = {38},
  number    = {6},
  pages     = {708--714},
  publisher = {Nature Publishing Group},
  issn      = {1546-1696},
  doi       = {10.1038/s41587-020-0497-0},
  urldate   = {2025-12-13},
  abstract  = {Large-scale sequencing of RNA from individual cells can reveal patterns of gene, isoform and allelic expression across cell types and states1. However, current short-read single-cell RNA-sequencing methods have limited ability to count RNAs at allele and isoform resolution, and long-read sequencing techniques lack the depth required for large-scale applications across cells2,3. Here we introduce Smart-seq3, which combines full-length transcriptome coverage with a 5{$\prime$} unique molecular identifier RNA counting strategy that enables in silico reconstruction of thousands of RNA molecules per cell. Of the counted and reconstructed molecules, 60\% could be directly assigned to allelic origin and 30--50\% to specific isoforms, and we identified substantial differences in isoform usage in different mouse strains and human cell types. Smart-seq3 greatly increased sensitivity compared to Smart-seq2, typically detecting thousands more transcripts per cell. We expect that Smart-seq3 will enable large-scale characterization of cell types and states across tissues and organisms.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Gene expression,RNA sequencing},
  file      = {/Users/jkobject/Zotero/storage/UQRY5PGX/Hagemann-Jensen et al. - 2020 - Single-cell RNA counting at allele and isoform resolution using Smart-seq3.pdf}
}

@article{haghverdiBatchEffectsSinglecell2018,
  title     = {Batch Effects in Single-Cell {{RNA-sequencing}} Data Are Corrected by Matching Mutual Nearest Neighbors},
  author    = {Haghverdi, Laleh and Lun, Aaron T. L. and Morgan, Michael D. and Marioni, John C.},
  year      = 2018,
  month     = may,
  journal   = {Nature Biotechnology},
  volume    = {36},
  number    = {5},
  pages     = {421--427},
  publisher = {Nature Publishing Group},
  issn      = {1546-1696},
  doi       = {10.1038/nbt.4091},
  urldate   = {2025-12-02},
  abstract  = {Differences in gene expression between individual cells of the same type are measured across batches and used to correct technical artifacts in single-cell RNA-sequencing data.},
  copyright = {2018 Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Data integration,Statistical methods,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/NZ449RVQ/Haghverdi et al. - 2018 - Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors.pdf}
}

@article{haibe-kainsInconsistencyLargePharmacogenomic2013,
  title        = {Inconsistency in Large Pharmacogenomic Studies},
  author       = {Haibe-Kains, Benjamin and El-Hachem, Nehme and Birkbak, Nicolai Juul and Jin, Andrew C. and Beck, Andrew H. and Aerts, Hugo J. W. L. and Quackenbush, John},
  date         = {2013-12},
  journaltitle = {Nature},
  volume       = {504},
  number       = {7480},
  pages        = {389--393},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gc3fj8},
  url          = {http://www.nature.com/articles/nature12831},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@unpublished{haleFindingSyntaxHuman2018,
  title       = {Finding {{Syntax}} in {{Human Encephalography}} with {{Beam Search}}},
  author      = {Hale, John and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan R.},
  date        = {2018-06-11},
  eprint      = {1806.04127},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1806.04127},
  urldate     = {2019-03-22},
  abstract    = {Recurrent neural network grammars (RNNGs) are generative models of (tree, string) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computation and Language},
  annotation  = {00000}
}

@article{hanahanHallmarksCancerNext2011,
  title        = {Hallmarks of {{Cancer}}: {{The Next Generation}}},
  shorttitle   = {Hallmarks of {{Cancer}}},
  author       = {Hanahan, Douglas and Weinberg, Robert A.},
  date         = {2011-03-04},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume       = {144},
  number       = {5},
  eprint       = {21376230},
  eprinttype   = {pmid},
  pages        = {646--674},
  publisher    = {Elsevier},
  issn         = {0092-8674, 1097-4172},
  doi          = {10.1016/j.cell.2011.02.013},
  url          = {https://www.cell.com/cell/abstract/S0092-8674(11)00127-9},
  urldate      = {2024-07-26},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/524KMLFL/Hanahan and Weinberg - 2011 - Hallmarks of Cancer The Next Generation.pdf}
}

@misc{balezoMIPHEIViTMultiplexImmunofluorescence2025,
  title = {{{MIPHEI-ViT}}: {{Multiplex Immunofluorescence Prediction}} from {{H}}\&{{E Images}} Using {{ViT Foundation Models}}},
  shorttitle = {{{MIPHEI-ViT}}},
  author = {Balezo, Guillaume and Trullo, Roger and Planas, Albert Pla and Decenciere, Etienne and Walter, Thomas},
  year = 2025,
  month = may,
  number = {arXiv:2505.10294},
  eprint = {2505.10294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.10294},
  urldate = {2026-01-15},
  abstract = {Histopathological analysis is a cornerstone of cancer diagnosis, with Hematoxylin and Eosin (H\&E) staining routinely acquired for every patient to visualize cell morphology and tissue architecture. On the other hand, multiplex immunofluorescence (mIF) enables more precise cell type identification via proteomic markers, but has yet to achieve widespread clinical adoption due to cost and logistical constraints. To bridge this gap, we introduce MIPHEI (Multiplex Immunofluorescence Prediction from H\&E), a U-Net-inspired architecture that integrates state-of-the-art ViT foundation models as encoders to predict mIF signals from H\&E images. MIPHEI targets a comprehensive panel of markers spanning nuclear content, immune lineages (T cells, B cells, myeloid), epithelium, stroma, vasculature, and proliferation. We train our model using the publicly available ORION dataset of restained H\&E and mIF images from colorectal cancer tissue, and validate it on two independent datasets. MIPHEI achieves accurate cell-type classification from H\&E alone, with F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, substantially outperforming both a state-of-the-art baseline and a random classifier for most markers. Our results indicate that our model effectively captures the complex relationships between nuclear morphologies in their tissue context, as visible in H\&E images and molecular markers defining specific cell types. MIPHEI offers a promising step toward enabling cell-type-aware analysis of large-scale H\&E datasets, in view of uncovering relationships between spatial cellular organization and patient outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Tissues and Organs},
  file = {/Users/jkobject/Zotero/storage/E4U7W4PM/Balezo et al. - 2025 - MIPHEI-ViT Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models.pdf;/Users/jkobject/Zotero/storage/Y67YHI2B/2505.html}
}

@article{cuiMultimodalFoundationModels2025,
  title = {Towards Multimodal Foundation Models in Molecular Cell Biology},
  author = {Cui, Haotian and {Tejada-Lapuerta}, Alejandro and Brbi{\'c}, Maria and {Saez-Rodriguez}, Julio and Cristea, Simona and Goodarzi, Hani and Lotfollahi, Mohammad and Theis, Fabian J. and Wang, Bo},
  year = 2025,
  month = apr,
  journal = {Nature},
  volume = {640},
  number = {8059},
  pages = {623--633},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-025-08710-y},
  urldate = {2026-01-15},
  abstract = {The rapid advent of high-throughput omics technologies has created an exponential growth in biological data, often outpacing our ability to derive molecular insights. Large-language models have shown a way out of this data deluge in natural language processing by integrating massive datasets into a joint model with manifold downstream use cases. Here we envision developing multimodal foundation models, pretrained on diverse omics datasets, including genomics, transcriptomics, epigenomics, proteomics, metabolomics and spatial profiling. These models are expected to exhibit unprecedented potential for characterizing the molecular states of cells across a broad continuum, thereby facilitating the creation of holistic maps of cells, genes and tissues. Context-specific transfer learning of the foundation models can empower diverse applications from novel cell-type recognition, biomarker discovery and gene regulation inference, to in silico perturbations. This new paradigm could launch an era of artificial intelligence-empowered analyses, one that promises to unravel the intricate complexities of molecular cell biology, to support experimental design and, more broadly, to profoundly extend our understanding of life sciences.},
  copyright = {2025 Springer Nature Limited},
  langid = {english},
  keywords = {Cell biology,Computational models,Functional genomics,Genomics,Machine learning},
  file = {/Users/jkobject/Zotero/storage/BQH8JAJF/Cui et al. - 2025 - Towards multimodal foundation models in molecular cell biology.pdf}
}

@misc{defardRNA2segGeneralistModel2025,
  title = {{{RNA2seg}}: A Generalist Model for Cell Segmentation in Image-Based Spatial Transcriptomics},
  shorttitle = {{{RNA2seg}}},
  author = {Defard, Thomas and Blondel, Alice and Bellow, Sebastien and Coleon, Anthony and de Melo, Guilherme Dias and Walter, Thomas and Mueller, Florian},
  year = 2025,
  month = mar,
  primaryclass = {New Results},
  pages = {2025.03.03.641259},
  publisher = {bioRxiv},
  doi = {10.1101/2025.03.03.641259},
  urldate = {2026-01-15},
  abstract = {Imaging-based spatial transcriptomics (IST) enables high-resolution spatial mapping of RNA species. A key challenge in IST is accurate cell segmentation to assign each RNA molecule to the right cell. Here, we present RNA2seg, a novel segmentation algorithm trained on over 4 million cells from MERFISH and CosMx datasets across seven organs using a teacher-student training scheme. RNA2seg integrates RNA point clouds and all available membrane and nuclear stainings. Validation on manually annotated data shows superior performance including in zero-shot and few-shot settings. The method is available as a documented pip package: https://github.com/fish-quant/rna2seg.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\copyright{} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english}
}

@article{nesterenkoPhyloformerFastAccurate2025,
  title = {Phyloformer: {{Fast}}, {{Accurate}}, and {{Versatile Phylogenetic Reconstruction}} with {{Deep Neural Networks}}},
  shorttitle = {Phyloformer},
  author = {Nesterenko, Luca and Blassel, Luc and Veber, Philippe and Boussau, Bastien and Jacob, Laurent},
  year = 2025,
  month = apr,
  journal = {Molecular Biology and Evolution},
  volume = {42},
  number = {4},
  publisher = {Oxford Academic},
  doi = {10.1093/molbev/msaf051},
  urldate = {2026-01-15},
  abstract = {Abstract. Phylogenetic inference aims at reconstructing the tree describing the evolution of a set of sequences descending from a common ancestor. The high},
  langid = {english},
  file = {/Users/jkobject/Zotero/storage/AKDFQBNT/Nesterenko et al. - 2025 - Phyloformer Fast, Accurate, and Versatile Phylogenetic Reconstruction with Deep Neural Networks.pdf}
}

@misc{pedrocchiSparseAutoencodersReveal2025,
  title = {Sparse {{Autoencoders Reveal Interpretable Features}} in {{Single-Cell Foundation Models}}},
  author = {Pedrocchi, Flavia and Barkmann, Florian and Joudaki, Amir and Boeva, Valentina},
  year = 2025,
  month = oct,
  primaryclass = {New Results},
  pages = {2025.10.22.681631},
  publisher = {bioRxiv},
  issn = {2692-8205},
  doi = {10.1101/2025.10.22.681631},
  urldate = {2026-01-15},
  abstract = {Single-cell foundation models (scFMs) hold promise for applications in cell type annotation and data integration, but their internal mechanisms remain poorly understood. We investigate the structure of these models by training sparse autoencoders (SAEs) on the hidden representations of two widely used scFMs, scGPT and scFoundation. The learned features reveal diverse and complex biological and technical signals, which emerge even in pre-trained models. We also observe that the encoding of this information differs between scFMs with distinct training protocols and architectures. Further, we find that while many features capture the information about cell types across several studies, they often fall short of unifying it into a single generalized representation. Finally, we demonstrate that SAE-derived features are causally related to model behavior and can be intervened upon to reduce unwanted technical effects while steering model outputs to preserve the core biological signal. These findings provide a path toward more interpretable and controllable single-cell foundation models.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\copyright{} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@misc{theusCancerFoundationSinglecellRNA2024,
  title = {{{CancerFoundation}}: {{A}} Single-Cell {{RNA}} Sequencing Foundation Model to Decipher Drug Resistance in Cancer},
  shorttitle = {{{CancerFoundation}}},
  author = {Theus, Alexander and Barkmann, Florian and Wissel, David and Boeva, Valentina},
  year = 2024,
  month = nov,
  primaryclass = {New Results},
  pages = {2024.11.01.621087},
  publisher = {bioRxiv},
  doi = {10.1101/2024.11.01.621087},
  urldate = {2026-01-15},
  abstract = {We present CancerFoundation, a novel single-cell RNA-seq foundation model (scFM) trained exclusively on malignant cells. Despite being trained on only one million total cells, a fraction of the data used by existing models, CancerFoundation outperforms other scFMs in key tasks such as zero-shot batch integration and drug response prediction. During training, we employ tissue and technologyaware oversampling and domain-invariant training to enhance performance on underrepresented cancer types and sequencing technologies. We propose survival prediction as a new downstream task to evaluate the generalizability of single-cell foundation models to bulk RNA data and their applicability to patient stratification. CancerFoundation demonstrates superior batch integration performance and shows significant improvements in predicting drug responses for both unseen cell lines and drugs. These results highlight the potential of focused, smaller foundation models in advancing drug discovery and our understanding of cancer biology. Our code is available here1.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\copyright{} 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english}
}

@misc{wangBiologicalReasoningReinforcement2025,
  title = {Biological {{Reasoning}} with {{Reinforcement Learning}} through {{Natural Language Enables Generalizable Zero-Shot Cell Type Annotations}}},
  author = {Wang, Xi and Tan, Runzi and Wang, Bo and Cristea, Simona},
  year = 2025,
  month = jun,
  primaryclass = {New Results},
  pages = {2025.06.17.659642},
  publisher = {bioRxiv},
  doi = {10.1101/2025.06.17.659642},
  urldate = {2026-01-15},
  abstract = {Single-cell RNA-sequencing (scRNAseq) has reshaped biomedical research, enabling the high-resolution characterization of cellular populations. Yet cell type annotation, a process typically performed by domain experts interpreting gene expression patterns by manual curation or with specialized algorithms, remains labor-intensive and limited by prior knowledge. In addition, while reasoning large language models (LLMs) have demonstrated remarkable performance on mathematics, coding and general-reasoning benchmarks, their potential in scRNAseq analyses remains underexplored. Here, we investigate the advantages and limitations of employing DeepSeek-R1-0528, a recently developed open-source 671B-parameter reasoning LLM, for zero-shot scRNAseq cell type annotation. We find that DeepSeek-R1 prompted with a ranked list of 10 differentially expressed marker genes per cluster of single cells outperforms both its reasoning-enhanced, non-reasoning equivalent (DeepSeek-V3-0324) and GPT-4o in cluster-level annotations. At the level of single cells, DeepSeek-R1 prompted with the top 500 expressed genes in a cell outperforms its non-reasoning counterpart DeepSeek-V3, illustrating test-time scaling for bioinformatics tasks through natural language. Running DeepSeek-R1 in zero-shot classifier mode, with a prompt that presents a broad catalogue of cell type labels to choose from, improves its performance and generalizability across different datasets. On data curated by the expert model scTab (termed in-domain data), the DeepSeek-R1 classifiers perform better than the expert model scGPT and on par with the specialized cell genomics LLM C2S-Scale-1B, but lag behind scTab. On out-of-distribution data unseen by the two expert models, DeepSeek-R1 and its classifier versions generalize better and outperform the other models in the majority of the evaluated datasets. Notably, DeepSeek-R1 supports its cell type calls with interpretable textual biological rationales underlying its reasoning, providing a learning opportunity for researchers. Nevertheless, peak annotation performance remains modest, highlighting the intrinsic complexity of scRNAseq cell type annotation.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\copyright{} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@misc{goncalvesAdaSplashAdaptiveSparse2025,
  title = {{{AdaSplash}}: {{Adaptive Sparse Flash Attention}}},
  shorttitle = {{{AdaSplash}}},
  author = {Gon{\c c}alves, Nuno and Treviso, Marcos and Martins, Andr{\'e} F. T.},
  year = 2025,
  month = jun,
  number = {arXiv:2502.12082},
  eprint = {2502.12082},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.12082},
  urldate = {2026-01-15},
  abstract = {The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which \${$\alpha\$$}-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of \${$\alpha\$$}-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the \${$\alpha\$$}-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing \${$\alpha\$$}-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jkobject/Zotero/storage/TNXNZI4T/Gonçalves et al. - 2025 - AdaSplash Adaptive Sparse Flash Attention.pdf;/Users/jkobject/Zotero/storage/BSBB7ACI/2502.html}
}


@misc{hanHyperAttentionLongcontextAttention2023,
  title         = {{{HyperAttention}}: {{Long-context Attention}} in {{Near-Linear Time}}},
  shorttitle    = {{{HyperAttention}}},
  author        = {Han, Insu and Jayaram, Rajesh and Karbasi, Amin and Mirrokni, Vahab and Woodruff, David P. and Zandieh, Amir},
  year          = 2023,
  month         = dec,
  number        = {arXiv:2310.05869},
  eprint        = {2310.05869},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2310.05869},
  urldate       = {2025-12-04},
  abstract      = {We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50\textbackslash\% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/353NFSFH/Han et al. - 2023 - HyperAttention Long-context Attention in Near-Linear Time.pdf;/Users/jkobject/Zotero/storage/NXQMHEJR/2310.html}
}

@article{haoLargescaleFoundationModel2024,
  title     = {Large-Scale Foundation Model on Single-Cell Transcriptomics},
  author    = {Hao, Minsheng and Gong, Jing and Zeng, Xin and Liu, Chiming and Guo, Yucheng and Cheng, Xingyi and Wang, Taifeng and Ma, Jianzhu and Zhang, Xuegong and Song, Le},
  year      = {2024},
  month     = jun,
  journal   = {Nature Methods},
  pages     = {1--11},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-024-02305-7},
  urldate   = {2024-07-15},
  abstract  = {Large pretrained models have become foundation models leading to breakthroughs in natural language processing and related fields. Developing foundation models for deciphering the `languages' of cells and facilitating biomedical research is promising yet challenging. Here we developed a large pretrained model scFoundation, also named `xTrimoscFoundation{$\alpha$}', with 100 million parameters covering about 20,000 genes, pretrained on over 50 million human single-cell transcriptomic profiles. scFoundation is a large-scale model in terms of the size of trainable parameters, dimensionality of genes and volume of training data. Its asymmetric transformer-like architecture and pretraining task design empower effectively capturing complex context relations among genes in a variety of cell types and states. Experiments showed its merit as a foundation model that achieved state-of-the-art performances in a diverse array of single-cell analysis tasks such as gene expression enhancement, tissue drug response prediction, single-cell drug response classification, single-cell perturbation prediction, cell type annotation and gene module inference.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Computational models,Machine learning,Software,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/XHMBF3Q3/Hao et al. - 2024 - Large-scale foundation model on single-cell transc.pdf}
}

@online{haradaDistinctCoreRegulatory,
  title     = {A Distinct Core Regulatory Module Enforces Oncogene Expression in {{KMT2A-rearranged}} Leukemia},
  author    = {Harada, Taku and Heshmati, Yaser and Kalfon, Jérémie and Perez, Monika W. and Ferrucio, Juliana Xavier and Ewers, Jazmin and Engler, Benjamin Hubbell and Kossenkov, Andrew and Ellegast, Jana M. and Yi, Joanna S. and Bowker, Allyson and Zhu, Qian and Eagle, Kenneth and Liu, Tianxin and Kai, Yan and Dempster, Joshua M. and Kugener, Guillaume and Wickramasinghe, Jayamanna and Herbert, Zachary T. and Li, Charles H. and Koren, Jošt Vrabič and Weinstock, David M. and Paralkar, Vikram R. and Nabet, Behnam and Lin, Charles Y. and Dharia, Neekesh V. and Stegmaier, Kimberly and Orkin, Stuart H. and Pimkin, Maxim},
  publisher = {Cold Spring Harbor Lab},
  issn      = {0890-9369, 1549-5477},
  doi       = {10.1101/gad.349284.121},
  url       = {http://genesdev.cshlp.org},
  urldate   = {2023-01-19},
  abstract  = {A biweekly scientific journal publishing high-quality research in molecular biology and genetics, cancer biology, biochemistry, and related fields},
  langid    = {english},
  file      = {/Users/jkobject/Zotero/storage/VFMWJWBB/Harada et al. - A distinct core regulatory module enforces oncogen.pdf}
}

@article{haradaDistinctCoreRegulatory2022,
  title        = {A Distinct Core Regulatory Module Enforces Oncogene Expression in {{KMT2A-rearranged}} Leukemia},
  author       = {Harada, Taku and Heshmati, Yaser and Kalfon, Jérémie and Perez, Monika W. and Ferrucio, Juliana Xavier and Ewers, Jazmin and Engler, Benjamin Hubbell and Kossenkov, Andrew and Ellegast, Jana M. and Yi, Joanna S. and Bowker, Allyson and Zhu, Qian and Eagle, Kenneth and Liu, Tianxin and Kai, Yan and Dempster, Joshua M. and Kugener, Guillaume and Wickramasinghe, Jayamanna and Herbert, Zachary T. and Li, Charles H. and Koren, Jošt Vrabič and Weinstock, David M. and Paralkar, Vikram R. and Nabet, Behnam and Lin, Charles Y. and Dharia, Neekesh V. and Stegmaier, Kimberly and Orkin, Stuart H. and Pimkin, Maxim},
  date         = {2022-03-01},
  journaltitle = {Genes \& Development},
  shortjournal = {Genes Dev.},
  volume       = {36},
  number       = {5--6},
  eprint       = {35301220},
  eprinttype   = {pmid},
  pages        = {368--389},
  publisher    = {Cold Spring Harbor Lab},
  issn         = {0890-9369, 1549-5477},
  doi          = {10.1101/gad.349284.121},
  url          = {http://genesdev.cshlp.org/content/36/5-6/368},
  urldate      = {2023-01-19},
  abstract     = {Acute myeloid leukemia with KMT2A (MLL) rearrangements is characterized by specific patterns of gene expression and enhancer architecture, implying unique core transcriptional regulatory circuitry. Here, we identified the transcription factors MEF2D and IRF8 as selective transcriptional dependencies of KMT2A-rearranged AML, where MEF2D displays partially redundant functions with its paralog, MEF2C. Rapid transcription factor degradation followed by measurements of genome-wide transcription rates and superresolution microscopy revealed that MEF2D and IRF8 form a distinct core regulatory module with a narrow direct transcriptional program that includes activation of the key oncogenes MYC, HOXA9, and BCL2. Our study illustrates a mechanism of context-specific transcriptional addiction whereby a specific AML subclass depends on a highly specialized core regulatory module to directly enforce expression of common leukemia oncogenes.},
  langid       = {english},
  keywords     = {IRF8,KMT2A-rearranged AML,MEF2D,transcriptional addiction},
  file         = {/Users/jkobject/Zotero/storage/7DS4MAKH/Harada et al. - 2022 - A distinct core regulatory module enforces oncogen.pdf}
}

@article{haradaDistinctCoreRegulatory2022a,
  title     = {A Distinct Core Regulatory Module Enforces Oncogene Expression in {{KMT2A-rearranged}} Leukemia},
  author    = {Harada, Taku and Heshmati, Yaser and Kalfon, J{\'e}r{\'e}mie and Perez, Monika W. and Ferrucio, Juliana Xavier and Ewers, Jazmin and Engler, Benjamin Hubbell and Kossenkov, Andrew and Ellegast, Jana M. and Yi, Joanna S. and Bowker, Allyson and Zhu, Qian and Eagle, Kenneth and Liu, Tianxin and Kai, Yan and Dempster, Joshua M. and Kugener, Guillaume and Wickramasinghe, Jayamanna and Herbert, Zachary T. and Li, Charles H. and Koren, Jo{\v s}t Vrabi{\v c} and Weinstock, David M. and Paralkar, Vikram R. and Nabet, Behnam and Lin, Charles Y. and Dharia, Neekesh V. and Stegmaier, Kimberly and Orkin, Stuart H. and Pimkin, Maxim},
  year      = {2022},
  month     = mar,
  journal   = {Genes \& Development},
  volume    = {36},
  number    = {5-6},
  pages     = {368--389},
  publisher = {Cold Spring Harbor Lab},
  issn      = {0890-9369, 1549-5477},
  doi       = {10.1101/gad.349284.121},
  urldate   = {2025-02-25},
  abstract  = {Acute myeloid leukemia with KMT2A (MLL) rearrangements is characterized by specific patterns of gene expression and enhancer architecture, implying unique core transcriptional regulatory circuitry. Here, we identified the transcription factors MEF2D and IRF8 as selective transcriptional dependencies of KMT2A-rearranged AML, where MEF2D displays partially redundant functions with its paralog, MEF2C. Rapid transcription factor degradation followed by measurements of genome-wide transcription rates and superresolution microscopy revealed that MEF2D and IRF8 form a distinct core regulatory module with a narrow direct transcriptional program that includes activation of the key oncogenes MYC, HOXA9, and BCL2. Our study illustrates a mechanism of context-specific transcriptional addiction whereby a specific AML subclass depends on a highly specialized core regulatory module to directly enforce expression of common leukemia oncogenes.},
  langid    = {english},
  pmid      = {35301220},
  keywords  = {IRF8,KMT2A-rearranged AML,MEF2D,transcriptional addiction},
  file      = {/Users/jkobject/Zotero/storage/FQHSVH3A/Harada et al. - 2022 - A distinct core regulatory module enforces oncogen.pdf}
}

@article{haradaLeukemiaCoreTranscriptional2023,
  title    = {Leukemia Core Transcriptional Circuitry Is a Sparsely Interconnected Hierarchy Stabilized by Incoherent Feed-Forward Loops},
  author   = {Harada, Taku and Kalfon, J{\'e}r{\'e}mie and Perez, Monika W. and Eagle, Kenneth and Braes, Flora Dievenich and Batley, Rashad and Heshmati, Yaser and Ferrucio, Juliana Xavier and Ewers, Jazmin and Mehta, Stuti and Kossenkov, Andrew and Ellegast, Jana M. and Bowker, Allyson and Wickramasinghe, Jayamanna and Nabet, Behnam and Paralkar, Vikram R. and Dharia, Neekesh V. and Stegmaier, Kimberly and Orkin, Stuart H. and Pimkin, Maxim},
  year     = {2023},
  month    = mar,
  journal  = {bioRxiv},
  pages    = {2023.03.13.532438},
  doi      = {10.1101/2023.03.13.532438},
  urldate  = {2025-01-28},
  abstract = {Lineage-defining transcription factors form densely interconnected circuits in chromatin occupancy assays, but the functional significance of these networks remains underexplored. We reconstructed the functional topology of a leukemia cell transcription network from the direct gene-regulatory programs of eight core transcriptional regulators established in pre-steady state assays coupling targeted protein degradation with nascent transcriptomics. The core regulators displayed narrow, largely non-overlapping direct transcriptional programs, forming a sparsely interconnected functional hierarchy stabilized by incoherent feed-forward loops. BET bromodomain and CDK7 inhibitors disrupted the core regulators' direct programs, acting as mixed agonists/antagonists. The network is predictive of dynamic gene expression behaviors in time-resolved assays and clinically relevant pathway activity in patient populations.},
  pmcid    = {PMC10054969},
  pmid     = {36993171},
  file     = {/Users/jkobject/Zotero/storage/SHJYIDGX/Harada et al. - 2023 - Leukemia core transcriptional circuitry is a spars.pdf}
}

@article{haradaRapidkineticsDegronBenchmarking2023,
  title    = {Rapid-Kinetics Degron Benchmarking Reveals off-Target Activities and Mixed Agonism-Antagonism of {{MYB}} Inhibitors},
  author   = {Harada, Taku and Perez, Monika W. and Kalfon, J{\'e}r{\'e}mie and Braes, Flora Dievenich and Batley, Rashad and Eagle, Kenneth and Nabet, Behnam and Leifer, Becky and Kruell, Jasmin and Paralkar, Vikram R. and Stegmaier, Kimberly and Koehler, Angela N. and Orkin, Stuart H. and Pimkin, Maxim},
  year     = 2023,
  month    = apr,
  journal  = {bioRxiv},
  pages    = {2023.04.07.536032},
  issn     = {2692-8205},
  doi      = {10.1101/2023.04.07.536032},
  urldate  = {2025-12-01},
  abstract = {Attenuating aberrant transcriptional circuits holds great promise for the treatment of numerous diseases, including cancer. However, development of transcriptional inhibitors is hampered by the lack of a generally accepted functional cellular readout to characterize their target specificity and on-target activity. We benchmarked the direct gene-regulatory signatures of six agents reported as inhibitors of the oncogenic transcription factor MYB against targeted MYB degradation in a nascent transcriptomics assay. The inhibitors demonstrated partial specificity for MYB target genes but displayed significant off-target activity. Unexpectedly, the inhibitors displayed bimodal on-target effects, acting as mixed agonists-antagonists. Our data uncover unforeseen agonist effects of small molecules originally developed as TF inhibitors and argue that rapid-kinetics benchmarking against degron models should be used for functional characterization of transcriptional modulators.},
  pmcid    = {PMC10104119},
  pmid     = {37066194},
  file     = {/Users/jkobject/Zotero/storage/F8Z2FSU3/Harada et al. - 2023 - Rapid-kinetics degron benchmarking reveals off-target activities and mixed agonism-antagonism of MYB.pdf}
}

@article{harrisonEnsembl20242024,
  title    = {Ensembl 2024},
  author   = {Harrison, Peter W and Amode, M Ridwan and {Austine-Orimoloye}, Olanrewaju and Azov, Andrey~G and Barba, Matthieu and Barnes, If and Becker, Arne and Bennett, Ruth and Berry, Andrew and Bhai, Jyothish and Bhurji, Simarpreet Kaur and Boddu, Sanjay and Branco~Lins, Paulo R and Brooks, Lucy and Ramaraju, Shashank~Budhanuru and Campbell, Lahcen~I and Martinez, Manuel Carbajo and Charkhchi, Mehrnaz and Chougule, Kapeel and Cockburn, Alexander and Davidson, Claire and De~Silva, Nishadi~H and Dodiya, Kamalkumar and Donaldson, Sarah and El Houdaigui, Bilal and Naboulsi, Tamara~El and Fatima, Reham and Giron, Carlos Garcia and Genez, Thiago and Grigoriadis, Dionysios and Ghattaoraya, Gurpreet~S and Martinez, Jose Gonzalez and Gurbich, Tatiana~A and Hardy, Matthew and Hollis, Zoe and Hourlier, Thibaut and Hunt, Toby and Kay, Mike and Kaykala, Vinay and Le, Tuan and Lemos, Diana and Lodha, Disha and {Marques-Coelho}, Diego and Maslen, Gareth and Merino, Gabriela~Alejandra and Mirabueno, Louisse~Paola and Mushtaq, Aleena and Hossain, Syed~Nakib and Ogeh, Denye~N and Sakthivel, Manoj Pandian and Parker, Anne and Perry, Malcolm and Pili{\v z}ota, Ivana and Poppleton, Daniel and Prosovetskaia, Irina and Raj, Shriya and {P{\'e}rez-Silva}, Jos{\'e}~G and Salam, Ahamed~Imran~Abdul and Saraf, Shradha and {Saraiva-Agostinho}, Nuno and Sheppard, Dan and Sinha, Swati and Sipos, Botond and Sitnik, Vasily and Stark, William and Steed, Emily and Suner, Marie-Marthe and Surapaneni, Likhitha and Sutinen, Ky{\"o}sti and Tricomi, Francesca Floriana and {Urbina-G{\'o}mez}, David and Veidenberg, Andres and Walsh, Thomas A and Ware, Doreen and Wass, Elizabeth and Willhoft, Natalie~L and Allen, Jamie and {Alvarez-Jarreta}, Jorge and Chakiachvili, Marc and Flint, Bethany and Giorgetti, Stefano and Haggerty, Leanne and Ilsley, Garth~R and Keatley, Jon and Loveland, Jane~E and Moore, Benjamin and Mudge, Jonathan~M and Naamati, Guy and Tate, John and Trevanion, Stephen~J and Winterbottom, Andrea and Frankish, Adam and Hunt, Sarah E and Cunningham, Fiona and Dyer, Sarah and Finn, Robert~D and Martin, Fergal~J and Yates, Andrew~D},
  year     = {2024},
  month    = jan,
  journal  = {Nucleic Acids Research},
  volume   = {52},
  number   = {D1},
  pages    = {D891-D899},
  issn     = {0305-1048},
  doi      = {10.1093/nar/gkad1049},
  urldate  = {2025-02-25},
  abstract = {Ensembl (https://www.ensembl.org) is a freely available genomic resource that has produced high-quality annotations, tools, and services for vertebrates and model organisms for more than two decades. In recent years, there has been a dramatic shift in the genomic landscape, with a large increase in the number and phylogenetic breadth of high-quality reference genomes, alongside major advances in the pan-genome representations of higher species. In order to support these efforts and accelerate downstream research, Ensembl continues to focus on scaling for the rapid annotation of new genome assemblies, developing new methods for comparative analysis, and expanding the depth and quality of our genome annotations. This year we have continued our expansion to support global biodiversity research, doubling the number of annotated genomes we support on our Rapid Release site to over 1700, driven by our close collaboration with biodiversity projects such as Darwin Tree of Life. We have also strengthened support for key agricultural species, including the first regulatory builds for farmed animals, and have updated key tools and resources that support the global scientific community, notably the Ensembl Variant Effect Predictor. Ensembl data, software, and tools are freely available.},
  file     = {/Users/jkobject/Zotero/storage/489XD6X7/Harrison et al. - 2024 - Ensembl 2024.pdf;/Users/jkobject/Zotero/storage/H5WRT6E9/7416379.html}
}

@article{hassabisNeuroscienceInspiredArtificialIntelligence2017,
  title        = {Neuroscience-{{Inspired Artificial Intelligence}}},
  author       = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  date         = {2017-07},
  journaltitle = {Neuron},
  volume       = {95},
  number       = {2},
  pages        = {245--258},
  issn         = {08966273},
  doi          = {10/gbp987},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0896627317305093},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@article{hauserPharmacogenomicsGPCRDrug2018,
  title        = {Pharmacogenomics of {{GPCR Drug Targets}}},
  author       = {Hauser, Alexander S. and Chavali, Sreenivas and Masuho, Ikuo and Jahn, Leonie J. and Martemyanov, Kirill A. and Gloriam, David E. and Babu, M. Madan},
  date         = {2018-01},
  journaltitle = {Cell},
  volume       = {172},
  number       = {1--2},
  pages        = {41-54.e19},
  issn         = {00928674},
  doi          = {10/chfz},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867417313843},
  urldate      = {2019-03-22},
  abstract     = {Natural genetic variation in the human genome is a cause of individual differences in responses to medications and is an underappreciated burden on public health. Although 108 G-protein-coupled receptors (GPCRs) are the targets of 475 (\$34\%) Food and Drug Administration (FDA)-approved drugs and account for a global sales volume of over 180 billion US dollars annually, the prevalence of genetic variation among GPCRs targeted by drugs is unknown. By analyzing data from 68,496 individuals, we find that GPCRs targeted by drugs show genetic variation within functional regions such as drug- and effector-binding sites in the human population. We experimentally show that certain variants of m-opioid and Cholecystokinin-A receptors could lead to altered or adverse drug response. By analyzing UK National Health Service drug prescription and sales data, we suggest that characterizing GPCR variants could increase prescription precision, improving patients’ quality of life, and relieve the economic and societal burden due to variable drug responsiveness.},
  langid       = {english}
}

@article{havertyReproduciblePharmacogenomicProfiling2016,
  title        = {Reproducible Pharmacogenomic Profiling of Cancer Cell Line Panels},
  author       = {Haverty, Peter M. and Lin, Eva and Tan, Jenille and Yu, Yihong and Lam, Billy and Lianoglou, Steve and Neve, Richard M. and Martin, Scott and Settleman, Jeff and Yauch, Robert L. and Bourgon, Richard},
  date         = {2016-05},
  journaltitle = {Nature},
  volume       = {533},
  number       = {7603},
  pages        = {333--337},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gfxbgz},
  url          = {http://www.nature.com/articles/nature17987},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{haWorldModels2018,
  title         = {World {{Models}}},
  author        = {Ha, David and Schmidhuber, J{\"u}rgen},
  year          = 2018,
  month         = mar,
  eprint        = {1803.10122},
  primaryclass  = {cs},
  doi           = {10.5281/zenodo.1207631},
  urldate       = {2025-12-02},
  abstract      = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/UG9H3NFE/Ha and Schmidhuber - 2018 - World Models.pdf;/Users/jkobject/Zotero/storage/2Y5U5H77/1803.html}
}

@article{hayesSimulating500Million2025,
  title     = {Simulating 500 Million Years of Evolution with a Language Model},
  author    = {Hayes, Thomas and Rao, Roshan and Akin, Halil and Sofroniew, Nicholas J. and Oktay, Deniz and Lin, Zeming and Verkuil, Robert and Tran, Vincent Q. and Deaton, Jonathan and Wiggert, Marius and Badkundri, Rohil and Shafkat, Irhum and Gong, Jun and Derry, Alexander and Molina, Raul S. and Thomas, Neil and Khan, Yousuf A. and Mishra, Chetan and Kim, Carolyn and Bartie, Liam J. and Nemeth, Matthew and Hsu, Patrick D. and Sercu, Tom and Candido, Salvatore and Rives, Alexander},
  year      = 2025,
  month     = feb,
  journal   = {Science},
  volume    = {387},
  number    = {6736},
  pages     = {850--858},
  publisher = {American Association for the Advancement of Science},
  doi       = {10.1126/science.ads0018},
  urldate   = {2025-11-29},
  abstract  = {More than 3 billion years of evolution have produced an image of biology encoded into the space of natural proteins. Here, we show that language models trained at scale on evolutionary data can generate functional proteins that are far away from known proteins. We present ESM3, a frontier multimodal generative language model that reasons over the sequence, structure, and function of proteins. ESM3 can follow complex prompts combining its modalities and is highly responsive to alignment to improve its fidelity. We have prompted ESM3 to generate fluorescent proteins. Among the generations that we synthesized, we found a bright fluorescent protein at a far distance (58\% sequence identity) from known fluorescent proteins, which we estimate is equivalent to simulating 500 million years of evolution.},
  file      = {/Users/jkobject/Zotero/storage/Q5JK8PN5/Hayes et al. - 2025 - Simulating 500 million years of evolution with a language model.pdf}
}

@misc{heDeepResidualLearning2015,
  title         = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year          = 2015,
  month         = dec,
  number        = {arXiv:1512.03385},
  eprint        = {1512.03385},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1512.03385},
  urldate       = {2025-12-07},
  abstract      = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {/Users/jkobject/Zotero/storage/8JDQ9AYR/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/jkobject/Zotero/storage/D4QTHR3I/1512.html}
}

@article{heGenomicPerspectiveAging2023,
  title    = {A Genomic Perspective of the Aging Human and Mouse Lung with a Focus on Immune Response and Cellular Senescence},
  author   = {He, Meng and Borlak, J{\"u}rgen},
  year     = 2023,
  month    = nov,
  journal  = {Immunity \& Ageing},
  volume   = {20},
  number   = {1},
  pages    = {58},
  issn     = {1742-4933},
  doi      = {10.1186/s12979-023-00373-5},
  urldate  = {2025-12-04},
  abstract = {The aging lung is a complex process and influenced by various stressors, especially airborne pathogens and xenobiotics. Additionally, a lifetime exposure to antigens results in structural and functional changes of the lung; yet an understanding of the cell type specific responses remains elusive. To gain insight into age-related changes in lung function and inflammaging, we evaluated 89 mouse and 414 individual human lung genomic data sets with a focus on genes mechanistically linked to extracellular matrix (ECM), cellular senescence, immune response and pulmonary surfactant, and we~interrogated single cell RNAseq data to fingerprint cell type specific changes.},
  langid   = {english},
  keywords = {Aging mouse and human lung,Comparative genomics,ECM remodeling,Immunosenescence,Inflammaging,Pulmonary surfactant,Single cell RNA sequencing},
  file     = {/Users/jkobject/Zotero/storage/6FNI2VKP/He and Borlak - 2023 - A genomic perspective of the aging human and mouse lung with a focus on immune response and cellular.pdf}
}

@article{herrmannLaplacianIsoparametricGrid1976,
  title   = {Laplacian-Isoparametric Grid Generation Scheme},
  author  = {Herrmann, Leonard R.},
  journal = {Journal of the Engineering Mechanics Division},
  volume  = {102},
  pages   = {749--756},
  year    = {1976}
}

@article{hertleHorizontalGenomeTransfer2021,
  title     = {Horizontal Genome Transfer by Cell-to-Cell Travel of Whole Organelles},
  author    = {Hertle, Alexander P. and Haberl, Benedikt and Bock, Ralph},
  year      = {2021},
  month     = jan,
  journal   = {Science Advances},
  volume    = {7},
  number    = {1},
  pages     = {eabd8215},
  publisher = {American Association for the Advancement of Science},
  doi       = {10.1126/sciadv.abd8215},
  urldate   = {2025-02-25},
  abstract  = {Recent work has revealed that both plants and animals transfer genomes between cells. In plants, horizontal transfer of entire plastid, mitochondrial, or nuclear genomes between species generates new combinations of nuclear and organellar genomes, or produces novel species that are allopolyploid. The mechanisms of genome transfer between cells are unknown. Here, we used grafting to identify the mechanisms involved in plastid genome transfer from plant to plant. We show that during proliferation of wound-induced callus, plastids dedifferentiate into small, highly motile, amoeboid organelles. Simultaneously, new intercellular connections emerge by localized cell wall disintegration, forming connective pores through which amoeboid plastids move into neighboring cells. Our work uncovers a pathway of organelle movement from cell to cell and provides a mechanistic framework for horizontal genome transfer.},
  file      = {/Users/jkobject/Zotero/storage/2594DF8F/Hertle et al. - 2021 - Horizontal genome transfer by cell-to-cell travel .pdf}
}

@article{higginsEarlyVisualConcept,
  title      = {Early {{Visual Concept Learning}} with {{Unsupervised Deep Learning}}},
  author     = {Higgins, Irina and Matthey, Loic and Glorot, Xavier and Pal, Arka and Uria, Benigno and Blundell, Charles and Mohamed, Shakir and Lerchner, Alexander},
  pages      = {12},
  abstract   = {Automated discovery of early visual concepts from raw image data is a major open challenge in AI research. Addressing this problem, we propose an unsupervised approach for learning disentangled representations of the underlying factors of variation. We draw inspiration from neuroscience, and show how this can be achieved in an unsupervised generative model by applying the same learning pressures as have been suggested to act in the ventral visual stream in the brain. By enforcing redundancy reduction, encouraging statistical independence, and exposure to data with transform continuities analogous to those to which human infants are exposed, we obtain a variational autoencoder (VAE) framework capable of learning disentangled factors. Our approach makes few assumptions and works well across a wide variety of datasets. Furthermore, our solution has useful emergent properties, such as zero-shot inference and an intuitive understanding of “objectness”.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/autoEncoder/Higgins et al. - Early Visual Concept Learning with Unsupervised De.pdf}
}

@book{hilbeNegativeBinomialRegression2011,
  title       = {Negative {{Binomial Regression}}},
  author      = {Hilbe, Joseph M.},
  year        = {2011},
  month       = mar,
  publisher   = {Cambridge University Press},
  abstract    = {This second edition of Hilbe's Negative Binomial Regression is a substantial enhancement to the popular first edition. The only text devoted entirely to the negative binomial model and its many variations, nearly every model discussed in the literature is addressed. The theoretical and distributional background of each model is discussed, together with examples of their construction, application, interpretation and evaluation. Complete Stata and R codes are provided throughout the text, with additional code (plus SAS), derivations and data provided on the book's website. Written for the practising researcher, the text begins with an examination of risk and rate ratios, and of the estimating algorithms used to model count data. The book then gives an in-depth analysis of Poisson regression and an evaluation of the meaning and nature of overdispersion, followed by a comprehensive analysis of the negative binomial distribution and of its parameterizations into various models for evaluating count data.},
  googlebooks = {0Q\_ijxOEBjMC},
  isbn        = {978-0-521-19815-8},
  langid      = {english},
  keywords    = {Mathematics / Algebra / General,Mathematics / Probability & Statistics / General}
}

@article{hillerForwardGenomicsApproach2012,
  title        = {A “{{Forward Genomics}}” {{Approach Links Genotype}} to {{Phenotype}} Using {{Independent Phenotypic Losses}} among {{Related Species}}},
  author       = {Hiller, Michael and Schaar, Bruce~T. and Indjeian, Vahan~B. and Kingsley, David~M. and Hagey, Lee~R. and Bejerano, Gill},
  date         = {2012-10},
  journaltitle = {Cell Reports},
  volume       = {2},
  number       = {4},
  pages        = {817--823},
  issn         = {22111247},
  doi          = {10/gfxbdj},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S2211124712002720},
  urldate      = {2018-04-11},
  abstract     = {Genotype-phenotype mapping is hampered by countless genomic changes between species. We introduce a computational ‘‘forward genomics’’ strategy that—given only an independently lost phenotype and whole genomes—matches genomic and phenotypic loss patterns to associate specific genomic regions with this phenotype. We conducted genome-wide screens for two metabolic phenotypes. First, our approach correctly matches the inactivated Gulo gene exactly with the species that lost the ability to synthesize vitamin C. Second, we attribute naturally low biliary phospholipid levels in guinea pigs and horses to the inactivated phospholipid transporter Abcb4. Human ABCB4 mutations also result in low phospholipid levels but lead to severe liver disease, suggesting compensatory mechanisms in guinea pig and horse. Our simulation studies, counts of independent changes in existing phenotype surveys, and the forthcoming availability of many new genomes all suggest that forward genomics can be applied to many phenotypes, including those relevant for human evolution and disease.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Hiller et al. - 2012 - A “Forward Genomics” Approach Links Genotype to Ph.pdf}
}

@article{hintonFastLearningAlgorithm2006,
  title        = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author       = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  date         = {2006-07},
  journaltitle = {Neural Computation},
  volume       = {18},
  number       = {7},
  pages        = {1527--1554},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/cjnhxz},
  url          = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527},
  urldate      = {2019-03-22},
  abstract     = {We show how to use “complementary priors” to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  langid       = {english},
  annotation   = {00000}
}

@article{hniszSuperEnhancersControlCell2013,
  title        = {Super-{{Enhancers}} in the {{Control}} of {{Cell Identity}} and {{Disease}}},
  author       = {Hnisz, Denes and Abraham, Brian~J. and Lee, Tong~Ihn and Lau, Ashley and Saint-André, Violaine and Sigova, Alla~A. and Hoke, Heather~A. and Young, Richard~A.},
  date         = {2013-11},
  journaltitle = {Cell},
  volume       = {155},
  number       = {4},
  pages        = {934--947},
  issn         = {00928674},
  doi          = {10/f5g2n9},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867413012270},
  urldate      = {2019-03-22},
  abstract     = {Super-enhancers are large clusters of transcriptional enhancers that drive expression of genes that define cell identity. Improved understanding of the roles that super-enhancers play in biology would be afforded by knowing the constellation of factors that constitute these domains and by identifying super-enhancers across the spectrum of human cell types. We describe here the population of transcription factors, cofactors, chromatin regulators, and transcription apparatus occupying super-enhancers in embryonic stem cells and evidence that super-enhancers are highly transcribed. We produce a catalog of super-enhancers in a broad range of human cell types and find that super-enhancers associate with genes that control and define the biology of these cells. Interestingly, disease-associated variation is especially enriched in the super-enhancers of disease-relevant cell types. Furthermore, we find that cancer cells generate super-enhancers at oncogenes and other genes important in tumor pathogenesis. Thus, super-enhancers play key roles in human cell identity in health and in disease.},
  langid       = {english},
  annotation   = {00000}
}

@article{hockingSegAnnDBInteractiveWebbased2014,
  title        = {{{SegAnnDB}}: Interactive {{Web-based}} Genomic Segmentation},
  shorttitle   = {{{SegAnnDB}}},
  author       = {Hocking, Toby D. and Boeva, Valentina and Rigaill, Guillem and Schleiermacher, Gudrun and Janoueix-Lerosey, Isabelle and Delattre, Olivier and Richer, Wilfrid and Bourdeaut, Franck and Suguro, Miyuki and Seto, Masao and Bach, Francis and Vert, Jean-Philippe},
  date         = {2014-06-01},
  journaltitle = {Bioinformatics},
  volume       = {30},
  number       = {11},
  pages        = {1539--1546},
  issn         = {1460-2059, 1367-4803},
  doi          = {10/f55tbn},
  url          = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btu072},
  urldate      = {2018-04-11},
  abstract     = {Motivation: DNA copy number profiles characterize regions of chromosome gains, losses and breakpoints in tumor genomes. Although many models have been proposed to detect these alterations, it is not clear which model is appropriate before visual inspection the signal, noise and models for a particular profile.},
  langid       = {english},
  annotation   = {00018}
}

@article{hosnyDeepLearningLung2018,
  title        = {Deep Learning for Lung Cancer Prognostication: {{A}} Retrospective Multi-Cohort Radiomics Study},
  shorttitle   = {Deep Learning for Lung Cancer Prognostication},
  author       = {Hosny, Ahmed and Parmar, Chintan and Coroller, Thibaud P. and Grossmann, Patrick and Zeleznik, Roman and Kumar, Avnish and Bussink, Johan and Gillies, Robert J. and Mak, Raymond H. and Aerts, Hugo J. W. L.},
  editor       = {Butte, Atul J.},
  date         = {2018-11-30},
  journaltitle = {PLOS Medicine},
  volume       = {15},
  number       = {11},
  pages        = {e1002711},
  issn         = {1549-1676},
  doi          = {10/gfqnp5},
  url          = {http://dx.plos.org/10.1371/journal.pmed.1002711},
  urldate      = {2019-03-22},
  langid       = {english}
}

@misc{HostCLIC4Expression,
  title        = {Host {{CLIC4}} Expression in the Tumor Microenvironment Is Essential for Breast Cancer Metastatic Competence {\textbar} {{PLOS Genetics}}},
  urldate      = {2024-07-25},
  howpublished = {https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1010271},
  file         = {/Users/jkobject/Zotero/storage/YX28IIYS/article.html}
}

@article{houthooftVIMEVariationalInformation,
  title      = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  author     = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and Turck, Filip De and Abbeel, Pieter},
  pages      = {11},
  abstract   = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as -greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent’s belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/unsupervised/Houthooft et al. - VIME Variational Information Maximizing Explorati.pdf}
}

@article{hsuDevelopmentApplicationsCRISPRCas92014,
  title        = {Development and {{Applications}} of {{CRISPR-Cas9}} for {{Genome Engineering}}},
  author       = {Hsu, Patrick~D. and Lander, Eric~S. and Zhang, Feng},
  date         = {2014-06},
  journaltitle = {Cell},
  volume       = {157},
  number       = {6},
  pages        = {1262--1278},
  issn         = {00928674},
  doi          = {10/f6d3wg},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867414006047},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{huangBayesianInferenceMicroRNA2007,
  title        = {Bayesian {{Inference}} of {{MicroRNA Targets}} from {{Sequence}} and {{Expression Data}}},
  author       = {Huang, Jim C. and Morris, Quaid D. and Frey, Brendan J.},
  date         = {2007-06},
  journaltitle = {Journal of Computational Biology},
  volume       = {14},
  number       = {5},
  pages        = {550--563},
  issn         = {1066-5277, 1557-8666},
  doi          = {10/cfgjfx},
  url          = {http://www.liebertonline.com/doi/abs/10.1089/cmb.2007.R002},
  urldate      = {2018-04-11},
  abstract     = {MicroRNAs (miRNAs) regulate a large proportion of mammalian genes by hybridizing to targeted messenger RNAs (mRNAs) and down-regulating their translation into protein. Although much work has been done in the genome-wide computational prediction of miRNA genes and their target mRNAs, an open question is how to efficiently obtain functional miRNA targets from a large number of candidate miRNA targets predicted by existing computational algorithms. In this paper, we propose a novel Bayesian model and learning algorithm, GenMiRCC (Generative model for miRNA regulation), that accounts for patterns of gene expression using miRNA expression data and a set of candidate miRNA targets. A set of high-confidence functional miRNA targets are then obtained from the data using a Bayesian learning algorithm. Our model scores 467 high-confidence miRNA targets out of 1,770 targets obtained from TargetScanS in mouse at a false detection rate of 2.5\%: several confirmed miRNA targets appear in our high-confidence set, such as the interactions between miR-92 and the signal transduction gene MAP2K4, as well as the relationship between miR16 and BCL2, an anti-apoptotic gene which has been implicated in chronic lymphocytic leukemia. We present results on the robustness of our model showing that our learning algorithm is not sensitive to various perturbations of the data. Our high-confidence targets represent a significant increase in the number of miRNA targets and represent a starting point for a global understanding of gene regulation.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Huang et al. - 2007 - Bayesian Inference of MicroRNA Targets from Sequen.pdf}
}

@unpublished{huangDenselyConnectedConvolutional2016,
  title       = {Densely {{Connected Convolutional Networks}}},
  author      = {Huang, Gao and Liu, Zhuang and family=Maaten, given=Laurens, prefix=van der, useprefix=true and Weinberger, Kilian Q.},
  date        = {2016-08-24},
  eprint      = {1608.06993},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1608.06993},
  urldate     = {2019-03-22},
  abstract    = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation  = {00000}
}

@article{huangNeuralAutoregressiveFlows,
  title      = {Neural {{Autoregressive Flows}}},
  author     = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  pages      = {16},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Huang et al. - Neural Autoregressive Flows.pdf}
}

@article{huangPredictiveCodingPredictive2011,
  title        = {Predictive Coding: {{Predictive}} Coding},
  shorttitle   = {Predictive Coding},
  author       = {Huang, Yanping and Rao, Rajesh P. N.},
  date         = {2011-09},
  journaltitle = {Wiley Interdisciplinary Reviews: Cognitive Science},
  volume       = {2},
  number       = {5},
  pages        = {580--593},
  issn         = {19395078},
  doi          = {10/ctb7pj},
  url          = {http://doi.wiley.com/10.1002/wcs.142},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Huang et Rao - 2011 - Predictive coding Predictive coding.pdf}
}

@misc{huangXAtlasOrionGenomewide2025,
  title         = {X-{{Atlas}}/{{Orion}}: {{Genome-wide Perturb-seq Datasets}} via a {{Scalable Fix-Cryopreserve Platform}} for {{Training Dose-Dependent Biological Foundation Models}}},
  shorttitle    = {X-{{Atlas}}/{{Orion}}},
  author        = {Huang, Ann C. and Hsieh, Tsung-Han S. and Zhu, Jiang and Michuda, Jackson and Teng, Ashton and Kim, Soohong and Rumsey, Elizabeth M. and Lam, Sharon K. and Anigbogu, Ikenna and Wright, Philip and Ameen, Mohamed and You, Kwontae and Graves, Christopher J. and Kim, Hyunsung John and Litterman, Adam J. and Sit, Rene V. and Blocker, Alex and Chu, Ci},
  year          = 2025,
  month         = jun,
  primaryclass  = {New Results},
  pages         = {2025.06.11.659105},
  publisher     = {bioRxiv},
  doi           = {10.1101/2025.06.11.659105},
  urldate       = {2025-12-13},
  abstract      = {The rapid expansion of massively parallel sequencing technologies has enabled the development of foundation models to uncover novel biological findings. While these have the potential to significantly accelerate scientific discoveries by creating AI-driven virtual cell models, their progress has been greatly limited by the lack of large-scale high-quality perturbation data, which remains constrained due to scalability bottlenecks and assay variability. Here, we introduce ``Fix-Cryopreserve-ScRNAseq'' (FiCS) Perturb-seq, an industrialized platform for scalable Perturb-seq data generation. We demonstrate that FiCS Perturb-seq exhibits high sensitivity and low batch effects, effectively capturing perturbation-induced transcriptomic changes and recapitulating known biological pathways and protein complexes. In addition, we release X-Atlas: Orion edition (X-Atlas/Orion), the largest publicly available Perturb-seq atlas. This atlas, generated from two genome-wide FiCS Perturb-seq experiments targeting all human protein-coding genes, comprises eight million cells deeply sequenced to over 16,000 unique molecular identifiers (UMIs) per cell. Furthermore, we show that single guide RNA (sgRNA) abundance can serve as a proxy for gene knockdown (KD) efficacy. Leveraging the deep sequencing and substantial cell numbers per perturbation, we also show that stratification by sgRNA expression can reveal dose-dependent genetic effects. Taken together, we demonstrate that FiCS Perturb-seq is an efficient and scalable platform for high-throughput Perturb-seq screens. Through the release of X-Atlas/Orion, we highlight the potential of FiCS Perturb-seq to address current scalability and variability challenges in data generation, advance foundation model development that incorporates gene-dosage effects, and accelerate biological discoveries.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {\copyright{} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/SQ53J7BN/Huang et al. - 2025 - X-AtlasOrion Genome-wide Perturb-seq Datasets via a Scalable Fix-Cryopreserve Platform for Trainin.pdf}
}

@article{hubertComparingPartitions1985,
  title    = {Comparing Partitions},
  author   = {Hubert, Lawrence and Arabie, Phipps},
  year     = {1985},
  month    = dec,
  journal  = {Journal of Classification},
  volume   = {2},
  number   = {1},
  pages    = {193--218},
  issn     = {1432-1343},
  doi      = {10.1007/BF01908075},
  urldate  = {2024-07-19},
  abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between {\textpm}1.},
  langid   = {english},
  keywords = {Consensus indices,Measures of agreement,Measures of association},
  file     = {/Users/jkobject/Zotero/storage/SZMR8EFY/Hubert and Arabie - 1985 - Comparing partitions.pdf}
}

@misc{huGRITGraphRegularized2025,
  title  = {GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type Annotation},
  author = {Hu, Tianle and others},
  year   = {2025},
  doi    = {10.48550/arXiv.2508.04747}
}

@misc{huGRITGraphRegularizedLogit2025,
  title         = {{{GRIT}}: {{Graph-Regularized Logit Refinement}} for {{Zero-shot Cell Type Annotation}}},
  shorttitle    = {{{GRIT}}},
  author        = {Hu, Tianxiang and Zhou, Chenyi and Liu, Jiaxiang and Wang, Jiongxin and Chen, Ruizhe and Xia, Haoxiang and Wang, Gaoang and Wu, Jian and Liu, Zuozhu},
  year          = 2025,
  month         = aug,
  number        = {arXiv:2508.04747},
  eprint        = {2508.04747},
  primaryclass  = {q-bio},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2508.04747},
  urldate       = {2025-12-03},
  abstract      = {Cell type annotation is a fundamental step in the analysis of single-cell RNA sequencing (scRNA-seq) data. In practice, human experts often rely on the structure revealed by principal component analysis (PCA) followed by \$k\$-nearest neighbor (\$k\$-NN) graph construction to guide annotation. While effective, this process is labor-intensive and does not scale to large datasets. Recent advances in CLIP-style models offer a promising path toward automating cell type annotation. By aligning scRNA-seq profiles with natural language descriptions, models like LangCell enable zero-shot annotation. While LangCell demonstrates decent zero-shot performance, its predictions remain suboptimal, particularly in achieving consistent accuracy across all cell types. In this paper, we propose to refine the zero-shot logits produced by LangCell through a graph-regularized optimization framework. By enforcing local consistency over the task-specific PCA-based k-NN graph, our method combines the scalability of the pre-trained models with the structural robustness relied upon in expert annotation. We evaluate our approach on 14 annotated human scRNA-seq datasets from 4 distinct studies, spanning 11 organs and over 200,000 single cells. Our method consistently improves zero-shot annotation accuracy, achieving accuracy gains of up to 10\%. Further analysis showcase the mechanism by which GRIT effectively propagates correct signals through the graph, pulling back mislabeled cells toward more accurate predictions. The method is training-free, model-agnostic, and serves as a simple yet effective plug-in for enhancing automated cell type annotation in practice.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning,Quantitative Biology - Genomics},
  file          = {/Users/jkobject/Zotero/storage/DSL96IGS/Hu et al. - 2025 - GRIT Graph-Regularized Logit Refinement for Zero-shot Cell Type Annotation.pdf;/Users/jkobject/Zotero/storage/I856823H/2508.html}
}

@article{huImprovingProteinproteinInteraction2024,
  title    = {Improving Protein-Protein Interaction Prediction Using Protein Language Model and Protein Network Features},
  author   = {Hu, Jun and Li, Zhe and Rao, Bing and Thafar, Maha A. and Arif, Muhammad},
  year     = {2024},
  month    = oct,
  journal  = {Analytical Biochemistry},
  volume   = {693},
  pages    = {115550},
  issn     = {0003-2697},
  doi      = {10.1016/j.ab.2024.115550},
  urldate  = {2024-07-15},
  abstract = {Interactions between proteins are ubiquitous in a wide variety of biological processes. Accurately identifying the protein-protein interaction (PPI) is of significant importance for understanding the mechanisms of protein functions and facilitating drug discovery. Although the wet-lab technological methods are the best way to identify PPI, their major constraints are their time-consuming nature, high cost, and labor-intensiveness. Hence, lots of efforts have been made towards developing computational methods to improve the performance of PPI prediction. In this study, we propose a novel hybrid computational method (called KSGPPI) that aims at improving the prediction performance of PPI via extracting the discriminative information from protein sequences and interaction networks. The KSGPPI model comprises two feature extraction modules. In the first feature extraction module, a large protein language model, ESM-2, is employed to exploit the global complex patterns concealed within protein sequences. Subsequently, feature representations are further extracted through CKSAAP, and a two-dimensional convolutional neural network (CNN) is utilized to capture local information. In the second feature extraction module, the query protein acquires its similar protein from the STRING database via the sequence alignment tool NW-align and then captures the graph embedding feature for the query protein in the protein interaction network of the similar protein using the algorithm of Node2vec. Finally, the features of these two feature extraction modules are efficiently fused; the fused features are then fed into the multilayer perceptron to predict PPI. The results of five-fold cross-validation on the used benchmarked datasets demonstrate that KSGPPI achieves an average prediction accuracy of 88.96~\%. Additionally, the average Matthews correlation coefficient value (0.781) of KSGPPI is significantly higher than that of those state-of-the-art PPI prediction methods. The standalone package of KSGPPI is freely downloaded at https://github.com/rickleezhe/KSGPPI.},
  keywords = {PPI network,Protein language model,Protein-protein interactions prediction},
  file     = {/Users/jkobject/Zotero/storage/A9S92PWA/S0003269724000940.html}
}

@article{huMultiscaleFootprintsReveal2025,
  title     = {Multiscale Footprints Reveal the Organization of Cis-Regulatory Elements},
  author    = {Hu, Yan and Horlbeck, Max A. and Zhang, Ruochi and Ma, Sai and Shrestha, Rojesh and Kartha, Vinay K. and Duarte, Fabiana M. and Hock, Conrad and Savage, Rachel E. and Labade, Ajay and Kletzien, Heidi and Meliki, Alia and Castillo, Andrew and Durand, Neva C. and Mattei, Eugenio and Anderson, Lauren J. and Tay, Tristan and Earl, Andrew S. and Shoresh, Noam and Epstein, Charles B. and Wagers, Amy J. and Buenrostro, Jason D.},
  year      = {2025},
  month     = jan,
  journal   = {Nature},
  pages     = {1--8},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/s41586-024-08443-4},
  urldate   = {2025-02-06},
  abstract  = {Cis-regulatory elements (CREs) control gene expression and are dynamic in their structure and function, reflecting changes in the composition of diverse effector proteins over time1. However, methods for measuring the organization of effector proteins at CREs across the genome are limited, hampering efforts to connect CRE structure to their function in cell fate and disease. Here we developed PRINT, a computational method that identifies footprints of DNA--protein interactions from bulk and single-cell chromatin accessibility data across multiple scales of protein size. Using these multiscale footprints, we created the seq2PRINT framework, which uses deep learning to allow precise inference of transcription factor and nucleosome binding and interprets regulatory logic at CREs. Applying seq2PRINT to single-cell chromatin accessibility data from human bone marrow, we observe sequential establishment and widening of CREs centred on pioneer factors across haematopoiesis. We further discover age-associated alterations in the structure of CREs in murine haematopoietic stem cells, including widespread reduction of nucleosome footprints and gain of de novo identified Ets composite motifs. Collectively, we establish a method for obtaining rich insights into DNA-binding protein dynamics from chromatin accessibility data, and reveal the architecture of regulatory elements across differentiation and ageing.},
  copyright = {2025 The Author(s)},
  langid    = {english},
  keywords  = {Epigenomics,Gene regulatory networks,Machine learning},
  file      = {/Users/jkobject/Zotero/storage/7GTZ9AED/Hu et al. - 2025 - Multiscale footprints reveal the organization of c.pdf}
}

@article{huNextgenerationSequencingTechnologies2021,
  title      = {Next-Generation Sequencing Technologies: {{An}} Overview},
  shorttitle = {Next-Generation Sequencing Technologies},
  author     = {Hu, Taishan and Chitnis, Nilesh and Monos, Dimitri and Dinh, Anh},
  year       = 2021,
  month      = nov,
  journal    = {Human Immunology},
  series     = {Next {{Generation Sequencing}} and Its {{Application}} to {{Medical Laboratory Immunology}}},
  volume     = {82},
  number     = {11},
  pages      = {801--811},
  issn       = {0198-8859},
  doi        = {10.1016/j.humimm.2021.02.012},
  urldate    = {2025-12-02},
  abstract   = {Since the days of Sanger sequencing, next-generation sequencing technologies have significantly evolved to provide increased data output, efficiencies, and applications. These next generations of technologies can be categorized based on read length. This review provides an overview of these technologies as two paradigms: short-read, or ``second-generation,'' technologies, and long-read, or ``third-generation,'' technologies. Herein, short-read sequencing approaches are represented by the most prevalent technologies, Illumina and Ion Torrent, and long-read sequencing approaches are represented by Pacific Biosciences and Oxford Nanopore technologies. All technologies are reviewed along with reported advantages and disadvantages. Until recently, short-read sequencing was thought to provide high accuracy limited by read-length, while long-read technologies afforded much longer read-lengths at the expense of accuracy. Emerging developments for third-generation technologies hold promise for the next wave of sequencing evolution, with the co-existence of longer read lengths and high accuracy.},
  keywords   = {Long-read sequencing,Next-generation sequencing,Short-read sequencing},
  file       = {/Users/jkobject/Zotero/storage/LY9SCMVT/S0198885921000628.html}
}

@inproceedings{hussainGlobalSelfAttentionReplacement2022,
  title         = {Global {{Self-Attention}} as a {{Replacement}} for {{Graph Convolution}}},
  booktitle     = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author        = {Hussain, Md Shamim and Zaki, Mohammed J. and Subramanian, Dharmashankar},
  year          = {2022},
  month         = aug,
  eprint        = {2108.03348},
  primaryclass  = {cs},
  pages         = {655--665},
  doi           = {10.1145/3534678.3539296},
  urldate       = {2024-04-19},
  abstract      = {We propose an extension to the transformer neural network architecture for general-purpose graph learning by adding a dedicated pathway for pairwise structural information, called edge channels. The resultant framework - which we call Edge-augmented Graph Transformer (EGT) - can directly accept, process and output structural information of arbitrary form, which is important for effective learning on graph-structured data. Our model exclusively uses global self-attention as an aggregation mechanism rather than static localized convolutional aggregation. This allows for unconstrained long-range dynamic interactions between nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. We verify the performance of EGT in a wide range of graph-learning experiments on benchmark datasets, in which it outperforms Convolutional/Message-Passing Graph Neural Networks. EGT sets a new state-of-the-art for the quantum-chemical regression task on the OGB-LSC PCQM4Mv2 dataset containing 3.8 million molecular graphs. Our findings indicate that global self-attention based aggregation can serve as a flexible, adaptive and effective replacement of graph convolution for general-purpose graph learning. Therefore, convolutional local neighborhood aggregation is not an essential inductive bias.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/622ZPMG9/Hussain et al. - 2022 - Global Self-Attention as a Replacement for Graph C.pdf;/Users/jkobject/Zotero/storage/R8KCCESE/2108.html}
}

@article{huynh-thuInferringRegulatoryNetworks2010,
  title     = {Inferring {{Regulatory Networks}} from {{Expression Data Using Tree-Based Methods}}},
  author    = {{Huynh-Thu}, V{\^a}n Anh and Irrthum, Alexandre and Wehenkel, Louis and Geurts, Pierre},
  year      = {2010},
  month     = sep,
  journal   = {PLOS ONE},
  volume    = {5},
  number    = {9},
  pages     = {e12776},
  publisher = {Public Library of Science},
  issn      = {1932-6203},
  doi       = {10.1371/journal.pone.0012776},
  urldate   = {2024-04-19},
  abstract  = {One of the pressing open problems of computational systems biology is the elucidation of the topology of genetic regulatory networks (GRNs) using high throughput genomic data, in particular microarray gene expression data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge aims to evaluate the success of GRN inference algorithms on benchmarks of simulated data. In this article, we present GENIE3, a new algorithm for the inference of GRNs that was best performer in the DREAM4 In Silico Multifactorial challenge. GENIE3 decomposes the prediction of a regulatory network between p genes into p different regression problems. In each of the regression problems, the expression pattern of one of the genes (target gene) is predicted from the expression patterns of all the other genes (input genes), using tree-based ensemble methods Random Forests or Extra-Trees. The importance of an input gene in the prediction of the target gene expression pattern is taken as an indication of a putative regulatory link. Putative regulatory links are then aggregated over all genes to provide a ranking of interactions from which the whole network is reconstructed. In addition to performing well on the DREAM4 In Silico Multifactorial challenge simulated data, we show that GENIE3 compares favorably with existing algorithms to decipher the genetic regulatory network of Escherichia coli. It doesn't make any assumption about the nature of gene regulation, can deal with combinatorial and non-linear interactions, produces directed GRNs, and is fast and scalable. In conclusion, we propose a new algorithm for GRN inference that performs well on both synthetic and real gene expression data. The algorithm, based on feature selection with tree-based ensemble methods, is simple and generic, making it adaptable to other types of genomic data and interactions.},
  langid    = {english},
  keywords  = {Algorithms,Gene expression,Gene regulation,Gene regulatory networks,Genetic networks,Machine learning,Regulator genes,Trees},
  file      = {/Users/jkobject/Zotero/storage/4JAC94VE/Huynh-Thu et al. - 2010 - Inferring Regulatory Networks from Expression Data.pdf}
}

@article{hwangSinglecellRNASequencing2018,
  title     = {Single-Cell {{RNA}} Sequencing Technologies and Bioinformatics Pipelines},
  author    = {Hwang, Byungjin and Lee, Ji Hyun and Bang, Duhee},
  year      = 2018,
  month     = aug,
  journal   = {Experimental \& Molecular Medicine},
  volume    = {50},
  number    = {8},
  pages     = {1--14},
  publisher = {Nature Publishing Group},
  issn      = {2092-6413},
  doi       = {10.1038/s12276-018-0071-8},
  urldate   = {2025-12-02},
  abstract  = {Rapid progress in the development of next-generation sequencing (NGS)technologies in recent years has provided many valuable insights into complexbiological systems, ranging from cancer genomics to diverse microbial communities.NGS-based technologies for genomics, transcriptomics, and epigenomics are nowincreasingly focused on the characterization of individual cells. These single-cellanalyses will allow researchers to uncover new and potentially unexpected biologicaldiscoveries relative to traditional profiling methods that assess bulk populations.Single-cell RNA sequencing (scRNA-seq), for example, can reveal complex and rarecell populations, uncover regulatory relationships between genes, and track thetrajectories of distinct cell lineages in development. In this review, we will focuson technical challenges in single-cell isolation and library preparation and oncomputational analysis pipelines available for analyzing scRNA-seq data. Furthertechnical improvements at the level of molecular and cell biology and in availablebioinformatics tools will greatly facilitate both the basic science and medicalapplications of these sequencing technologies.},
  copyright = {2018 The Author(s)},
  langid    = {english},
  keywords  = {Bioinformatics,Sequencing},
  file      = {/Users/jkobject/Zotero/storage/SLYB3IN9/Hwang et al. - 2018 - Single-cell RNA sequencing technologies and bioinformatics pipelines.pdf}
}

@article{hyvarinenFastRobustFixedpoint1999,
  title        = {Fast and Robust Fixed-Point Algorithms for Independent Component Analysis},
  author       = {Hyvarinen, A.},
  date         = {1999-05},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume       = {10},
  number       = {3},
  pages        = {626--634},
  issn         = {1045-9227, 1941-0093},
  doi          = {10/ftk87t},
  url          = {http://ieeexplore.ieee.org/document/761722/},
  urldate      = {2019-03-22},
  abstract     = {Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. In this paper, we use a combination of two different approaches for linear ICA: Comon’s informationtheoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast (objective) functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions. These algorithms optimize the contrast functions very fast and reliably.},
  langid       = {english}
}

@article{hyvarinenNonlinearICATemporally,
  title      = {Nonlinear {{ICA}} of {{Temporally Dependent Stationary Sources}}},
  author     = {Hyvarinen, Aapo and Morioka, Hiroshi},
  pages      = {14},
  abstract   = {We develop a nonlinear generalization of independent component analysis (ICA) or blind source separation, based on temporal dependencies (e.g. autocorrelations). We introduce a nonlinear generative model where the independent sources are assumed to be temporally dependent, non-Gaussian, and stationary, and we observe arbitrarily nonlinear mixtures of them. We develop a method for estimating the model (i.e. separating the sources) based on logistic regression in a neural network which learns to discriminate between a short temporal window of the data vs. a temporal window of temporally permuted data. We prove that the method estimates the sources for general smooth mixing nonlinearities, assuming the sources have sufficiently strong temporal dependencies, and these dependencies are in a certain way different from dependencies found in Gaussian processes. For Gaussian (and similar) sources, the method estimates the nonlinear part of the mixing. We thus provide the first rigorous and general proof of identifiability of nonlinear ICA for temporally dependent stationary sources, together with a practical method for its estimation.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{hyvarinenUnsupervisedFeatureExtraction,
  title      = {Unsupervised {{Feature Extraction}} by {{Time-Contrastive Learning}} and {{Nonlinear ICA}}},
  author     = {Hyvärinen, Aapo and Morioka, Hiroshi},
  pages      = {10},
  abstract   = {Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique — thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{icgcpedbrain-seqprojectLandscapeGenomicAlterations2018,
  title        = {The Landscape of Genomic Alterations across Childhood Cancers},
  author       = {{ICGC PedBrain-Seq Project} and {ICGC MMML-Seq Project} and Gröbner, Susanne N. and Worst, Barbara C. and Weischenfeldt, Joachim and Buchhalter, Ivo and Kleinheinz, Kortine and Rudneva, Vasilisa A. and Johann, Pascal D. and Balasubramanian, Gnana Prakash and Segura-Wang, Maia and Brabetz, Sebastian and Bender, Sebastian and Hutter, Barbara and Sturm, Dominik and Pfaff, Elke and Hübschmann, Daniel and Zipprich, Gideon and Heinold, Michael and Eils, Jürgen and Lawerenz, Christian and Erkek, Serap and Lambo, Sander and Waszak, Sebastian and Blattmann, Claudia and Borkhardt, Arndt and Kuhlen, Michaela and Eggert, Angelika and Fulda, Simone and Gessler, Manfred and Wegert, Jenny and Kappler, Roland and Baumhoer, Daniel and Burdach, Stefan and Kirschner-Schwabe, Renate and Kontny, Udo and Kulozik, Andreas E. and Lohmann, Dietmar and Hettmer, Simone and Eckert, Cornelia and Bielack, Stefan and Nathrath, Michaela and Niemeyer, Charlotte and Richter, Günther H. and Schulte, Johannes and Siebert, Reiner and Westermann, Frank and Molenaar, Jan J. and Vassal, Gilles and Witt, Hendrik and Burkhardt, Birgit and Kratz, Christian P. and Witt, Olaf and family=Tilburg, given=Cornelis M., prefix=van, useprefix=true and Kramm, Christof M. and Fleischhack, Gudrun and Dirksen, Uta and Rutkowski, Stefan and Frühwald, Michael and family=Hoff, given=Katja, prefix=von, useprefix=true and Wolf, Stephan and Klingebiel, Thomas and Koscielniak, Ewa and Landgraf, Pablo and Koster, Jan and Resnick, Adam C. and Zhang, Jinghui and Liu, Yanling and Zhou, Xin and Waanders, Angela J. and Zwijnenburg, Danny A. and Raman, Pichai and Brors, Benedikt and Weber, Ursula D. and Northcott, Paul A. and Pajtler, Kristian W. and Kool, Marcel and Piro, Rosario M. and Korbel, Jan O. and Schlesner, Matthias and Eils, Roland and Jones, David T. W. and Lichter, Peter and Chavez, Lukas and Zapatka, Marc and Pfister, Stefan M.},
  date         = {2018-03},
  journaltitle = {Nature},
  volume       = {555},
  number       = {7696},
  pages        = {321--327},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gc3wqt},
  url          = {http://www.nature.com/articles/nature25480},
  urldate      = {2019-03-22},
  langid       = {english}
}

@online{IGFBP7PromotesEndothelial,
  title   = {{{IGFBP7}} Promotes Endothelial Cell Repair in the Recovery Phase of Acute Lung Injury - {{PMC}}},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11196208/},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/XWXVRMV6/PMC11196208.html}
}

@inproceedings{ilseAttentionbasedDeepMultiple2018,
  title     = {Attention-Based {{Deep Multiple Instance Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author    = {Ilse, Maximilian and Tomczak, Jakub and Welling, Max},
  year      = {2018},
  month     = jul,
  pages     = {2127--2136},
  publisher = {PMLR},
  issn      = {2640-3498},
  urldate   = {2025-03-27},
  abstract  = {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.},
  langid    = {english},
  file      = {/Users/jkobject/Zotero/storage/JVR2JV4S/Ilse et al. - 2018 - Attention-based Deep Multiple Instance Learning.pdf;/Users/jkobject/Zotero/storage/ZQEPW3VS/Ilse et al. - 2018 - Attention-based Deep Multiple Instance Learning.pdf}
}

@article{imotoAnalysisGeneNetworks2007,
  title    = {Analysis of Gene Networks for Drug Target Discovery and Validation},
  author   = {Imoto, Seiya and Tamada, Yoshinori and Savoie, Christopher J. and Miyano, Satoru},
  year     = {2007},
  journal  = {Methods in Molecular Biology (Clifton, N.J.)},
  volume   = {360},
  pages    = {33--56},
  issn     = {1064-3745},
  doi      = {10.1385/1-59745-165-7:33},
  abstract = {Understanding responses of the cellular system for a dosing molecule is one of the most important problems in pharmacogenomics. In this chapter, we describe computational methods for identifying and validating drug target genes based on the gene networks estimated from microarray gene expression data. We use two types of microarray gene expression data: gene disruptant microarray data and time-course drug response microarray data. For this purpose, the information of gene networks plays an essential role and is unattainable from clustering methods, which are the standard for gene expression analysis. The gene network is estimated from disruptant microarray data by the Bayesian network model, and then the proposed method automatically identifies sets of genes or gene regulatory pathways affected by the drug. We use an actual example from analysis of Saccharomyces cerevisiae gene expression profile data to express a concrete strategy for the application of gene network information toward drug target discovery.},
  langid   = {english},
  pmid     = {17172724},
  keywords = {Antifungal Agents,Bayes Theorem,Computer Simulation,Drug Design,Gene Expression Regulation Fungal,Gene Regulatory Networks,Griseofulvin,Humans,Oligonucleotide Array Sequence Analysis,Reproducibility of Results,Saccharomyces cerevisiae,Time Factors},
  file     = {/Users/jkobject/Zotero/storage/8SWP36TG/Imoto et al. - 2007 - Analysis of gene networks for drug target discover.pdf}
}

@article{iniguezEWSFLIConfers2018,
  title        = {{{EWS}}/{{FLI Confers Tumor Cell Synthetic Lethality}} to {{CDK12 Inhibition}} in {{Ewing Sarcoma}}},
  author       = {Iniguez, Amanda Balboni and Stolte, Björn and Wang, Emily Jue and Conway, Amy Saur and Alexe, Gabriela and Dharia, Neekesh V. and Kwiatkowski, Nicholas and Zhang, Tinghu and Abraham, Brian J. and Mora, Jaume and Kalev, Peter and Leggett, Alan and Chowdhury, Dipanjan and Benes, Cyril H. and Young, Richard A. and Gray, Nathanael S. and Stegmaier, Kimberly},
  date         = {2018-02},
  journaltitle = {Cancer Cell},
  volume       = {33},
  number       = {2},
  pages        = {202-216.e6},
  issn         = {15356108},
  doi          = {10/gc3x5m},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1535610817305615},
  urldate      = {2019-03-22},
  abstract     = {Many cancer types are driven by oncogenic transcription factors that have been difficult to drug. Transcriptional inhibitors, however, may offer inroads into targeting these cancers. Through chemical genomics screening, we identified that Ewing sarcoma is a disease with preferential sensitivity to THZ1, a covalent small-molecule CDK7/12/13 inhibitor. The selective CDK12/13 inhibitor, THZ531, impairs DNA damage repair in an EWS/FLI-dependent manner, supporting a synthetic lethal relationship between response to THZ1/ THZ531 and EWS/FLI expression. The combination of these molecules with PARP inhibitors showed striking synergy in cell viability and DNA damage assays in vitro and in multiple models of Ewing sarcoma, including a PDX, in vivo without hematopoietic toxicity.},
  langid       = {english},
  annotation   = {00000}
}

@article{iorioLandscapePharmacogenomicInteractions2016,
  title        = {A {{Landscape}} of {{Pharmacogenomic Interactions}} in {{Cancer}}},
  author       = {Iorio, Francesco and Knijnenburg, Theo A. and Vis, Daniel J. and Bignell, Graham R. and Menden, Michael P. and Schubert, Michael and Aben, Nanne and Gonçalves, Emanuel and Barthorpe, Syd and Lightfoot, Howard and Cokelaer, Thomas and Greninger, Patricia and family=Dyk, given=Ewald, prefix=van, useprefix=true and Chang, Han and family=Silva, given=Heshani, prefix=de, useprefix=true and Heyn, Holger and Deng, Xianming and Egan, Regina K. and Liu, Qingsong and Mironenko, Tatiana and Mitropoulos, Xeni and Richardson, Laura and Wang, Jinhua and Zhang, Tinghu and Moran, Sebastian and Sayols, Sergi and Soleimani, Maryam and Tamborero, David and Lopez-Bigas, Nuria and Ross-Macdonald, Petra and Esteller, Manel and Gray, Nathanael S. and Haber, Daniel A. and Stratton, Michael R. and Benes, Cyril H. and Wessels, Lodewyk F.A. and Saez-Rodriguez, Julio and McDermott, Ultan and Garnett, Mathew J.},
  date         = {2016-07},
  journaltitle = {Cell},
  volume       = {166},
  number       = {3},
  pages        = {740--754},
  issn         = {00928674},
  doi          = {10/f8wq4s},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867416307462},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@misc{jabriScalableAdaptiveComputation2023,
  title  = {Scalable Adaptive Computation for Iterative Generation},
  author = {Jabri, Allan and Fleet, David and Chen, Ting},
  year   = {2023},
  doi    = {10.48550/arXiv.2212.11972}
}

@misc{jabriScalableAdaptiveComputation2023a,
  title         = {Scalable {{Adaptive Computation}} for {{Iterative Generation}}},
  author        = {Jabri, Allan and Fleet, David and Chen, Ting},
  year          = 2023,
  month         = jun,
  number        = {arXiv:2212.11972},
  eprint        = {2212.11972},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2212.11972},
  urldate       = {2025-12-03},
  abstract      = {Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024X1024 images without cascades or guidance, while being domain-agnostic and up to 10X more efficient than 2D and 3D U-Nets.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file          = {/Users/jkobject/Zotero/storage/3YSUHKZZ/Jabri et al. - 2023 - Scalable Adaptive Computation for Iterative Generation.pdf;/Users/jkobject/Zotero/storage/TA6UVVG5/2212.html}
}

@article{jaderbergDecoupledNeuralInterfaces,
  title      = {Decoupled {{Neural Interfaces}} Using {{Synthetic Gradients}}},
  author     = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  pages      = {20},
  abstract   = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one’s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning},
  annotation = {00000}
}

@inproceedings{jaeglePerceiverGeneralPerception2021,
  title     = {Perceiver: General Perception with Iterative Attention},
  author    = {Jaegle, Andrew and others},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {4651--4664},
  year      = {2021},
  publisher = {PMLR}
}

@misc{jaeglePerceiverIOGeneral2022,
  title  = {Perceiver IO: A General Architecture for Structured Inputs \& Outputs},
  author = {Jaegle, Andrew and others},
  year   = {2022},
  doi    = {10.48550/arXiv.2107.14795}
}

@article{jaganathanPredictingSplicingPrimary2019,
  title        = {Predicting {{Splicing}} from {{Primary Sequence}} with {{Deep Learning}}},
  author       = {Jaganathan, Kishore and Kyriazopoulou Panagiotopoulou, Sofia and McRae, Jeremy F. and Darbandi, Siavash Fazel and Knowles, David and Li, Yang I. and Kosmicki, Jack A. and Arbelaez, Juan and Cui, Wenwu and Schwartz, Grace B. and Chow, Eric D. and Kanterakis, Efstathios and Gao, Hong and Kia, Amirali and Batzoglou, Serafim and Sanders, Stephan J. and Farh, Kyle Kai-How},
  date         = {2019-01},
  journaltitle = {Cell},
  volume       = {176},
  number       = {3},
  pages        = {535-548.e24},
  issn         = {00928674},
  doi          = {10/gfthvw},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867418316295},
  urldate      = {2019-03-22},
  abstract     = {The splicing of pre-mRNAs into mature transcripts is remarkable for its precision, but the mechanisms by which the cellular machinery achieves such specificity are incompletely understood. Here, we describe a deep neural network that accurately predicts splice junctions from an arbitrary pre-mRNA transcript sequence, enabling precise prediction of noncoding genetic variants that cause cryptic splicing. Synonymous and intronic mutations with predicted splice-altering consequence validate at a high rate on RNA-seq and are strongly deleterious in the human population. De novo mutations with predicted splice-altering consequence are significantly enriched in patients with autism and intellectual disability compared to healthy controls and validate against RNA-seq in 21 out of 28 of these patients. We estimate that 9\%–11\% of pathogenic mutations in patients with rare genetic disorders are caused by this previously underappreciated class of disease variation.},
  langid       = {english}
}

@misc{jeremiekalfonTrainingFoundationModels,
  title        = {Training Foundation Models on Large Collections of {{scRNA-seq}} Data},
  author       = {ribakov {jeremie kalfon}, sergey},
  journal      = {Lamin Blog},
  urldate      = {2024-04-18},
  abstract     = {A few labs and companies now train models on large-scale scRNA-seq count matrices and related data modalities. But unlike for many other data types, there isn't yet a playbook for data scales that ...},
  howpublished = {https://lamin.ai/blog/arrayloader-benchmarks},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/YX5BBAUM/arrayloader-benchmarks.html}
}

@article{jiangStatisticsBiologyZeroinflation2022,
  title      = {Statistics or Biology: The Zero-Inflation Controversy about {{scRNA-seq}} Data},
  shorttitle = {Statistics or Biology},
  author     = {Jiang, Ruochen and Sun, Tianyi and Song, Dongyuan and Li, Jingyi Jessica},
  year       = {2022},
  month      = jan,
  journal    = {Genome Biology},
  volume     = {23},
  number     = {1},
  pages      = {31},
  issn       = {1474-760X},
  doi        = {10.1186/s13059-022-02601-5},
  urldate    = {2024-04-19},
  abstract   = {Researchers view vast zeros in single-cell RNA-seq data differently: some regard zeros as biological signals representing no or low gene expression, while others regard zeros as missing data to be corrected. To help address the controversy, here we discuss the sources of biological and non-biological zeros; introduce five mechanisms of adding non-biological zeros in computational benchmarking; evaluate the impacts of non-biological zeros on data analysis; benchmark three input data types: observed counts, imputed counts, and binarized counts; discuss the open questions regarding non-biological zeros; and advocate the importance of transparent analysis.},
  file       = {/Users/jkobject/Zotero/storage/YLX7JEXI/Jiang et al. - 2022 - Statistics or biology the zero-inflation controve.pdf;/Users/jkobject/Zotero/storage/KU3V7SQ5/s13059-022-02601-5.html}
}

@article{jiaoDeepLearningSystem2019,
  title        = {A Deep Learning System Can Accurately Classify Primary and Metastatic Cancers Based on Patterns of Passenger Mutations},
  author       = {Jiao, Wei and Atwal, Gurnit and Polak, Paz and Karlic, Rosa and Cuppen, Edwin and Danyi, Alexandra and family=Ridder, given=Jeroen, prefix=de, useprefix=true and family=Herpen, given=Carla, prefix=van, useprefix=true and Lolkema, Martijn P and Steeghs, Neeltje and Getz, Gad and Morris, Quaid D and Stein, Lincoln D.},
  date         = {2019-01-22},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbfq},
  url          = {http://biorxiv.org/lookup/doi/10.1101/214494},
  urldate      = {2019-03-22},
  abstract     = {In cancer, the primary tumour's organ of origin and histopathology are the strongest determinants of its clinical behaviour, but in 3\% of the time a cancer patient presents with metastatic tumour and no obvious primary. Challenges also arise when distinguishing a metastatic recurrence of a previously treated cancer from the emergence of a new one. Here we train a deep learning classifier to predict cancer type based on patterns of somatic passenger mutations detected in whole genome sequencing (WGS) of 2606 tumours representing 24 common cancer types. Our classifier achieves an accuracy of 91\% on held-out tumor samples  and 82\% and 85\% respectively on independent primary and metastatic samples, roughly double the accuracy of trained pathologists when presented with a metastatic tumour without knowledge of the primary. Surprisingly, adding information on driver mutations reduced classifier accuracy. Our results have immediate clinical applicability, underscoring how patterns of somatic passenger mutations encode the state of the cell of origin, and can inform future strategies to detect the source of cell-free circulating tumour DNA.},
  langid       = {english},
  annotation   = {00000}
}

@article{jimenez-carreteroTox_RCNNDeepLearningbased2018,
  title        = {Tox\_({{R}}){{CNN}}: {{Deep}} Learning-Based Nuclei Profiling Tool for Drug Toxicity Screening},
  shorttitle   = {Tox\_({{R}}){{CNN}}},
  author       = {Jimenez-Carretero, Daniel and Abrishami, Vahid and Fernández-de-Manuel, Laura and Palacios, Irene and Quílez-Álvarez, Antonio and Díez-Sánchez, Alberto and family=Pozo, given=Miguel A., prefix=del, useprefix=true and Montoya, María C.},
  editor       = {Gallo, James},
  date         = {2018-11-30},
  journaltitle = {PLOS Computational Biology},
  volume       = {14},
  number       = {11},
  pages        = {e1006238},
  issn         = {1553-7358},
  doi          = {10/gfqwhw},
  url          = {http://dx.plos.org/10.1371/journal.pcbi.1006238},
  urldate      = {2019-03-22},
  abstract     = {OPEN ACCESS Citation: Jimenez-Carretero D, Abrishami V, Ferna´ndez-de-Manuel L, Palacios I, Qu´ılez-A´lvarez A, D´ıez-Sa´nchez A, et al. (2018) Tox\_(R)CNN: Deep learning-based nuclei profiling tool for drug toxicity screening. PLoS Comput Biol 14(11): e1006238.},
  langid       = {english}
}

@article{jorstadTranscriptomicCytoarchitectureReveals2023,
  title   = {Transcriptomic cytoarchitecture reveals principles of human neocortex organization},
  author  = {Jorstad, Nikolas L. and others},
  journal = {Science},
  volume  = {382},
  pages   = {eadf6812},
  year    = {2023},
  doi     = {10.1126/science.adf6812}
}

@article{jorstadTranscriptomicCytoarchitectureReveals2023a,
  title     = {Transcriptomic Cytoarchitecture Reveals Principles of Human Neocortex Organization},
  author    = {Jorstad, Nikolas L. and Close, Jennie and Johansen, Nelson and Yanny, Anna Marie and Barkan, Eliza R. and Travaglini, Kyle J. and Bertagnolli, Darren and Campos, Jazmin and Casper, Tamara and Crichton, Kirsten and Dee, Nick and Ding, Song-Lin and Gelfand, Emily and Goldy, Jeff and Hirschstein, Daniel and Kiick, Katelyn and Kroll, Matthew and Kunst, Michael and Lathia, Kanan and Long, Brian and Martin, Naomi and McMillen, Delissa and Pham, Trangthanh and Rimorin, Christine and Ruiz, Augustin and Shapovalova, Nadiya and Shehata, Soraya and Siletti, Kimberly and Somasundaram, Saroja and Sulc, Josef and Tieu, Michael and Torkelson, Amy and Tung, Herman and Callaway, Edward M. and Hof, Patrick R. and Keene, C. Dirk and Levi, Boaz P. and Linnarsson, Sten and Mitra, Partha P. and Smith, Kimberly and Hodge, Rebecca D. and Bakken, Trygve E. and Lein, Ed S},
  year      = 2023,
  month     = oct,
  journal   = {Science},
  volume    = {382},
  number    = {6667},
  pages     = {eadf6812},
  publisher = {American Association for the Advancement of Science},
  doi       = {10.1126/science.adf6812},
  urldate   = {2025-12-03},
  abstract  = {Variation in cytoarchitecture is the basis for the histological definition of cortical areas. We used single cell transcriptomics and performed cellular characterization of the human cortex to better understand cortical areal specialization. Single-nucleus RNA-sequencing of 8 areas spanning cortical structural variation showed a highly consistent cellular makeup for 24 cell subclasses. However, proportions of excitatory neuron subclasses varied substantially, likely reflecting differences in connectivity across primary sensorimotor and association cortices. Laminar organization of astrocytes and oligodendrocytes also differed across areas. Primary visual cortex showed characteristic organization with major changes in the excitatory to inhibitory neuron ratio, expansion of layer 4 excitatory neurons, and specialized inhibitory neurons. These results lay the groundwork for a refined cellular and molecular characterization of human cortical cytoarchitecture and areal specialization.},
  file      = {/Users/jkobject/Zotero/storage/DTJIC2GA/Jorstad et al. - 2023 - Transcriptomic cytoarchitecture reveals principles of human neocortex organization.pdf}
}

@article{josephSingleCellAnalysis2021,
  title    = {Single Cell Analysis of Mouse and Human Prostate Reveals Novel Fibroblasts with Specialized Distribution and Microenvironment Interactions},
  author   = {Joseph, Diya B. and Henry, Gervaise H. and Malewska, Alicia and Reese, Jeffrey C. and Mauck, Ryan J. and Gahan, Jeffrey C. and Hutchinson, Ryan C. and Malladi, Venkat S. and Roehrborn, Claus G. and Vezina, Chad M. and Strand, Douglas W.},
  year     = {2021},
  month    = oct,
  journal  = {The Journal of pathology},
  volume   = {255},
  number   = {2},
  pages    = {141--154},
  issn     = {0022-3417},
  doi      = {10.1002/path.5751},
  urldate  = {2024-07-23},
  abstract = {Stromal-epithelial interactions are critical to the morphogenesis, differentiation, and homeostasis of the prostate, but the molecular identity and anatomy of discrete stromal cell types is poorly understood. Using single cell RNA-sequencing, we identified and validated the in situ localization of three smooth muscle subtypes (prostate smooth muscle, pericytes, and vascular smooth muscle) and two novel fibroblast subtypes in human prostate. Peri-epithelial fibroblasts (APOD+) wrap around epithelial structures while interstitial fibroblasts (C7+) are interspersed in extracellular matrix. In contrast, the mouse displayed three fibroblast subtypes with distinct proximal-distal and lobe specific distribution patterns. Statistical analysis of mouse and human fibroblasts showed transcriptional correlation between mouse prostate (C3+) and urethral (Lgr5+) fibroblasts and the human interstitial fibroblast subtype. Both urethral fibroblasts (Lgr5+) and ductal fibroblasts (Wnt2+) in the mouse contribute to a proximal Wnt/Tgfb signaling niche that is absent in human prostate. Instead, human peri-epithelial fibroblasts express secreted WNT inhibitors SFRPs and DKK1, which could serve as a buffer against stromal WNT ligands by creating a localized signaling niche around individual prostate glands. We also identified proximal-distal fibroblast density differences in human prostate that could amplify stromal signaling around proximal prostate ducts. In human Benign Prostatic Hyperplasia, fibroblast subtypes upregulate critical immunoregulatory pathways and show distinct distributions in stromal and glandular phenotypes. A detailed taxonomy of leukocytes in BPH reveals an influx of myeloid dendritic cells, T cells and B cells, resembling a mucosal inflammatory disorder. A receptor-ligand interaction analysis of all cell types revealed a central role for fibroblasts in growth factor, morphogen and chemokine signaling to endothelia, epithelia, and leukocytes. These data are foundational to the development of new therapeutic targets in benign prostatic hyperplasia.},
  pmcid    = {PMC8429220},
  pmid     = {34173975}
}


@misc{joshiTransformersAreGraph2025,
  title         = {Transformers Are {{Graph Neural Networks}}},
  author        = {Joshi, Chaitanya K.},
  year          = 2025,
  month         = jun,
  number        = {arXiv:2506.22084},
  eprint        = {2506.22084},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2506.22084},
  urldate       = {2025-12-12},
  abstract      = {We establish connections between the Transformer architecture, originally introduced for natural language processing, and Graph Neural Networks (GNNs) for representation learning on graphs. We show how Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure. Thus, Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Despite this mathematical connection to GNNs, Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing. This leads to the perspective that Transformers are GNNs currently winning the hardware lottery.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/6IN8QAEM/Joshi - 2025 - Transformers are Graph Neural Networks.pdf;/Users/jkobject/Zotero/storage/R6TTXHND/2506.html}
}

@misc{kalfonCantinilabGRnnData2025,
  title        = {Cantinilab/{{GRnnData}}},
  author       = {Kalfon, Jeremie},
  year         = {2025},
  month        = jan,
  urldate      = {2025-02-17},
  abstract     = {Awesome GRN enhanced AnnData toolkit},
  copyright    = {GPL-3.0},
  howpublished = {Machine Learning for Integrative Genomics lab},
  isbn         = {10.5281/zenodo.10573141},
  keywords     = {anndata,gene-regulatory-network,rna-seq,scverse,single-cell,utils}
}

@misc{kalfonCantinilabScPRINT2025,
  title        = {Cantinilab/{{scPRINT}}},
  author       = {Kalfon, Jeremie},
  year         = {2025},
  month        = feb,
  urldate      = {2025-02-17},
  abstract     = {single cell foundation model for Gene network inference and more},
  copyright    = {MIT},
  howpublished = {Machine Learning for Integrative Genomics lab},
  keywords     = {batch-correction,cell-biology,celltype-annotation,denoising,embeddings,gene-regulatory-network,large-cell-models,large-language-models,llms,open-source,rnaseq,single-cell-rna-seq,zero-imputation,zero-shot-learning}
}

@misc{kalfonJkobjectBenGRNAwesome2025,
  title     = {Jkobject/{{benGRN}}: {{Awesome Benchmark}} of {{Gene Regulatory Networks}}},
  author    = {Kalfon, Jeremie},
  year      = {2025},
  month     = jan,
  urldate   = {2025-02-17},
  abstract  = {Benchmark your gene regulatory networks inference algorithm (from scRNAseq or bulk RNAseq dataset) with BenGRN},
  copyright = {GPL-3.0},
  isbn      = {10.5281/zenodo.10573209},
  file      = {/Users/jkobject/Zotero/storage/468W7FNX/benGRN.html}
}

@misc{kalfonJkobjectScDataLoader2025,
  title     = {Jkobject/{{scDataLoader}}},
  author    = {Kalfon, J{\'e}r{\'e}mie},
  year      = {2025},
  month     = feb,
  urldate   = {2025-02-17},
  abstract  = {a dataloader to work with large single cell datasets from lamindb},
  copyright = {GPL-3.0},
  isbn      = {10.5281/zenodo.10573143},
  keywords  = {dataloader,lamindb,ml,pytorch,single-cell-rna-seq}
}

@misc{kalfonTowardsFoundationModels2025,
  title  = {Towards foundation models that learn across biological scales},
  author = {Kalfon, Jérémie and Cantini, Laura and Peyre, Gabriel},
  year   = {2025},
  eprint = {2025.05.16.653447},
  doi    = {10.1101/2025.05.16.653447}
}

@article{kamimotoDissectingCellIdentity2023,
  title     = {Dissecting Cell Identity via Network Inference and in Silico Gene Perturbation},
  author    = {Kamimoto, Kenji and Stringa, Blerta and Hoffmann, Christy M. and Jindal, Kunal and {Solnica-Krezel}, Lilianna and Morris, Samantha A.},
  year      = {2023},
  month     = feb,
  journal   = {Nature},
  volume    = {614},
  number    = {7949},
  pages     = {742--751},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/s41586-022-05688-9},
  urldate   = {2024-04-18},
  abstract  = {Cell identity is governed by the complex regulation of gene expression, represented as gene-regulatory networks1. Here we use gene-regulatory networks inferred from single-cell multi-omics data to perform in silico transcription factor perturbations, simulating the consequent changes in cell identity using only unperturbed wild-type data. We apply this machine-learning-based approach, CellOracle, to well-established paradigms---mouse and human haematopoiesis, and zebrafish embryogenesis---and we correctly model reported changes in phenotype that occur as a result of transcription factor perturbation. Through systematic in silico transcription factor perturbation in the developing zebrafish, we simulate and experimentally validate a previously unreported~phenotype that results from the loss of noto, an established notochord regulator. Furthermore, we identify an axial mesoderm regulator, lhx1a. Together, these results show that CellOracle can be used to analyse the regulation of cell identity by transcription factors, and can provide mechanistic insights into development and differentiation.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid    = {english},
  keywords  = {Developmental biology,Gene regulatory networks},
  file      = {/Users/jkobject/Zotero/storage/D3UI2EIE/Kamimoto et al. - 2023 - Dissecting cell identity via network inference and.pdf}
}

@misc{kaplanScalingLawsNeural2020,
  title  = {Scaling Laws for Neural Language Models},
  author = {Kaplan, Jared and others},
  year   = {2020},
  doi    = {10.48550/arXiv.2001.08361}
}

@article{karklinEfficientCodingNatural,
  title    = {Efficient Coding of Natural Images with a Population of Noisy {{Linear-Nonlinear}} Neurons},
  author   = {Karklin, Yan and Simoncelli, Eero P},
  pages    = {9},
  abstract = {Efficient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented filters consistent with responses in primary visual cortex. Here we show that an efficient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the firing rate – predicts receptive fields and response nonlinearities similar to those observed in the retina. Specifically, we develop numerical methods for simultaneously learning the linear filters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields filters that are center-surround and nonlinearities that are rectifying. The filters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have filters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal filters are localized and oriented.},
  langid   = {english},
  file     = {/Users/jeremie/Dropbox/Journal Club/Karklin et Simoncelli - Efficient coding of natural images with a populati.pdf}
}

@article{kedlianVariabilityExpressionMany2019,
  title        = {The Variability of Expression of Many Genes and Most Functional Pathways Is Observed to Increase with Age in Brain Transcriptome Data},
  author       = {Kedlian, Veronika R and Donertas, Handan Melike and Thornton, Janet M},
  date         = {2019-01-22},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbd5},
  url          = {http://biorxiv.org/lookup/doi/10.1101/526491},
  urldate      = {2019-03-22},
  abstract     = {Ageing is broadly defined as a time-dependent progressive decline in the functional and physiological integrity of organisms. Previous studies and evolutionary theories of ageing suggest that ageing is not a programmed process but reflects dynamic stochastic events. In this study, we test whether transcriptional noise shows an increase with age, which would be expected from stochastic theories. Using human brain transcriptome dataset, we analysed the heterogeneity in the transcriptome for individual genes and functional pathways, employing different analysis methods and pre-processing steps. We show that unlike expression level changes, changes in heterogeneity are highly dependent on the methodology and the underlying assumptions. Although the particular set of genes that can be characterized as differentially variable is highly dependent on the methods, we observe a consistent increase in heterogeneity at every level, independent of the method. In particular, we demonstrate a weak but reproducible transcriptome-wide shift towards an increase in heterogeneity, with twice as many genes significantly increasing as opposed to decreasing their heterogeneity. Furthermore, this pattern of increasing heterogeneity is not specific but is associated with a wide range of pathways.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/BW3LH68C/Kedlian et al. - 2019 - The variability of expression of many genes and mo.pdf}
}

@misc{kedzierskaAssessingLimitsZeroshot2023,
  title    = {Assessing the Limits of Zero-Shot Foundation Models in Single-Cell Biology},
  author   = {Kedzierska, Kasia Z. and Crawford, Lorin and Amini, Ava P. and Lu, Alex X.},
  year     = {2023},
  month    = oct,
  doi      = {10.1101/2023.10.16.561085},
  urldate  = {2024-04-18},
  abstract = {Abstract                        The advent and success of foundation models such as GPT has sparked growing interest in their application to single-cell biology. Models like Geneformer and scGPT have emerged with the promise of serving as versatile tools for this specialized field. However, the efficacy of these models, particularly in zero-shot settings where models are not fine-tuned but used without any further training, remains an open question, especially as practical constraints require useful models to function in settings that preclude fine-tuning (e.g., discovery settings where labels are not fully known). This paper presents a rigorous evaluation of the zero-shot performance of these proposed single-cell foundation models. We assess their utility in tasks such as cell type clustering and batch effect correction, and evaluate the generality of their pretraining objectives. Our results indicate that both Geneformer and scGPT exhibit limited reliability in zero-shot settings and often underperform compared to simpler methods. These findings serve as a cautionary note for the deployment of proposed single-cell foundation models and highlight the need for more focused research to realize their potential.             2},
  langid   = {english},
  file     = {/Users/jkobject/Zotero/storage/CC7KZK9Z/Kedzierska et al. - 2023 - Assessing the limits of zero-shot foundation model.pdf}
}

@article{kelleyBassetLearningRegulatory2016,
  title        = {Basset: Learning the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks},
  shorttitle   = {Basset},
  author       = {Kelley, David R. and Snoek, Jasper and Rinn, John L.},
  date         = {2016-07},
  journaltitle = {Genome Research},
  volume       = {26},
  number       = {7},
  pages        = {990--999},
  issn         = {1088-9051, 1549-5469},
  doi          = {10/f8sw35},
  url          = {http://genome.cshlp.org/lookup/doi/10.1101/gr.200535.115},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{kelleySequentialRegulatoryActivity2018,
  title        = {Sequential Regulatory Activity Prediction across Chromosomes with Convolutional Neural Networks},
  author       = {Kelley, David R and Reshef, Yakir A. and Bileschi, Maxwell and Belanger, David and McLean, Cory and Snoek, Jasper},
  date         = {2018-03-22},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbfh},
  url          = {http://biorxiv.org/lookup/doi/10.1101/161851},
  urldate      = {2019-03-22},
  abstract     = {Models for predicting phenotypic outcomes from genotypes have important applications to understanding genomic function and improving human health. Here, we develop a machinelearning system to predict cell type-specific epigenetic and transcriptional profiles in large mammalian genomes from DNA sequence alone. Using convolutional neural networks, this system identifies promoters and distal regulatory elements and synthesizes their content to make effective gene expression predictions. We show that model predictions for the influence of genomic variants on gene expression align well to causal variants underlying eQTLs in human populations and can be useful for generating mechanistic hypotheses to enable fine mapping of disease loci.},
  langid       = {english},
  annotation   = {00000}
}

@article{kidderChIPSeqTechnicalConsiderations2011,
  title      = {{{ChIP-Seq}}: {{Technical Considerations}} for {{Obtaining High Quality Data}}},
  shorttitle = {{{ChIP-Seq}}},
  author     = {Kidder, Benjamin L. and Hu, Gangqing and Zhao, Keji},
  year       = {2011},
  month      = sep,
  journal    = {Nature immunology},
  volume     = {12},
  number     = {10},
  pages      = {918--922},
  issn       = {1529-2908},
  doi        = {10.1038/ni.2117},
  urldate    = {2024-07-19},
  abstract   = {Chromatin immunoprecipitation followed by sequencing analysis (ChIP-Seq) is a powerful method to investigate genome-wide distributions of chromatin-binding proteins and histone modifications in any genome with a known sequence. Application of this technique to a variety of developmental and differentiation systems has provided global views of cis regulatory elements, transcription factor function, and epigenetic processes involved in the control of gene transcription. Here, we describe several technical aspects of the ChIP-Seq assay to reduce bias and background noise, and to consistently generate high quality data.},
  pmcid      = {PMC3541830},
  pmid       = {21934668},
  file       = {/Users/jkobject/Zotero/storage/PN4F3JVR/Kidder et al. - 2011 - ChIP-Seq Technical Considerations for Obtaining H.pdf}
}

@unpublished{killoranGeneratingDesigningDNA2017,
  title       = {Generating and Designing {{DNA}} with Deep Generative Models},
  author      = {Killoran, Nathan and Lee, Leo J. and Delong, Andrew and Duvenaud, David and Frey, Brendan J.},
  date        = {2017-12-17},
  eprint      = {1712.06148},
  eprinttype  = {arXiv},
  eprintclass = {cs, q-bio, stat},
  url         = {http://arxiv.org/abs/1712.06148},
  urldate     = {2019-03-22},
  abstract    = {We propose generative neural network methods to generate DNA sequences and tune them to have desired properties. We present three approaches: creating synthetic DNA sequences using a generative adversarial network (GAN); a DNAbased variant of the activation maximization (“deep dream”) design method; and a joint procedure which combines these two approaches together. We show that these tools capture important structures of the data and, when applied to designing probes for protein binding microarrays (PBMs), allow us to generate new sequences whose properties are estimated to be superior to those found in the training data. We believe that these results open the door for applying deep generative models to advance genomics research.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Quantitative Biology - Genomics,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{kilpinenCoordinatedEffectsSequence2013,
  title        = {Coordinated {{Effects}} of {{Sequence Variation}} on {{DNA Binding}}, {{Chromatin Structure}}, and {{Transcription}}},
  author       = {Kilpinen, H. and Waszak, S. M. and Gschwind, A. R. and Raghav, S. K. and Witwicki, R. M. and Orioli, A. and Migliavacca, E. and Wiederkehr, M. and Gutierrez-Arcelus, M. and Panousis, N. I. and Yurovsky, A. and Lappalainen, T. and Romano-Palumbo, L. and Planchon, A. and Bielser, D. and Bryois, J. and Padioleau, I. and Udin, G. and Thurnheer, S. and Hacker, D. and Core, L. J. and Lis, J. T. and Hernandez, N. and Reymond, A. and Deplancke, B. and Dermitzakis, E. T.},
  date         = {2013-11-08},
  journaltitle = {Science},
  volume       = {342},
  number       = {6159},
  pages        = {744--747},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/f5f4pv},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.1242463},
  urldate      = {2018-04-11},
  abstract     = {DNA sequence variation has been associated with quantitative changes in molecular phenotypes such as gene expression, but its impact on chromatin states is poorly characterized. To understand the interplay between chromatin and genetic control of gene regulation we quantified allelic variability in transcription factor binding, histone modifications, and gene expression within humans. We found abundant allelic specificity in chromatin and extensive local, short-, and longrange allelic coordination among the studied molecular phenotypes. We observed genetic influence on most of these phenotypes, with histone modifications exhibiting strong contextdependent behavior. Our results implicate transcription factors as primary mediators of sequencespecific regulation of gene expression programs, with histone modifications frequently reflecting the primary regulatory event.},
  langid       = {english}
}

@misc{kingmaAdamMethodStochastic2017,
  title         = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle    = {Adam},
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  year          = 2017,
  month         = jan,
  number        = {arXiv:1412.6980},
  eprint        = {1412.6980},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1412.6980},
  urldate       = {2025-12-02},
  abstract      = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/LL4D836U/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/jkobject/Zotero/storage/8Y2WR4JH/1412.html}
}

@unpublished{kingmaAutoEncodingVariationalBayes2013,
  title       = {Auto-{{Encoding Variational Bayes}}},
  author      = {Kingma, Diederik P. and Welling, Max},
  date        = {2013-12-20},
  eprint      = {1312.6114},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1312.6114},
  urldate     = {2019-03-22},
  abstract    = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{kirkpatrickOvercomingCatastrophicForgetting2017,
  title        = {Overcoming Catastrophic Forgetting in Neural Networks},
  author       = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  date         = {2017-03-28},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume       = {114},
  number       = {13},
  pages        = {3521--3526},
  issn         = {0027-8424, 1091-6490},
  doi          = {10/gfvjcp},
  url          = {http://www.pnas.org/lookup/doi/10.1073/pnas.1611835114},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@article{kladi-skandaliExpressionalProfilingClinical2018,
  title        = {Expressional Profiling and Clinical Relevance of {{RNase}} K in Prostate Cancer: A Novel Indicator of Favorable Progression-Free Survival},
  shorttitle   = {Expressional Profiling and Clinical Relevance of {{RNase}} K in Prostate Cancer},
  author       = {Kladi-Skandali, Athina and Mavridis, Konstantinos and Scorilas, Andreas and Sideris, Diamantis C.},
  date         = {2018-10},
  journaltitle = {Journal of Cancer Research and Clinical Oncology},
  shortjournal = {J Cancer Res Clin Oncol},
  volume       = {144},
  number       = {10},
  eprint       = {30054827},
  eprinttype   = {pmid},
  pages        = {2049--2057},
  issn         = {1432-1335},
  doi          = {10.1007/s00432-018-2719-0},
  abstract     = {PURPOSE: Considering the unmet need for novel molecular tumor markers capable of improving prostate cancer (CaP) patients' management along with the fruitful results regarding the future use of ribonucleases (RNases) as molecular diagnostic and prognostic markers in CaP, we aimed to study the expressional profile of RNase K in CaP and BPH and to investigate its clinical significance in CaP. METHODS: Total RNA was extracted from 212 prostatic tissue samples (101 BPH and 111 CaP) and, following cDNA synthesis, quantitative real-time PCR (qPCR) was performed for the expressional quantification of RNase K. Extensive statistical analysis, including bootstrap resampling, was performed to investigate the differential expression of RNase K in patients with BPH and CaP and its associations with patients' clinicopathological and survival data. RESULTS: RNase K was significantly downregulated (P\,=\,0.002) in CaP patients compared to BPH ones. RNase K overexpression was associated with decreased risk of CaP development and can discriminate between CaP and BPH independently of serum PSA levels (crude odds ratio\,=\,0.93, P\,=\,0.001). RNase K upregulation was also associated with less advanced (P\,=\,0.018) and less aggressive (P\,=\,0.001) tumors as well as with longer progression-free survival (PFS) (P\,=\,0.003). Finally univariate bootstrap Cox regression confirmed that RNase K was associated with favorable prognosis (HR\,=\,0.85, P\,=\,0.002). CONCLUSIONS: RNase K is a biomarker of favorable prognosis in CaP, which is significantly associated with less advanced and aggressive disease, as well as with enhanced PFS.},
  langid       = {english},
  keywords     = {Aged,Biochemical recurrence,Biomarkers Tumor,Case-Control Studies,Endoribonucleases,Follow-Up Studies,Gene Expression Profiling,Humans,Male,Middle Aged,Molecular tumor markers,Prognosis,Prostate cancer biomarkers,Prostate cancer prognosis,Prostatic Hyperplasia,Prostatic Neoplasms,Ribonucleases,Survival Rate}
}

@article{kongLandscapeImmuneDysregulation2023,
  title     = {The Landscape of Immune Dysregulation in {{Crohn}}'s Disease Revealed through Single-Cell Transcriptomic Profiling in the Ileum and Colon},
  author    = {Kong, Lingjia and Pokatayev, Vladislav and Lefkovith, Ariel and Carter, Grace T. and Creasey, Elizabeth A. and Krishna, Chirag and Subramanian, Sathish and Kochar, Bharati and Ashenberg, Orr and Lau, Helena and Ananthakrishnan, Ashwin N. and Graham, Daniel B. and Deguine, Jacques and Xavier, Ramnik J.},
  year      = {2023},
  month     = feb,
  journal   = {Immunity},
  volume    = {56},
  number    = {2},
  pages     = {444-458.e5},
  publisher = {Elsevier},
  issn      = {1074-7613},
  doi       = {10.1016/j.immuni.2023.01.002},
  urldate   = {2024-07-23},
  langid    = {english},
  pmid      = {36720220},
  keywords  = {Crohn's disease,high-dimensional profiling,IBD,inflammation,inflammatory bowel disease,myofibroblasts,single-cell RNA sequencing},
  file      = {/Users/jkobject/Zotero/storage/Q8EA8MGJ/Kong et al. - 2023 - The landscape of immune dysregulation in Crohn’s d.pdf}
}

@article{korsunskyFastSensitiveAccurate2019,
  title     = {Fast, Sensitive and Accurate Integration of Single-Cell Data with {{Harmony}}},
  author    = {Korsunsky, Ilya and Millard, Nghia and Fan, Jean and Slowikowski, Kamil and Zhang, Fan and Wei, Kevin and Baglaenko, Yuriy and Brenner, Michael and Loh, Po-ru and Raychaudhuri, Soumya},
  year      = {2019},
  month     = dec,
  journal   = {Nature Methods},
  volume    = {16},
  number    = {12},
  pages     = {1289--1296},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-019-0619-0},
  urldate   = {2024-07-19},
  abstract  = {The emerging diversity of single-cell RNA-seq datasets allows for the full transcriptional characterization of cell types across a wide variety of biological and clinical conditions. However, it is challenging to analyze them together, particularly when datasets are assayed with different technologies, because biological and technical differences are interspersed. We present Harmony (https://github.com/immunogenomics/harmony), an algorithm that projects cells into a shared embedding in which cells group by cell type rather than dataset-specific conditions. Harmony simultaneously accounts for multiple experimental and biological factors. In six analyses, we demonstrate the superior performance of Harmony to previously published algorithms while requiring fewer computational resources. Harmony enables the integration of {\textasciitilde}106 cells on a personal computer. We apply Harmony to peripheral blood mononuclear cells from datasets with large experimental differences, five studies of pancreatic islet cells, mouse embryogenesis datasets and the integration of scRNA-seq with spatial transcriptomics data.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Computational models,Data integration,Statistical methods},
  file      = {/Users/jkobject/Zotero/storage/RATAS7MB/Korsunsky et al. - 2019 - Fast, sensitive and accurate integration of single.pdf}
}

@misc{kovacevicNoFoundationsFoundations2025,
  title         = {No {{Foundations}} without {{Foundations}} -- {{Why}} Semi-Mechanistic Models Are Essential for Regulatory Biology},
  author        = {Kova{\v c}evi{\'c}, Luka and Gaudelet, Thomas and Opzoomer, James and Triendl, Hagen and Whittaker, John and Uhler, Caroline and Edwards, Lindsay and {Taylor-King}, Jake P.},
  year          = {2025},
  month         = jan,
  number        = {arXiv:2501.19178},
  eprint        = {2501.19178},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2501.19178},
  urldate       = {2025-02-06},
  abstract      = {Despite substantial efforts, deep learning has not yet delivered a transformative impact on elucidating regulatory biology, particularly in the realm of predicting gene expression profiles. Here, we argue that genuine "foundation models" of regulatory biology will remain out of reach unless guided by frameworks that integrate mechanistic insight with principled experimental design. We present one such ground-up, semi-mechanistic framework that unifies perturbation-based experimental designs across both in vitro and in vivo CRISPR screens, accounting for differentiating and non-differentiating cellular systems. By revealing previously unrecognised assumptions in published machine learning methods, our approach clarifies links with popular techniques such as variational autoencoders and structural causal models. In practice, this framework suggests a modified loss function that we demonstrate can improve predictive performance, and further suggests an error analysis that informs batching strategies. Ultimately, since cellular regulation emerges from innumerable interactions amongst largely uncharted molecular components, we contend that systems-level understanding cannot be achieved through structural biology alone. Instead, we argue that real progress will require a first-principles perspective on how experiments capture biological phenomena, how data are generated, and how these processes can be reflected in more faithful modelling architectures.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/DXE2PTCV/Kovačević et al. - 2025 - No Foundations without Foundations -- Why semi-mec.pdf;/Users/jkobject/Zotero/storage/WVZ98K2X/2501.html}
}

@inproceedings{kozinskyScalingLeadingAccuracy2023,
  title     = {Scaling the {{Leading Accuracy}} of {{Deep Equivariant Models}} to {{Biomolecular Simulations}} of {{Realistic Size}}},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author    = {Kozinsky, Boris and Musaelian, Albert and Johansson, Anders and Batzner, Simon},
  year      = {2023},
  month     = nov,
  series    = {{{SC}} '23},
  pages     = {1--12},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  doi       = {10.1145/3581784.3627041},
  urldate   = {2025-02-25},
  abstract  = {This work brings the leading accuracy, sample efficiency, and robustness of deep equivariant neural networks to the extreme computational scale. This is achieved through a combination of innovative model architecture, massive parallelization, and models and implementations optimized for efficient GPU utilization. The resulting Allegro architecture bridges the accuracy-speed tradeoff of atomistic simulations and enables description of dynamics in structures of unprecedented complexity at quantum fidelity. To illustrate the scalability of Allegro, we perform nanoseconds-long stable simulations of protein dynamics and scale up to a 44-million atom structure of a complete, all-atom, explicitly solvated HIV capsid on the Perlmutter supercomputer. We demonstrate excellent strong scaling up to 100 million atoms and 70\% weak scaling to 5120 A100 GPUs.},
  isbn      = {9798400701092},
  file      = {/Users/jkobject/Zotero/storage/RWAFGGU8/Kozinsky et al. - 2023 - Scaling the Leading Accuracy of Deep Equivariant M.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title        = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date         = {2017-05-24},
  journaltitle = {Communications of the ACM},
  volume       = {60},
  number       = {6},
  pages        = {84--90},
  issn         = {00010782},
  doi          = {10/gbhhxs},
  url          = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
  urldate      = {2019-03-22},
  abstract     = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid       = {english},
  annotation   = {00000}
}

@article{kumarAskMeAnything,
  title    = {Ask {{Me Anything}}:{{Dynamic Memory Networks}} for {{Natural Language Processing}}},
  author   = {Kumar, Ankit and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  pages    = {10},
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook’s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
  langid   = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file     = {/Users/jeremie/Documents/science/ML/NN/Kumar et al. - Ask Me AnythingDynamic Memory Networks for Natura.pdf}
}

@article{kusumotoAutomatedDeepLearningBased2018,
  title        = {Automated {{Deep Learning-Based System}} to {{Identify Endothelial Cells Derived}} from {{Induced Pluripotent Stem Cells}}},
  author       = {Kusumoto, Dai and Lachmann, Mark and Kunihiro, Takeshi and Yuasa, Shinsuke and Kishino, Yoshikazu and Kimura, Mai and Katsuki, Toshiomi and Itoh, Shogo and Seki, Tomohisa and Fukuda, Keiichi},
  date         = {2018-06},
  journaltitle = {Stem Cell Reports},
  volume       = {10},
  number       = {6},
  pages        = {1687--1695},
  issn         = {22136711},
  doi          = {10/gfxbgt},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S2213671118301759},
  urldate      = {2019-03-22},
  abstract     = {Deep learning technology is rapidly advancing and is now used to solve complex problems. Here, we used deep learning in convolutional neural networks to establish an automated method to identify endothelial cells derived from induced pluripotent stem cells (iPSCs), without the need for immunostaining or lineage tracing. Networks were trained to predict whether phase-contrast images contain endothelial cells based on morphology only. Predictions were validated by comparison to immunofluorescence staining for CD31, a marker of endothelial cells. Method parameters were then automatically and iteratively optimized to increase prediction accuracy. We found that prediction accuracy was correlated with network depth and pixel size of images to be analyzed. Finally, K-fold cross-validation confirmed that optimized convolutional neural networks can identify endothelial cells with high performance, based only on morphology.},
  langid       = {english}
}

@article{kuznickiCalcyclinMarkerHuman1992,
  title        = {Calcyclin as a Marker of Human Epithelial Cells and Fibroblasts},
  author       = {Kuźnicki, J. and Kordowska, J. and Puzianowska, M. and Woźniewicz, B. M.},
  date         = {1992-06-01},
  journaltitle = {Experimental Cell Research},
  shortjournal = {Experimental Cell Research},
  volume       = {200},
  number       = {2},
  pages        = {425--430},
  issn         = {0014-4827},
  doi          = {10.1016/0014-4827(92)90191-A},
  url          = {https://www.sciencedirect.com/science/article/pii/001448279290191A},
  urldate      = {2024-07-26},
  abstract     = {The distribution of calcyclin in human tissues was studied using polyclonal antibodies against this protein. In all organs examined (breast, heart, intestine, kidney, liver, ovary, placenta, stomach, thymus, and uterus) only epithelial cells and fibroblasts were stained. This suggests that calcyclin expression is related either to proliferation rate or secretion activity. The data show that calcyclin might be considered as a marker of some human epithelial cells and fibroblasts.},
  file         = {/Users/jkobject/Zotero/storage/IF2ENLKW/001448279290191A.html}
}

@incollection{lachiche1BC2TrueFirstOrder2003,
  title       = {{{1BC2}}: {{A True First-Order Bayesian Classifier}}},
  shorttitle  = {{{1BC2}}},
  booktitle   = {Inductive {{Logic Programming}}},
  author      = {Lachiche, Nicolas and Flach, Peter A.},
  editor      = {Matwin, Stan and Sammut, Claude},
  editora     = {Goos, G. and Hartmanis, J. and family=Leeuwen, given=J., prefix=van, useprefix=true},
  editoratype = {redactor},
  date        = {2003},
  volume      = {2583},
  pages       = {133--148},
  publisher   = {Springer Berlin Heidelberg},
  location    = {Berlin, Heidelberg},
  doi         = {10.1007/3-540-36468-4_9},
  url         = {http://link.springer.com/10.1007/3-540-36468-4_9},
  urldate     = {2018-04-11},
  abstract    = {In previous work [3] we presented 1BC, a first-order Bayesian classifier. 1BC applies dynamic propositionalisation, in the sense that attributes representing first-order features are generated exhaustively within a given feature bias, but during learning rather than as a pre-processing step. In this paper we describe 1BC2, which learns from structured data by fitting various parametric distributions over sets and lists to the data. We evaluate the feasibility of the approach by various experiments.},
  isbn        = {978-3-540-00567-4 978-3-540-36468-9},
  langid      = {english},
  annotation  = {00000}
}

@article{lahnemannElevenGrandChallenges2020,
  title    = {Eleven Grand Challenges in Single-Cell Data Science},
  author   = {L{\"a}hnemann, David and K{\"o}ster, Johannes and Szczurek, Ewa and McCarthy, Davis J. and Hicks, Stephanie C. and Robinson, Mark D. and Vallejos, Catalina A. and Campbell, Kieran R. and Beerenwinkel, Niko and Mahfouz, Ahmed and Pinello, Luca and Skums, Pavel and Stamatakis, Alexandros and Attolini, Camille Stephan-Otto and Aparicio, Samuel and Baaijens, Jasmijn and Balvert, Marleen and de Barbanson, Buys and Cappuccio, Antonio and Corleone, Giacomo and Dutilh, Bas E. and Florescu, Maria and Guryev, Victor and Holmer, Rens and Jahn, Katharina and Lobo, Thamar Jessurun and Keizer, Emma M. and Khatri, Indu and Kielbasa, Szymon M. and Korbel, Jan O. and Kozlov, Alexey M. and Kuo, Tzu-Hao and Lelieveldt, Boudewijn P.F. and Mandoiu, Ion I. and Marioni, John C. and Marschall, Tobias and M{\"o}lder, Felix and Niknejad, Amir and R{\k a}czkowska, Alicja and Reinders, Marcel and de Ridder, Jeroen and Saliba, Antoine-Emmanuel and Somarakis, Antonios and Stegle, Oliver and Theis, Fabian J. and Yang, Huan and Zelikovsky, Alex and McHardy, Alice C. and Raphael, Benjamin J. and Shah, Sohrab P. and Sch{\"o}nhuth, Alexander},
  year     = {2020},
  month    = feb,
  journal  = {Genome Biology},
  volume   = {21},
  number   = {1},
  pages    = {31},
  issn     = {1474-760X},
  doi      = {10.1186/s13059-020-1926-6},
  urldate  = {2024-04-19},
  abstract = {The recent boom in microfluidics and combinatorial indexing strategies, combined with low sequencing costs, has empowered single-cell sequencing technology. Thousands---or even millions---of cells analyzed in a single experiment amount to a data revolution in single-cell biology and pose unique data science problems. Here, we outline eleven challenges that will be central to bringing this emerging field of single-cell data science forward. For each challenge, we highlight motivating research questions, review prior work, and formulate open problems. This compendium is for established researchers, newcomers, and students alike, highlighting interesting and rewarding problems for the coming years.},
  file     = {/Users/jkobject/Zotero/storage/AKUZYB88/Lähnemann et al. - 2020 - Eleven grand challenges in single-cell data scienc.pdf;/Users/jkobject/Zotero/storage/3YQR735H/s13059-020-1926-6.html}
}

@article{lakeBuildingMachinesThat2017,
  title        = {Building Machines That Learn and Think like People},
  author       = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  date         = {2017},
  journaltitle = {Behavioral and Brain Sciences},
  volume       = {40},
  issn         = {0140-525X, 1469-1825},
  doi          = {10/gcw3n8},
  url          = {https://www.cambridge.org/core/product/identifier/S0140525X16001837/type/journal_article},
  urldate      = {2018-04-11},
  abstract     = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  langid       = {english},
  keywords     = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file         = {/Users/jeremie/Documents/science/ML/NN/Lake et al. - 2017 - Building machines that learn and think like people.pdf}
}


@article{lamLearningSkillfulMediumrange2023,
  title     = {Learning Skillful Medium-Range Global Weather Forecasting},
  author    = {Lam, Remi and {Sanchez-Gonzalez}, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and {Eaton-Rosen}, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
  year      = 2023,
  month     = dec,
  journal   = {Science},
  volume    = {382},
  number    = {6677},
  pages     = {1416--1421},
  publisher = {American Association for the Advancement of Science},
  doi       = {10.1126/science.adi2336},
  urldate   = {2025-12-02},
  abstract  = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learning--based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25{$^\circ$} resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems.},
  file      = {/Users/jkobject/Zotero/storage/TF3BJPN5/Lam et al. - 2023 - Learning skillful medium-range global weather forecasting.pdf}
}

@article{landerInitialSequencingAnalysis2001,
  title     = {Initial Sequencing and Analysis of the Human Genome},
  author    = {Lander, Eric S. and Linton, Lauren M. and Birren, Bruce and Nusbaum, Chad and Zody, Michael C. and Baldwin, Jennifer and Devon, Keri and Dewar, Ken and Doyle, Michael and FitzHugh, William and Funke, Roel and Gage, Diane and Harris, Katrina and Heaford, Andrew and Howland, John and Kann, Lisa and Lehoczky, Jessica and LeVine, Rosie and McEwan, Paul and McKernan, Kevin and Meldrim, James and Mesirov, Jill P. and Miranda, Cher and Morris, William and Naylor, Jerome and Raymond, Christina and Rosetti, Mark and Santos, Ralph and Sheridan, Andrew and Sougnez, Carrie and {Stange-Thomann}, Nicole and Stojanovic, Nikola and Subramanian, Aravind and Wyman, Dudley and Rogers, Jane and Sulston, John and Ainscough, Rachael and Beck, Stephan and Bentley, David and Burton, John and Clee, Christopher and Carter, Nigel and Coulson, Alan and Deadman, Rebecca and Deloukas, Panos and Dunham, Andrew and Dunham, Ian and Durbin, Richard and French, Lisa and Grafham, Darren and Gregory, Simon and Hubbard, Tim and Humphray, Sean and Hunt, Adrienne and Jones, Matthew and Lloyd, Christine and McMurray, Amanda and Matthews, Lucy and Mercer, Simon and Milne, Sarah and Mullikin, James C. and Mungall, Andrew and Plumb, Robert and Ross, Mark and Shownkeen, Ratna and Sims, Sarah and Waterston, Robert H. and Wilson, Richard K. and Hillier, LaDeana W. and McPherson, John D. and Marra, Marco A. and Mardis, Elaine R. and Fulton, Lucinda A. and Chinwalla, Asif T. and Pepin, Kymberlie H. and Gish, Warren R. and Chissoe, Stephanie L. and Wendl, Michael C. and Delehaunty, Kim D. and Miner, Tracie L. and Delehaunty, Andrew and Kramer, Jason B. and Cook, Lisa L. and Fulton, Robert S. and Johnson, Douglas L. and Minx, Patrick J. and Clifton, Sandra W. and Hawkins, Trevor and Branscomb, Elbert and Predki, Paul and Richardson, Paul and Wenning, Sarah and Slezak, Tom and Doggett, Norman and Cheng, Jan-Fang and Olsen, Anne and Lucas, Susan and Elkin, Christopher and Uberbacher, Edward and Frazier, Marvin and Gibbs, Richard A. and Muzny, Donna M. and Scherer, Steven E. and Bouck, John B. and Sodergren, Erica J. and Worley, Kim C. and Rives, Catherine M. and Gorrell, James H. and Metzker, Michael L. and Naylor, Susan L. and Kucherlapati, Raju S. and Nelson, David L. and Weinstock, George M. and Sakaki, Yoshiyuki and Fujiyama, Asao and Hattori, Masahira and Yada, Tetsushi and Toyoda, Atsushi and Itoh, Takehiko and Kawagoe, Chiharu and Watanabe, Hidemi and Totoki, Yasushi and Taylor, Todd and Weissenbach, Jean and Heilig, Roland and Saurin, William and Artiguenave, Francois and Brottier, Philippe and Bruls, Thomas and Pelletier, Eric and Robert, Catherine and Wincker, Patrick and Rosenthal, Andr{\'e} and Platzer, Matthias and Nyakatura, Gerald and Taudien, Stefan and Rump, Andreas and Smith, Douglas R. and {Doucette-Stamm}, Lynn and Rubenfield, Marc and Weinstock, Keith and Lee, Hong Mei and Dubois, JoAnn and Yang, Huanming and Yu, Jun and Wang, Jian and Huang, Guyang and Gu, Jun and Hood, Leroy and Rowen, Lee and Madan, Anup and Qin, Shizen and Davis, Ronald W. and Federspiel, Nancy A. and Abola, A. Pia and Proctor, Michael J. and Roe, Bruce A. and Chen, Feng and Pan, Huaqin and Ramser, Juliane and Lehrach, Hans and Reinhardt, Richard and McCombie, W. Richard and {de la Bastide}, Melissa and Dedhia, Neilay and Bl{\"o}cker, Helmut and Hornischer, Klaus and Nordsiek, Gabriele and Agarwala, Richa and Aravind, L. and Bailey, Jeffrey A. and Bateman, Alex and Batzoglou, Serafim and Birney, Ewan and Bork, Peer and Brown, Daniel G. and Burge, Christopher B. and Cerutti, Lorenzo and Chen, Hsiu-Chuan and Church, Deanna and Clamp, Michele and Copley, Richard R. and Doerks, Tobias and Eddy, Sean R. and Eichler, Evan E. and Furey, Terrence S. and Galagan, James and Gilbert, James G. R. and Harmon, Cyrus and Hayashizaki, Yoshihide and Haussler, David and Hermjakob, Henning and Hokamp, Karsten and Jang, Wonhee and Johnson, L. Steven and Jones, Thomas A. and Kasif, Simon and Kaspryzk, Arek and Kennedy, Scot and Kent, W. James and Kitts, Paul and Koonin, Eugene V. and Korf, Ian and Kulp, David and Lancet, Doron and Lowe, Todd M. and McLysaght, Aoife and Mikkelsen, Tarjei and Moran, John V. and Mulder, Nicola and Pollara, Victor J. and Ponting, Chris P. and Schuler, Greg and Schultz, J{\"o}rg and Slater, Guy and Smit, Arian F. A. and Stupka, Elia and Szustakowki, Joseph and {Thierry-Mieg}, Danielle and {Thierry-Mieg}, Jean and Wagner, Lukas and Wallis, John and Wheeler, Raymond and Williams, Alan and Wolf, Yuri I. and Wolfe, Kenneth H. and Yang, Shiaw-Pyng and Yeh, Ru-Fang and Collins, Francis and Guyer, Mark S. and Peterson, Jane and Felsenfeld, Adam and Wetterstrand, Kris A. and Myers, Richard M. and Schmutz, Jeremy and Dickson, Mark and Grimwood, Jane and Cox, David R. and Olson, Maynard V. and Kaul, Rajinder and Raymond, Christopher and Shimizu, Nobuyoshi and Kawasaki, Kazuhiko and Minoshima, Shinsei and Evans, Glen A. and Athanasiou, Maria and Schultz, Roger and Patrinos, Aristides and Morgan, Michael J. and {International Human Genome Sequencing Consortium} and {Whitehead Institute for Biomedical Research}, Center for Genome Research: and {The Sanger Centre:} and {Washington University Genome Sequencing Center} and {US DOE Joint Genome Institute:} and {Baylor College of Medicine Human Genome Sequencing Center:} and {RIKEN Genomic Sciences Center:} and {Genoscope and CNRS UMR-8030:} and {Department of Genome Analysis}, Institute of Molecular Biotechnology: and {GTC Sequencing Center:} and {Beijing Genomics Institute/Human Genome Center:} and Multimegabase Sequencing Center, The Institute for Systems Biology: and {Stanford Genome Technology Center:} and {University of Oklahoma's Advanced Center for Genome Technology:} and {Max Planck Institute for Molecular Genetics:} and Cold Spring Harbor Laboratory, Lita Annenberg Hazen Genome Center: and {GBF---German Research Centre for Biotechnology:} and includes individuals listed under other headings):{\null} {*Genome Analysis Group (listed in alphabetical order}, also and {Scientific management: National Human Genome Research Institute}, US National Institutes of Health: and {Stanford Human Genome Center:} and {University of Washington Genome Center:} and {Department of Molecular Biology}, Keio University School of Medicine: and {University of Texas Southwestern Medical Center at Dallas:} and {Office of Science}, US Department of Energy: and {The Wellcome Trust:}},
  year      = 2001,
  month     = feb,
  journal   = {Nature},
  volume    = {409},
  number    = {6822},
  pages     = {860--921},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/35057062},
  urldate   = {2025-12-02},
  abstract  = {The human genome holds an extraordinary trove of information about human development, physiology, medicine and evolution. Here we report the results of an international collaboration to produce and make freely available a draft sequence of the human genome. We also present an initial analysis of the data, describing some of the insights that can be gleaned from the sequence.},
  copyright = {2001 Macmillan Magazines Ltd.},
  langid    = {english},
  keywords  = {Humanities and Social Sciences,multidisciplinary,Science},
  file      = {/Users/jkobject/Zotero/storage/NCKBXFU6/Lander et al. - 2001 - Initial sequencing and analysis of the human genome.pdf}
}

@article{lawrenceDiscoverySaturationAnalysis2014,
  title        = {Discovery and Saturation Analysis of Cancer Genes across 21 Tumour Types},
  author       = {Lawrence, Michael S. and Stojanov, Petar and Mermel, Craig H. and Robinson, James T. and Garraway, Levi A. and Golub, Todd R. and Meyerson, Matthew and Gabriel, Stacey B. and Lander, Eric S. and Getz, Gad},
  date         = {2014-01},
  journaltitle = {Nature},
  volume       = {505},
  number       = {7484},
  pages        = {495--501},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/f5nv94},
  url          = {http://www.nature.com/articles/nature12912},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{lawrenceMutationalHeterogeneityCancer2013,
  title        = {Mutational Heterogeneity in Cancer and the Search for New Cancer-Associated Genes},
  author       = {Lawrence, Michael S. and Stojanov, Petar and Polak, Paz and Kryukov, Gregory V. and Cibulskis, Kristian and Sivachenko, Andrey and Carter, Scott L. and Stewart, Chip and Mermel, Craig H. and Roberts, Steven A. and Kiezun, Adam and Hammerman, Peter S. and McKenna, Aaron and Drier, Yotam and Zou, Lihua and Ramos, Alex H. and Pugh, Trevor J. and Stransky, Nicolas and Helman, Elena and Kim, Jaegil and Sougnez, Carrie and Ambrogio, Lauren and Nickerson, Elizabeth and Shefler, Erica and Cortés, Maria L. and Auclair, Daniel and Saksena, Gordon and Voet, Douglas and Noble, Michael and DiCara, Daniel and Lin, Pei and Lichtenstein, Lee and Heiman, David I. and Fennell, Timothy and Imielinski, Marcin and Hernandez, Bryan and Hodis, Eran and Baca, Sylvan and Dulak, Austin M. and Lohr, Jens and Landau, Dan-Avi and Wu, Catherine J. and Melendez-Zajgla, Jorge and Hidalgo-Miranda, Alfredo and Koren, Amnon and McCarroll, Steven A. and Mora, Jaume and Lee, Ryan S. and Crompton, Brian and Onofrio, Robert and Parkin, Melissa and Winckler, Wendy and Ardlie, Kristin and Gabriel, Stacey B. and Roberts, Charles W. M. and Biegel, Jaclyn A. and Stegmaier, Kimberly and Bass, Adam J. and Garraway, Levi A. and Meyerson, Matthew and Golub, Todd R. and Gordenin, Dmitry A. and Sunyaev, Shamil and Lander, Eric S. and Getz, Gad},
  date         = {2013-07},
  journaltitle = {Nature},
  volume       = {499},
  number       = {7457},
  pages        = {214--218},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/mw8},
  url          = {http://www.nature.com/articles/nature12213},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{lecunDeepLearning2015,
  title     = {Deep Learning},
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year      = 2015,
  month     = may,
  journal   = {Nature},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/nature14539},
  urldate   = {2025-12-02},
  abstract  = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Springer Nature Limited},
  langid    = {english},
  keywords  = {Computer science,Mathematics and computing},
  file      = {/Users/jkobject/Zotero/storage/VALPYP9W/LeCun et al. - 2015 - Deep learning.pdf}
}

@misc{leeNVEmbedImprovedTechniques2025,
  title         = {{{NV-Embed}}: {{Improved Techniques}} for {{Training LLMs}} as {{Generalist Embedding Models}}},
  shorttitle    = {{{NV-Embed}}},
  author        = {Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  year          = {2025},
  month         = jan,
  number        = {arXiv:2405.17428},
  eprint        = {2405.17428},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2405.17428},
  urldate       = {2025-02-06},
  abstract      = {Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last {$<$}EOS{$>$} token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024 and August 30, 2024, respectively) across 56 embedding tasks, demonstrating the sustained effectiveness of the proposed methods over time. Additionally, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/7YGGQPBC/Lee et al. - 2025 - NV-Embed Improved Techniques for Training LLMs as.pdf;/Users/jkobject/Zotero/storage/MXPQXCWQ/2405.html}
}

@misc{leeSetTransformerFramework2019,
  title  = {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  author = {Lee, Juho and others},
  year   = {2019},
  doi    = {10.48550/arXiv.1810.00825}
}

@article{leisersonPancancerNetworkAnalysis2015,
  title        = {Pan-Cancer Network Analysis Identifies Combinations of Rare Somatic Mutations across Pathways and Protein Complexes},
  author       = {Leiserson, Mark D M and Vandin, Fabio and Wu, Hsin-Ta and Dobson, Jason R and Eldridge, Jonathan V and Thomas, Jacob L and Papoutsaki, Alexandra and Kim, Younhun and Niu, Beifang and McLellan, Michael and Lawrence, Michael S and Gonzalez-Perez, Abel and Tamborero, David and Cheng, Yuwei and Ryslik, Gregory A and Lopez-Bigas, Nuria and Getz, Gad and Ding, Li and Raphael, Benjamin J},
  date         = {2015-02},
  journaltitle = {Nature Genetics},
  volume       = {47},
  number       = {2},
  pages        = {106--114},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gfxbgq},
  url          = {http://www.nature.com/articles/ng.3168},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{leoteRegulatoryNetworkbasedImputation2022,
  title     = {Regulatory Network-Based Imputation of Dropouts in Single-Cell {{RNA}} Sequencing Data},
  author    = {Leote, Ana Carolina and Wu, Xiaohui and Beyer, Andreas},
  year      = {2022},
  month     = feb,
  journal   = {PLOS Computational Biology},
  volume    = {18},
  number    = {2},
  pages     = {e1009849},
  publisher = {Public Library of Science},
  issn      = {1553-7358},
  doi       = {10.1371/journal.pcbi.1009849},
  urldate   = {2024-07-15},
  abstract  = {Single-cell RNA sequencing (scRNA-seq) methods are typically unable to quantify the expression levels of all genes in a cell, creating a need for the computational prediction of missing values (`dropout imputation'). Most existing dropout imputation methods are limited in the sense that they exclusively use the scRNA-seq dataset at hand and do not exploit external gene-gene relationship information. Further, it is unknown if all genes equally benefit from imputation or which imputation method works best for a given gene. Here, we show that a transcriptional regulatory network learned from external, independent gene expression data improves dropout imputation. Using a variety of human scRNA-seq datasets we demonstrate that our network-based approach outperforms published state-of-the-art methods. The network-based approach performs particularly well for lowly expressed genes, including cell-type-specific transcriptional regulators. Further, the cell-to-cell variation of 11.3\% to 48.8\% of the genes could not be adequately imputed by any of the methods that we tested. In those cases gene expression levels were best predicted by the mean expression across all cells, i.e. assuming no measurable expression variation between cells. These findings suggest that different imputation methods are optimal for different genes. We thus implemented an R-package called ADImpute (available via Bioconductor https://bioconductor.org/packages/release/bioc/html/ADImpute.html) that automatically determines the best imputation method for each gene in a dataset. Our work represents a paradigm shift by demonstrating that there is no single best imputation method. Instead, we propose that imputation should maximally exploit external information and be adapted to gene-specific features, such as expression level and expression variation across cells.},
  langid    = {english},
  keywords  = {Cell differentiation,Gene expression,Gene prediction,Gene regulation,Gene regulatory networks,Genetic networks,Network analysis,Transcriptional control},
  file      = {/Users/jkobject/Zotero/storage/VVJCWJLX/Leote et al. - 2022 - Regulatory network-based imputation of dropouts in.pdf}
}

@article{liAccumulationNCOA1Dependent2023,
  title        = {Accumulation of {{NCOA1}} Dependent on {{HERC3}} Deficiency Transactivates Matrix Metallopeptidases and Promotes Extracellular Matrix Degradation in Intervertebral Disc Degeneration},
  author       = {Li, Xingguo and Wang, Xuenan and Chen, Chao and Zhang, Enyu and Zhang, Yuan and Li, Hongkun and Lei, Yu and Lou, Zhenkai and Zhang, Fan},
  date         = {2023-05-01},
  journaltitle = {Life Sciences},
  shortjournal = {Life Sciences},
  volume       = {320},
  pages        = {121555},
  issn         = {0024-3205},
  doi          = {10.1016/j.lfs.2023.121555},
  url          = {https://www.sciencedirect.com/science/article/pii/S0024320523001893},
  urldate      = {2024-07-26},
  abstract     = {Background Matrix metallopeptidases (MMPs) are critical matrix-degrading molecules and they are frequently overexpressed in degenerative discs. This study aimed to investigate the mechanism for MMP upregulation. Methods Immunoblot and RT-qPCR were used for detecting protein and gene expression levels. 4-month-old and 24-month-old C57BL/6 mice were used for evaluating intervertebral disc degeneration (IDD). An ubiquitination assay was used to determine protein modification. Immunoprecipitation and mass spectrometry were used for identifying protein complex members. Results We identified the elevation of 14 MMPs among 23 members in aged mice with IDD. Eleven of these 14 MMP gene promoters contained a Runx2 (runt-related transcription factor 2) binding site. Biochemical analyses revealed that Runx2 recruited a histone acetyltransferase p300 and a coactivator NCOA1 (nuclear receptor coactivator 1) to assemble a complex, transactivating MMP expression. The deficiency of an E3 ligase called HERC3 (HECT and RLD domain containing E3 ubiquitin-protein ligase 3) resulted in the accumulation of NCOA1 in the inflammatory microenvironment. High throughput screening of small molecules that specifically target the NCOA1-p300 interaction identified a compound SMTNP-191, which showed an inhibitory effect on suppressing MMP expression and attenuating the IDD process in aged mice. Conclusion Our data support a model in which deficiency of HERC3 fails to ubiquitinate NCOA1, leading to the assembly of NCOA1-p300-Runx2 and causing the transactivation of MMPs. These findings offer new insight into inflammation-mediated MMP accumulation and also provide a new therapeutic strategy to retard the IDD process.},
  keywords     = {HECT and RLD domain containing E3 ubiquitin-protein ligase 3,Histone acetyltransferase p300,Intervertebral disc degeneration,Matrix metallopeptidases,Nuclear receptor coactivator 1,Runt-related transcription factor 2},
  file         = {/Users/jkobject/Zotero/storage/4CDIJIL8/S0024320523001893.html}
}

@article{libbrechtMachineLearningApplications2015,
  title        = {Machine Learning Applications in Genetics and Genomics},
  author       = {Libbrecht, Maxwell W. and Noble, William Stafford},
  date         = {2015-06},
  journaltitle = {Nature Reviews Genetics},
  volume       = {16},
  number       = {6},
  pages        = {321--332},
  issn         = {1471-0056, 1471-0064},
  doi          = {10/gcgk8p},
  url          = {http://www.nature.com/articles/nrg3920},
  urldate      = {2019-03-22},
  abstract     = {The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.},
  langid       = {english},
  annotation   = {00000}
}

@article{liberzonMolecularSignaturesDatabase2015,
  title    = {The {{Molecular Signatures Database}} ({{MSigDB}}) Hallmark Gene Set Collection},
  author   = {Liberzon, Arthur and Birger, Chet and Thorvaldsd{\'o}ttir, Helga and Ghandi, Mahmoud and Mesirov, Jill P. and Tamayo, Pablo},
  year     = {2015},
  month    = dec,
  journal  = {Cell Systems},
  volume   = {1},
  number   = {6},
  pages    = {417--425},
  issn     = {2405-4712},
  doi      = {10.1016/j.cels.2015.12.004},
  abstract = {The Molecular Signatures Database (MSigDB) is one of the most widely used and comprehensive databases of gene sets for performing gene set enrichment analysis. Since its creation, MSigDB has grown beyond its roots in metabolic disease and cancer to include {$>$}10,000 gene sets. These better represent a wider range of biological processes and diseases, but the utility of the database is reduced by increased redundancy across, and heterogeneity within, gene sets. To address this challenge, here we use a combination of automated approaches and expert curation to develop a collection of "hallmark" gene sets as part of MSigDB. Each hallmark in this collection consists of a "refined" gene set, derived from multiple "founder" sets, that conveys a specific biological state or process and displays coherent expression. The hallmarks effectively summarize most of the relevant information of the original founder sets and, by reducing both variation and redundancy, provide more refined and concise inputs for gene set enrichment analysis.},
  langid   = {english},
  pmcid    = {PMC4707969},
  pmid     = {26771021},
  keywords = {gene expression,gene set enrichment analysis,gene sets},
  file     = {/Users/jkobject/Zotero/storage/W34DFR6T/Liberzon et al. - 2015 - The Molecular Signatures Database (MSigDB) hallmar.pdf}
}

@article{liCancerMetastasisDetection,
  title      = {Cancer {{Metastasis Detection With Neural Conditional Random Field}}},
  author     = {Li, Yi and Ping, Wei},
  pages      = {9},
  abstract   = {Breast cancer diagnosis often requires accurate detection of metastasis in lymph nodes through Whole-slide Images (WSIs). Recent advances in deep convolutional neural networks (CNNs) have shown significant successes in medical image analysis and particularly in computational histopathology. Because of the outrageous large size of WSIs, most of the methods divide one slide into lots of small image patches and perform classification on each patch independently. However, neighboring patches often share spatial correlations, and ignoring these spatial correlations may result in inconsistent predictions. In this paper, we propose a neural conditional random field (NCRF) deep learning framework to detect cancer metastasis in WSIs. NCRF considers the spatial correlations between neighboring patches through a fully connected CRF which is directly incorporated on top of a CNN feature extractor. The whole deep network can be trained end-to-end with standard back-propagation algorithm with minor computational overhead from the CRF component. The CNN feature extractor can also benefit from considering spatial correlations via the CRF component. Compared to the baseline method without considering spatial correlations, we show that the proposed NCRF framework obtains probability maps of patch predictions with better visual quality. We also demonstrate that our method outperforms the baseline in cancer metastasis detection on the Camelyon16 dataset and achieves an average FROC score of 0.8096 on the test set. NCRF is open sourced at https://github.com/baidu-research/NCRF.},
  langid     = {english},
  annotation = {00000}
}

@article{liDetectingCircularRNA2019,
  title        = {Detecting {{Circular RNA}} from {{High-throughput Sequence Data}} with de {{Bruijn Graph}}},
  author       = {Li, Xin and Wu, Yufeng},
  date         = {2019-01-02},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbf6},
  url          = {http://biorxiv.org/lookup/doi/10.1101/509422},
  urldate      = {2019-03-22},
  abstract     = {Circular RNA is a type of non-coding RNA, which has a circular structure. Many circular RNAs are stable and contain exons, but are not translated into proteins. Circular RNA has important functions in gene regulation and plays an important role in some human diseases. Several biological methods, such as RNase R treatment, have been developed to identify circular RNA. Multiple bioinformatics tools have also been developed for circular RNA detection with high-throughput sequence data. In this paper, we present circDBG, a new method for circular RNA detection with de Bruijn graph. We conduct various experiments to evaluate the performance of CircDBG based on both simulated and real data. Our results show that CircDBG finds more reliable circRNAs with low bias, has more efficiency in running time, and performs better in balancing accuracy and sensitivity than existing methods. As a byproduct, we also introduce a new method to classify circular RNAs based on reads alignment. Finally, we report a potential chimeric circular RNA that is found by CircDBG based on real sequence data. CircDBG can be downloaded from https://github.com/lxwgcool/CircDBG.},
  langid       = {english},
  annotation   = {00000}
}

@unpublished{liLearningGraphLevelRepresentation2017,
  title       = {Learning {{Graph-Level Representation}} for {{Drug Discovery}}},
  author      = {Li, Junying and Cai, Deng and He, Xiaofei},
  date        = {2017-09-12},
  eprint      = {1709.03741},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1709.03741},
  urldate     = {2019-03-22},
  abstract    = {Predicating macroscopic influences of drugs on human body, like efficacy and toxicity, is a central problem of smallmolecule based drug discovery. Molecules can be represented as an undirected graph, and we can utilize graph convolution networks to predication molecular properties. However, graph convolutional networks and other graph neural networks all focus on learning node-level representation rather than graph-level representation. Previous works simply sum all feature vectors for all nodes in the graph to obtain the graph feature vector for drug predication. In this paper, we introduce a dummy super node that is connected with all nodes in the graph by a directed edge as the representation of the graph and modify the graph operation to help the dummy super node learn graph-level feature. Thus, we can handle graph-level classification and regression in the same way as node-level classification and regression. In addition, we apply focal loss to address class imbalance in drug datasets. The experiments on MoleculeNet show that our method can effectively improve the performance of molecular properties predication.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/YLACQBRU/Li et al. - 2017 - Learning Graph-Level Representation for Drug Disco.pdf}
}

@article{lillicrapRandomSynapticFeedback2016,
  title        = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author       = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  date         = {2016-11-08},
  journaltitle = {Nature Communications},
  volume       = {7},
  pages        = {13276},
  issn         = {2041-1723},
  doi          = {10/f9rgsv},
  url          = {http://www.nature.com/doifinder/10.1038/ncomms13276},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf}
}

@article{liMAGeCKEnablesRobust2014,
  title      = {{{MAGeCK}} Enables Robust Identification of Essential Genes from Genome-Scale {{CRISPR}}/{{Cas9}} Knockout Screens},
  author     = {Li, Wei and Xu, Han and Xiao, Tengfei and Cong, Le and Love, Michael I and Zhang, Feng and Irizarry, Rafael A and Liu, Jun S and Brown, Myles and Liu, X Shirley},
  date       = {2014},
  pages      = {12},
  doi        = {10/gfgxjs},
  abstract   = {We propose the Model-based Analysis of Genome-wide CRISPR/Cas9 Knockout (MAGeCK) method for prioritizing single-guide RNAs, genes and pathways in genome-scale CRISPR/Cas9 knockout screens. MAGeCK demonstrates better performance compared with existing methods, identifies both positively and negatively selected genes simultaneously, and reports robust results across different experimental conditions. Using public datasets, MAGeCK identified novel essential genes and pathways, including EGFR in vemurafenib-treated A375 cells harboring a BRAF mutation. MAGeCK also detected cell type-specific essential genes, including BCR and ABL1, in KBM7 cells bearing a BCR-ABL fusion, and IGF1R in HL-60 cells, which depends on the insulin signaling pathway for proliferation.},
  langid     = {english},
  annotation = {00000}
}

@misc{linLanguageModelsProtein2022,
  title  = {Language models of protein sequences at the scale of evolution enable accurate structure prediction},
  author = {Lin, Zeming and others},
  year   = {2022},
  eprint = {2022.07.20.500902},
  doi    = {10.1101/2022.07.20.500902}
}

@unpublished{linMultiHopKnowledgeGraph2018,
  title       = {Multi-{{Hop Knowledge Graph Reasoning}} with {{Reward Shaping}}},
  author      = {Lin, Xi Victoria and Socher, Richard and Xiong, Caiming},
  date        = {2018-08-30},
  eprint      = {1808.10568},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1808.10568},
  urldate     = {2019-03-22},
  abstract    = {Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained onehop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation  = {00000}
}

@unpublished{linNetworkNetwork2013,
  title       = {Network {{In Network}}},
  author      = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  date        = {2013-12-16},
  eprint      = {1312.4400},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1312.4400},
  urldate     = {2019-03-22},
  abstract    = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation  = {00000}
}

@online{linNetworkNetwork2014,
  title       = {Network {{In Network}}},
  author      = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  date        = {2014-03-04},
  eprint      = {1312.4400},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1312.4400},
  urldate     = {2023-01-19},
  abstract    = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{linskerSelforganizationPerceptualNetwork1988,
  title        = {Self-Organization in a Perceptual Network},
  author       = {Linsker, R.},
  date         = {1988-03},
  journaltitle = {Computer},
  volume       = {21},
  number       = {3},
  pages        = {105--117},
  issn         = {0018-9162},
  doi          = {10/cs2w4n},
  url          = {http://ieeexplore.ieee.org/document/36/},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Dropbox/Journal Club/Linsker - 1988 - Self-organization in a perceptual network.pdf}
}

@article{liPathwayPerturbationsSignaling2018,
  title        = {Pathway Perturbations in Signaling Networks: {{Linking}} Genotype to Phenotype},
  shorttitle   = {Pathway Perturbations in Signaling Networks},
  author       = {Li, Yongsheng and McGrail, Daniel J. and Latysheva, Natasha and Yi, Song and Babu, M. Madan and Sahni, Nidhi},
  date         = {2018-05},
  journaltitle = {Seminars in Cell \& Developmental Biology},
  issn         = {10849521},
  doi          = {10.1016/j.semcdb.2018.05.001},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1084952117305281},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{liProstateAssociatedGene42022,
  title        = {The {{Prostate-Associated Gene}} 4 ({{PAGE4}}) {{Could Play}} a {{Role}} in the {{Development}} of {{Benign Prostatic Hyperplasia}} under {{Oxidative Stress}}},
  author       = {Li, Yan and Liu, Jianmin and Liu, Daoquan and Wang, Zhen and Zhou, Yongying and Yang, Shu and Guo, Feng and Yang, Liang and Zhang, Xinhua},
  date         = {2022},
  journaltitle = {Oxidative Medicine and Cellular Longevity},
  shortjournal = {Oxid Med Cell Longev},
  volume       = {2022},
  eprint       = {35633887},
  eprinttype   = {pmid},
  pages        = {7041739},
  issn         = {1942-0994},
  doi          = {10.1155/2022/7041739},
  abstract     = {Benign prostatic hyperplasia (BPH) is a common disease in elderly men with uncertain molecular mechanism, and oxidative stress (OS) has also been found associated with BPH development. Recently, we found that prostate-associated gene 4 (PAGE4) was one of the most significantly changed differentially expressed genes (DEGs) in BPH, which can protect cells against stress stimulation. However, the exact role of PAGE4 in BPH remains unclear. This study is aimed at exploring the effect of PAGE4 in BPH under OS. Human prostate tissues and cultured WPMY-1 and PrPF cells were utilized. The expression and localization of PAGE4 were determined with qRT-PCR, Western blotting, and immunofluorescence staining. OS cell models induced with H2O2 were treated with PAGE4 silencing or PAGE4 overexpression or inhibitor (N-acetyl-L-cysteine (NAC)) of OS. The proliferation activity, apoptosis, OS markers, and MAPK signaling pathways were detected by CCK-8 assay, flow cytometry analysis, and Western blotting. PAGE4 was shown to be upregulated in human hyperplastic prostate and mainly located in the stroma. Acute OS induced with H2O2 increased PAGE4 expression (which was prevented by OS inhibitor), apoptosis, cell cycle arrest, and reactive oxygen species (ROS) accumulation in WPMY-1 and PrPF cells. siPAGE4 plus H2O2 potentiated H2O2 effect via reducing the p-ERK1/2 level and increasing p-JNK1/2 level. Consistently, overexpression of PAGE4 offset the effect of H2O2 and partially reversed the PAGE4 silencing effect. However, knocking down and overexpression of PAGE4 alone determined no significant effects. Our novel data demonstrated that augmented PAGE4 promotes cell survival by activating p-ERK1/2 and decreases cell apoptosis by inhibiting p-JNK1/2 under the OS, which could contribute to the development of BPH.},
  langid       = {english},
  pmcid        = {PMC9135540},
  keywords     = {Aged,Antigens Neoplasm,Humans,Hydrogen Peroxide,Male,Oxidative Stress,Prostate,Prostatic Hyperplasia,Prostatic Neoplasms},
  file         = {/Users/jkobject/Zotero/storage/MB32FZ32/Li et al. - 2022 - The Prostate-Associated Gene 4 (PAGE4) Could Play .pdf}
}

@article{liRNA3DCNNLocalGlobal2018,
  title        = {{{RNA3DCNN}}: {{Local}} and Global Quality Assessments of {{RNA 3D}} Structures Using {{3D}} Deep Convolutional Neural Networks},
  shorttitle   = {{{RNA3DCNN}}},
  author       = {Li, Jun and Zhu, Wei and Wang, Jun and Li, Wenfei and Gong, Sheng and Zhang, Jian and Wang, Wei},
  editor       = {Chen, Shi-Jie},
  date         = {2018-11-27},
  journaltitle = {PLOS Computational Biology},
  volume       = {14},
  number       = {11},
  pages        = {e1006514},
  issn         = {1553-7358},
  doi          = {10/gfxbg2},
  url          = {http://dx.plos.org/10.1371/journal.pcbi.1006514},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{liSyntheticdiploidBenchmarkAccurate2018,
  title        = {A Synthetic-Diploid Benchmark for Accurate Variant-Calling Evaluation},
  author       = {Li, Heng and Bloom, Jonathan M. and Farjoun, Yossi and Fleharty, Mark and Gauthier, Laura and Neale, Benjamin and MacArthur, Daniel},
  date         = {2018-08},
  journaltitle = {Nature Methods},
  volume       = {15},
  number       = {8},
  pages        = {595--597},
  issn         = {1548-7091, 1548-7105},
  doi          = {10/gfxbfd},
  url          = {http://www.nature.com/articles/s41592-018-0054-7},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{littmanSCINGInferenceRobust2023,
  title      = {{{SCING}}: {{Inference}} of Robust, Interpretable Gene Regulatory Networks from Single Cell and Spatial Transcriptomics},
  shorttitle = {{{SCING}}},
  author     = {Littman, Russell and Cheng, Michael and Wang, Ning and Peng, Chao and Yang, Xia},
  year       = {2023},
  month      = jul,
  journal    = {iScience},
  volume     = {26},
  number     = {7},
  pages      = {107124},
  issn       = {2589-0042},
  doi        = {10.1016/j.isci.2023.107124},
  urldate    = {2024-04-18},
  abstract   = {Gene regulatory network (GRN) inference is an integral part of understanding physiology and disease. Single cell/nuclei RNA-seq (scRNA-seq/snRNA-seq) data has been used to elucidate cell-type GRNs; however, the accuracy and speed of current scRNAseq-based GRN approaches are suboptimal. Here, we present Single Cell INtegrative Gene regulatory network inference (SCING), a gradient boosting and mutual information-based approach for identifying robust GRNs from scRNA-seq, snRNA-seq, and spatial transcriptomics data. Performance evaluation using Perturb-seq datasets, held-out data, and the mouse cell atlas combined with the DisGeNET database demonstrates the improved accuracy and biological interpretability of SCING compared to existing methods. We applied SCING to the entire mouse single cell atlas, human Alzheimer's disease (AD), and mouse AD spatial transcriptomics. SCING GRNs reveal unique disease subnetwork modeling capabilities, have intrinsic capacity to correct for batch effects, retrieve disease relevant genes and pathways, and are informative on spatial specificity of disease pathogenesis.},
  keywords   = {Biocomputational method,Machine learning,Transcriptomics},
  file       = {/Users/jkobject/Zotero/storage/3BGGBVZW/Littman et al. - 2023 - SCING Inference of robust, interpretable gene reg.pdf;/Users/jkobject/Zotero/storage/2SWUTHTN/S2589004223012014.html}
}

@misc{liuEvaluatingUtilitiesFoundation2024,
  title  = {Evaluating the Utilities of Foundation Models in Single-cell Data Analysis},
  author = {Liu, Tianyu and Li, Kexing and Wang, Yizhou and Li, Hongyu and Zhao, Hongyu},
  year   = {2024},
  eprint = {2023.09.08.555192},
  doi    = {10.1101/2023.09.08.555192}
}

@article{liuHIERARCHICALREPRESENTATIONSEFFICIENT,
  title      = {{{HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITECTURE SEARCH}}},
  author     = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  pages      = {13},
  abstract   = {We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6\% on CIFAR-10 and 20.3\% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3\% less top-1 accuracy on CIFAR-10 and 0.1\% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {00688},
  file       = {/Users/jeremie/Documents/science/ML/NN/NE/Liu et al. - HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITE.pdf}
}

@article{liuJointlyDefiningCell2020,
  title     = {Jointly Defining Cell Types from Multiple Single-Cell Datasets Using {{LIGER}}},
  author    = {Liu, Jialin and Gao, Chao and Sodicoff, Joshua and Kozareva, Velina and Macosko, Evan Z. and Welch, Joshua D.},
  year      = {2020},
  month     = nov,
  journal   = {Nature Protocols},
  volume    = {15},
  number    = {11},
  pages     = {3632--3662},
  publisher = {Nature Publishing Group},
  issn      = {1750-2799},
  doi       = {10.1038/s41596-020-0391-8},
  urldate   = {2025-02-25},
  abstract  = {High-throughput single-cell sequencing technologies hold tremendous potential for defining cell types in an unbiased fashion using gene expression and epigenomic state. A key challenge in realizing this potential is integrating single-cell datasets from multiple protocols, biological contexts, and data modalities into a joint definition of cellular identity. We previously developed an approach, called linked inference of genomic experimental relationships (LIGER), that uses integrative nonnegative matrix factorization to address this challenge. Here, we provide a step-by-step protocol for using LIGER to jointly define cell types from multiple single-cell datasets. The main stages of the protocol are data preprocessing and normalization, joint factorization, quantile normalization and joint clustering, and visualization. We describe how to jointly define cell types from single-cell RNA-seq (scRNA-seq) and single-nucleus ATAC-seq (snATAC-seq) data, but similar steps apply across a wide range of other settings and data types, including cross-species analysis, single-nucleus DNA methylation, and spatial transcriptomics. Our protocol contains examples of expected results, describes common pitfalls, and relies only on our freely available, open-source R implementation of LIGER. We also provide R Markdown tutorials showing the outputs from each individual code segment. The analysis process can be performed in 1--4 h, depending on dataset size, and assumes no specialized bioinformatics training.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid    = {english},
  keywords  = {Bioinformatics,Genome informatics,Machine learning,Software},
  file      = {/Users/jkobject/Zotero/storage/JJI89CCN/Liu et al. - 2020 - Jointly defining cell types from multiple single-c.pdf}
}

@article{liuRegNetworkIntegratedDatabase2015,
  title      = {{{RegNetwork}}: An Integrated Database of Transcriptional and Post-Transcriptional Regulatory Networks in Human and Mouse},
  shorttitle = {{{RegNetwork}}},
  author     = {Liu, Zhi-Ping and Wu, Canglin and Miao, Hongyu and Wu, Hulin},
  year       = {2015},
  month      = sep,
  journal    = {Database: The Journal of Biological Databases and Curation},
  volume     = {2015},
  pages      = {bav095},
  doi        = {10.1093/database/bav095},
  urldate    = {2024-10-28},
  abstract   = {Transcriptional and post-transcriptional regulation of gene expression is of fundamental importance to numerous biological processes. Nowadays, an increasing amount of gene regulatory relationships have been documented in various databases and ...},
  langid     = {english},
  pmid       = {26424082},
  file       = {/Users/jkobject/Zotero/storage/67N95TCT/Liu et al. - 2015 - RegNetwork an integrated database of transcriptio.pdf}
}

@article{liuUnsupervisedEmbeddingSinglecell2018,
  title      = {Unsupervised Embedding of Single-Cell {{Hi-C}} Data},
  author     = {Liu, Jie and Lin, Dejun and Yardimci, Gurkan and Noble, William},
  date       = {2018-01-30},
  doi        = {10/gfxbdm},
  url        = {http://biorxiv.org/lookup/doi/10.1101/257048},
  urldate    = {2018-04-11},
  abstract   = {Single-cell Hi-C (scHi-C) data promises to enable scientists to interrogate the 3D architecture of DNA in the nucleus of the cell, studying how this structure varies stochastically or along developmental or cell cycle axes. However, Hi-C data analysis requires methods that take into account the unique characteristics of this type of data. In this work, we explore whether methods that have been developed previously for the analysis of bulk Hi-C data can be applied to scHi-C data. In this work, we apply methods designed for analysis of bulk Hi-C data to scHi-C data in conjunction with unsupervised embedding. We find that one of these methods, HiCRep, when used in conjunction with multidimensional scaling (MDS), strongly outperforms three other methods, including a technique that has been used previously for scHi-C analysis. We also provide evidence that the HiCRep/MDS method is robust to extremely low per-cell sequencing depth, that this robustness is improved even further when high-coverage and low-coverage cells are projected together, and that the method can be used to jointly embed cells from multiple published datasets.},
  langid     = {english},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/bio info/genomics/Liu et al. - 2018 - Unsupervised embedding of single-cell Hi-C data.pdf}
}

@misc{liVisualizingLossLandscape2018,
  title         = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author        = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year          = 2018,
  month         = nov,
  number        = {arXiv:1712.09913},
  eprint        = {1712.09913},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1712.09913},
  urldate       = {2025-12-02},
  abstract      = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/3ZCYINLY/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf;/Users/jkobject/Zotero/storage/FI6SARD7/1712.html}
}

@misc{llora-batlle10xGenomicsGene2024,
  title         = {10x {{Genomics Gene Expression Flex}} Is a Powerful Tool for Single-Cell Transcriptomics of Xenograft Models},
  author        = {{Llora-Batlle}, Oriol and Farcas, Anca and Fransen, Doreth and Floc'h, Nicolas and Talbot, Sara and Schwiening, Alix and Bojko, Laura and Calver, John and Josipovic, Natasa and Lashuk, Kanstantsin and Schueler, Julia and Prodan, Andrei and Mooijman, Dylan and McDermott, Ultan and Consortium, Persist-Seq},
  year          = 2024,
  month         = jan,
  primaryclass  = {New Results},
  pages         = {2024.01.25.577066},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.01.25.577066},
  urldate       = {2025-12-13},
  abstract      = {The 10x Genomics Gene Expression Flex protocol allows profiling of fixed or frozen material, greatly simplifying the logistics of sample collection, storage and transfer prior to single -cell sequencing. The method makes single-cell transcriptomics possible for existing fresh-frozen or FFPE tissue samples, but also facilitates the logistics of the sampling process, allowing instant preservation of samples. The technology relies on species-specific probes available for human and mouse. Nevertheless, processing of patient-derived (PDX) or cell line (CDX) xenografts, which contain mixed human and mouse cells, is currently not supported by this protocol due to the high degree of homology between the probe sets. Here we show that it is feasible to simultaneously profile populations containing both human and mouse cells by mixing the transcriptome probe sets of both species. Cellranger outputs a count table for each of the species allowing evaluation of the performance of the different probe sets. Cross-reactive probes are greatly outperformed by the specific probe hybridizations leading to a clear difference in the recovery of UMIs and unique genes per cell. Furthermore, we developed a pipeline that removes cross-reactive signal from the data and provides species-specific count tables for further downstream analysis. Hence, the 10x Genomics Gene Expression Flex protocol can be used to process xenograft samples without the need for separation of human and mouse cells by flow sorting and allows analysis of the human and mouse single-cell transcriptome from each sample. We anticipate it will be increasingly used for single-cell sequencing of cancer cell line and patient-derived xenografts, facilitating the preservation of the samples and allowing the interrogation of both the (human) xenograft and the (mouse) tumor microenvironment at single-cell resolution.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {\copyright{} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/6IREKT9T/Llora-Batlle et al. - 2024 - 10x Genomics Gene Expression Flex is a powerful tool for single-cell transcriptomics of xenograft mo.pdf}
}

@article{lopezDeepGenerativeModeling2018,
  title   = {Deep generative modeling for single-cell transcriptomics},
  author  = {Lopez, Romain and Regier, Jeffrey and Cole, Michael B. and Jordan, Michael I. and Yosef, Nir},
  journal = {Nature Methods},
  volume  = {15},
  pages   = {1053--1058},
  year    = {2018},
  doi     = {10.1038/s41592-018-0229-2}
}

@misc{lora,
  title         = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle    = {{{LoRA}}},
  author        = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year          = {2021},
  month         = oct,
  number        = {arXiv:2106.09685},
  eprint        = {2106.09685},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2106.09685},
  urldate       = {2025-02-06},
  abstract      = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/EKKXKW86/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/jkobject/Zotero/storage/4HLUTQEK/2106.html}
}

@misc{loshchilovDecoupledWeightDecay2019,
  title         = {Decoupled {{Weight Decay Regularization}}},
  author        = {Loshchilov, Ilya and Hutter, Frank},
  year          = {2019},
  month         = jan,
  number        = {arXiv:1711.05101},
  eprint        = {1711.05101},
  primaryclass  = {cs, math},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1711.05101},
  urldate       = {2024-07-21},
  abstract      = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file          = {/Users/jkobject/Zotero/storage/8CN6T8YB/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/jkobject/Zotero/storage/2I7A2NFR/1711.html}
}

@article{lotfollahiScGenPredictsSinglecell2019,
  title    = {{{scGen}} Predicts Single-Cell Perturbation Responses},
  author   = {Lotfollahi, Mohammad and Wolf, F. Alexander and Theis, Fabian J.},
  year     = 2019,
  month    = aug,
  journal  = {Nature Methods},
  volume   = {16},
  number   = {8},
  pages    = {715--721},
  issn     = {1548-7105},
  doi      = {10.1038/s41592-019-0494-8},
  abstract = {Accurately modeling cellular response to perturbations is a central goal of computational biology. While such modeling has been based on statistical, mechanistic and machine learning models in specific settings, no generalization of predictions to phenomena absent from training data (out-of-sample) has yet been demonstrated. Here, we present scGen (https://github.com/theislab/scgen), a model combining variational autoencoders and latent space vector arithmetics for high-dimensional single-cell gene expression data. We show that scGen accurately models perturbation and infection response of cells across cell types, studies and species. In particular, we demonstrate that scGen learns cell-type and species-specific responses implying that it captures features that distinguish responding from non-responding genes and cells. With the upcoming availability of large-scale atlases of organs in a healthy state, we envision scGen to become a tool for experimental design through in silico screening of perturbation response in the context of disease and drug treatment.},
  langid   = {english},
  pmid     = {31363220},
  keywords = {Algorithms,Animals,Computational Biology,Computer Simulation,Gene Expression Profiling,Humans,Leukocytes Mononuclear,Machine Learning,Mice,Phagocytes,Single-Cell Analysis,Species Specificity,Transcriptome}
}

@article{louizosLEARNINGSPARSENEURAL,
  title      = {{{LEARNING SPARSE NEURAL NETWORKS THROUGH L0 REGULARIZATION}}},
  author     = {Louizos, Christos and Welling, Max and Kingma, Diederik P},
  pages      = {13},
  abstract   = {We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  langid     = {english},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/Louizos et al. - LEARNING SPARSE NEURAL NETWORKS THROUGH L0 REGULAR.pdf}
}

@unpublished{louizosLearningSparseNeural2017,
  title       = {Learning {{Sparse Neural Networks}} through \${{L}}\_0\$ {{Regularization}}},
  author      = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  date        = {2017-12-04},
  eprint      = {1712.01312},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1712.01312},
  urldate     = {2019-03-22},
  abstract    = {We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{lovenSelectiveInhibitionTumor2013,
  title        = {Selective {{Inhibition}} of {{Tumor Oncogenes}} by {{Disruption}} of {{Super-Enhancers}}},
  author       = {Lovén, Jakob and Hoke, Heather~A. and Lin, Charles~Y. and Lau, Ashley and Orlando, David~A. and Vakoc, Christopher~R. and Bradner, James~E. and Lee, Tong~Ihn and Young, Richard~A.},
  date         = {2013-04},
  journaltitle = {Cell},
  volume       = {153},
  number       = {2},
  pages        = {320--334},
  issn         = {00928674},
  doi          = {10/f4r7xz},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867413003930},
  urldate      = {2019-03-22},
  abstract     = {Chromatin regulators have become attractive targets for cancer therapy, but it is unclear why inhibition of these ubiquitous regulators should have gene-specific effects in tumor cells. Here, we investigate how inhibition of the widely expressed transcriptional coactivator BRD4 leads to selective inhibition of the MYC oncogene in multiple myeloma (MM). BRD4 and Mediator were found to co-occupy thousands of enhancers associated with active genes. They also co-occupied a small set of exceptionally large super-enhancers associated with genes that feature prominently in MM biology, including the MYC oncogene. Treatment of MM tumor cells with the BET-bromodomain inhibitor JQ1 led to preferential loss of BRD4 at super-enhancers and consequent transcription elongation defects that preferentially impacted genes with super-enhancers, including MYC. Super-enhancers were found at key oncogenic drivers in many other tumor cells. These observations have implications for the discovery of cancer therapeutics directed at components of super-enhancers in diverse tumor types.},
  langid       = {english},
  annotation   = {00000}
}

@inproceedings{lucPredictingDeeperFuture2017,
  title      = {Predicting {{Deeper}} into the {{Future}} of {{Semantic Segmentation}}},
  booktitle  = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author     = {Luc, Pauline and Neverova, Natalia and Couprie, Camille and Verbeek, Jakob and LeCun, Yann},
  date       = {2017-10},
  pages      = {648--657},
  publisher  = {IEEE},
  location   = {Venice},
  doi        = {10/gfxbff},
  url        = {http://ieeexplore.ieee.org/document/8237339/},
  urldate    = {2019-03-22},
  abstract   = {The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn       = {978-1-5386-1032-9},
  langid     = {english},
  annotation = {00000}
}

@article{lueckenBenchmarkingAtlaslevelData2022,
  title     = {Benchmarking Atlas-Level Data Integration in Single-Cell Genomics},
  author    = {Luecken, Malte D. and B{\"u}ttner, M. and Chaichoompu, K. and Danese, A. and Interlandi, M. and Mueller, M. F. and Strobl, D. C. and Zappia, L. and Dugas, M. and {Colom{\'e}-Tatch{\'e}}, M. and Theis, Fabian J.},
  year      = {2022},
  month     = jan,
  journal   = {Nature Methods},
  volume    = {19},
  number    = {1},
  pages     = {41--50},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-021-01336-8},
  urldate   = {2024-04-19},
  abstract  = {Single-cell atlases often include samples that span locations, laboratories and conditions, leading to complex, nested batch effects in data. Thus, joint analysis of atlas datasets requires reliable data integration. To guide integration method choice, we benchmarked 68 method and preprocessing combinations on 85 batches of gene expression, chromatin accessibility and simulation data from 23 publications, altogether representing {$>$}1.2 million cells distributed in 13 atlas-level integration tasks. We evaluated methods according to scalability, usability and their ability to remove batch effects while retaining biological variation using 14 evaluation metrics. We show that highly variable gene selection improves the performance of data integration methods, whereas scaling pushes methods to prioritize batch removal over conservation of biological variation. Overall, scANVI, Scanorama, scVI and scGen perform well, particularly on complex integration tasks, while single-cell ATAC-sequencing integration performance is strongly affected by choice of feature space. Our freely available Python module and benchmarking pipeline can identify optimal data integration methods for new data, benchmark new methods and improve method development.},
  copyright = {2021 The Author(s)},
  langid    = {english},
  keywords  = {Data integration,Machine learning,Software,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/W8II8P4R/Luecken et al. - 2022 - Benchmarking atlas-level data integration in singl.pdf}
}

@article{lueckenDefiningBenchmarkingOpen2025,
  title   = {Defining and benchmarking open problems in single-cell analysis},
  author  = {Luecken, Malte D. and others},
  journal = {Nature Biotechnology},
  volume  = {43},
  pages   = {1035--1040},
  year    = {2025},
  doi     = {10.1038/s41587-025-02582-3}
}

@article{luoAdvancesProteinSequencing2025,
  title      = {Advances in Protein Sequencing: {{Techniques}}, Challenges and Prospects},
  shorttitle = {Advances in Protein Sequencing},
  author     = {Luo, Xiuer and Ng, Cheuk Chi A. and Lam, Henry Hei Ning and Yao, Zhong-Ping},
  year       = 2025,
  month      = oct,
  journal    = {TrAC Trends in Analytical Chemistry},
  volume     = {191},
  pages      = {118341},
  issn       = {0165-9936},
  doi        = {10.1016/j.trac.2025.118341},
  urldate    = {2025-12-02},
  abstract   = {Proteins are major building blocks of life, which are the principal undertakers of life activities. The protein components and distribution of a cell or an organ can give us lots of key information to understand the biological process in human life as well as diagnose diseases. Protein sequencing can determine individual proteins' sequences, structures, and post-translational modifications, deliver detailed insights into protein-protein interactions and functions, which lead to advances such as characterization of disease biomarkers and personalized medicine, transforming our comprehension of complex biological systems. While conventional methods such as Edman degradation and mass spectrometry are being enhanced by integrating with other techniques, emerging sequencing approaches such as DNA point accumulation in nanoscale topology (DNA-PAINT), recognition tunneling and nanopore sequencing have advanced significantly, moving beyond the proof-of-concept stage and achieving notable progress recently. This review will examine these techniques, discuss the advancements and inherent challenges, such as sensitivity, sample complexity, and low abundance of protein sample, and propose some corresponding solutions. Finally, a forward-looking perspective on the future of protein sequencing, emphasizing the need for more sensitive and high-throughput methods, the integration of multi-omics data, the role of computational approaches, and the expansion of protein sequencing into new application domains will also be presented. This review will serve as a valuable resource for researchers and practitioners, guiding future developments and applications in protein sequencing.},
  keywords   = {DNA-PAINT,Mass spectrometry,Nanopore technology,Recognition tunneling,Single molecule protein sequencing},
  file       = {/Users/jkobject/Zotero/storage/SEUPZ49Q/S0165993625002092.html}
}

@article{luoClairvoyanteMultitaskConvolutional2018,
  title        = {Clairvoyante: A Multi-Task Convolutional Deep Neural Network for Variant Calling in {{Single Molecule Sequencing}}},
  shorttitle   = {Clairvoyante},
  author       = {Luo, Ruibang and Sedlazeck, Fritz J and Lam, Tak-Wah and Schatz, Michael},
  date         = {2018-09-26},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbfk},
  url          = {http://biorxiv.org/lookup/doi/10.1101/310458},
  urldate      = {2019-03-22},
  abstract     = {The accurate identification of DNA sequence variants is an important but challenging task in genomics. It is particularly difficult for single molecule sequencing, which has a pernucleotide error rate of \textasciitilde 5\%-15\%. Meeting this demand, we developed Clairvoyante, a multitask five-layer convolutional neural network model for predicting variant type (SNP or indel), zygosity, alternative allele and indel length from aligned reads. For the well-characterized NA12878 human sample, Clairvoyante achieved 99.73\%, 97.68\% and 95.36\% precision on known variants, and 98.65\%, 92.57\%, 77.89\% F1-score for whole-genome analysis, using Illumina, PacBio, and Oxford Nanopore data, respectively. Training on a second human sample shows Clairvoyante is sample agnostic and finds variants in less than two hours on a standard server. Furthermore, we identified 3,135 variants that are not yet indexed but are strongly supported by both PacBio and Oxford Nanopore data. Clairvoyante is available open-source (https://github.com/aquaskyline/Clairvoyante), with modules to train, utilize and visualize the model.},
  langid       = {english},
  annotation   = {00000}
}

@article{luoMechanismPrognosticMarker2023,
  title        = {Mechanism of Prognostic Marker {{SPOCK3}} Affecting Malignant Progression of Prostate Cancer and Construction of Prognostic Model},
  author       = {Luo, Jiawen and Lai, Cong and Xu, Xiaoting and Shi, Juanyi and Hu, Jintao and Guo, Kaixuan and Mulati, Yelisudan and Xiao, Yunfei and Kong, Degeng and Liu, Cheng and Xu, Kewei},
  date         = {2023-08-11},
  journaltitle = {BMC Cancer},
  shortjournal = {BMC Cancer},
  volume       = {23},
  eprint       = {37563543},
  eprinttype   = {pmid},
  pages        = {741},
  issn         = {1471-2407},
  doi          = {10.1186/s12885-023-11151-3},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10416445/},
  urldate      = {2024-07-26},
  abstract     = {Background SPOCK3 is a secreted extracellular matrix proteoglycan. This study aimed to investigate the effect of SPOCK3 on the malignant progression of prostate cancer and to construct a prognostic model to predict DFS of patients with prostate cancer. Methods Clinical and transcriptome sequencing data for prostate cancer were download from the TCGA and GEO databases. The survival curve showed that SPOCK3 has prognostic significance. GO, KEGG, and GSEA enrichment analysis were used to investigate how SPOCK3 affects the malignant progression of prostate cancer. Based on ESTIMATE and ssGSEA, the relationship between SPOCK3 and immune cell infiltration in prostate cancer tissue was clarified. Univariate and multivariate COX regression analysis was used to identify the independent prognostic factors of prostate cancer OS and to construct a nomogram. The calibration curve and ROC curves were drawn to assess the nomogram's predictive power. Results The survival curve revealed that patients in the low-expression group of SPOCK3 had a poor prognosis. According to enrichment analysis, SOPCK3-related genes were enriched in collagen-containing extracellular matrix, PI3K-Akt, and MAPK signaling pathway. ESTIMATE analysis revealed that SPOCK3 expression was positively correlated with the interstitial score, immune score, and ESTIMATE score. The results of ssGSEA analysis revealed that the infiltration levels of Mast cells, NK cells, and B cells were higher in the SPOCK3 high expression group. Cox regression analysis showed that SPOCK3 expression level, T and Gleason score were independent risk factors of patient prognosis, and a nomogram was constructed. The ROC curve showed the AUCs of DFS at 2, 3, and 5~years. Conclusion SPOCK3 is a protective factor for DFS in prostate cancer patients. SPOCK3 is significantly associated with immune cell infiltration. The prognostic model constructed based on SPOCK3 has excellent predictive performance. Supplementary Information The online version contains supplementary material available at 10.1186/s12885-023-11151-3.},
  pmcid        = {PMC10416445},
  file         = {/Users/jkobject/Zotero/storage/7EHFRGUM/Luo et al. - 2023 - Mechanism of prognostic marker SPOCK3 affecting ma.pdf}
}

@article{lvPAGE4PromotesProstate2019,
  title        = {{{PAGE4}} Promotes Prostate Cancer Cells Survive under Oxidative Stress through Modulating {{MAPK}}/{{JNK}}/{{ERK}} Pathway},
  author       = {Lv, Chengcheng and Fu, Shui and Dong, Qingzhuo and Yu, Zi and Zhang, Gejun and Kong, Chuize and Fu, Cheng and Zeng, Yu},
  date         = {2019-01-18},
  journaltitle = {Journal of experimental \& clinical cancer research: CR},
  shortjournal = {J Exp Clin Cancer Res},
  volume       = {38},
  number       = {1},
  eprint       = {30658679},
  eprinttype   = {pmid},
  pages        = {24},
  issn         = {1756-9966},
  doi          = {10.1186/s13046-019-1032-3},
  abstract     = {BACKGROUND: Prostate cancer (PCa) is one of the most common cancers in male worldwide. Oxidative stress has been recognized as one of the driving signals pathologically linked to PCa progression. Nevertheless, the association of oxidative stress with PCa progression remains unclear. METHODS: Western blot, q-RT-PCR and bioinformatics analyses were used to examine PAGE4 expression. Comet assay and Annexin V/ PI dual staining assay were performed to investigate DNA damage and cell death under oxidative stress. Mouse xenograft model of PCa cells was established to verify the role of PAGE4 in vivo. Transcriptomic analysis was performed to investigate the underlying mechanism for the function of PAGE4 under oxidative stress. Western blot assay was conducted to determine the status of MAPK pathway. Immunohistochemistry was used to identify protein expression of PAGE4 in tumor tissues. RESULTS: In this study, we found that PAGE4 expression was increased in PCa cells under oxidative stress condition. PAGE4 overexpression protected PCa cells from oxidative stress-inducing cell death by reducing DNA damage. PAGE4 overexpression promoted PCa cells growth in vivo. Mechanistically, PAGE4 promoted the survival of prostate cancer cells through regulating MAPK pathway which reflected in decreasing the phosphorylation of MAP2K4, JNK and c-JUN but increasing phosphorylation of ERK1/2. CONCLUSION: Our findings indicate that PAGE4 protects PCa cells from DNA damage and apoptosis under oxidative stress by modulating MAPK signalling pathway. PAGE4 expression may serve as a prognostic biomarker for clinical applications.},
  langid       = {english},
  pmcid        = {PMC6339303},
  keywords     = {Animals,Antigens Neoplasm,Cell Line Tumor,Disease Models Animal,ERK,Gene Expression,Heterografts,Humans,Immunohistochemistry,Male,MAP Kinase Signaling System,MAPK,Mice,Models Biological,Oxidative stress,Oxidative Stress,PAGE4,Prognosis,Prostate Cancer,Prostatic Neoplasms,Reactive Oxygen Species},
  file         = {/Users/jkobject/Zotero/storage/GQL3MJIM/Lv et al. - 2019 - PAGE4 promotes prostate cancer cells survive under.pdf}
}

@article{mackayPracticalBayesianFramework1992,
  title        = {A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}},
  author       = {MacKay, David J. C.},
  date         = {1992-05},
  journaltitle = {Neural Computation},
  volume       = {4},
  number       = {3},
  pages        = {448--472},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/bnn9r9},
  url          = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.3.448},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/MacKay - 1992 - A Practical Bayesian Framework for Backpropagation.pdf}
}

@article{macoskoHighlyParallelGenomewide2015,
  title    = {Highly {{Parallel Genome-wide Expression Profiling}} of {{Individual Cells Using Nanoliter Droplets}}},
  author   = {Macosko, Evan Z. and Basu, Anindita and Satija, Rahul and Nemesh, James and Shekhar, Karthik and Goldman, Melissa and Tirosh, Itay and Bialas, Allison R. and Kamitaki, Nolan and Martersteck, Emily M. and Trombetta, John J. and Weitz, David A. and Sanes, Joshua R. and Shalek, Alex K. and Regev, Aviv and McCarroll, Steven A.},
  year     = 2015,
  month    = may,
  journal  = {Cell},
  volume   = {161},
  number   = {5},
  pages    = {1202--1214},
  issn     = {1097-4172},
  doi      = {10.1016/j.cell.2015.05.002},
  abstract = {Cells, the basic units of biological structure and function, vary broadly in type and state. Single-cell genomics can characterize cell identity and function, but limitations of ease and scale have prevented its broad application. Here we describe Drop-seq, a strategy for quickly profiling thousands of individual cells by separating them into nanoliter-sized aqueous droplets, associating a different barcode with each cell's RNAs, and sequencing them all together. Drop-seq analyzes mRNA transcripts from thousands of individual cells simultaneously while remembering transcripts' cell of origin. We analyzed transcriptomes from 44,808 mouse retinal cells and identified 39 transcriptionally distinct cell populations, creating a molecular atlas of gene expression for known retinal cell classes and novel candidate cell subtypes. Drop-seq will accelerate biological discovery by enabling routine transcriptional profiling at single-cell resolution. VIDEO ABSTRACT.},
  langid   = {english},
  pmcid    = {PMC4481139},
  pmid     = {26000488},
  keywords = {Animals,Gene Expression Profiling,Genome-Wide Association Study,High-Throughput Nucleotide Sequencing,Mice,Microfluidic Analytical Techniques,Retina,Sequence Analysis RNA,Single-Cell Analysis},
  file     = {/Users/jkobject/Zotero/storage/CR8B4YWB/Macosko et al. - 2015 - Highly Parallel Genome-wide Expression Profiling of Individual Cells Using Nanoliter Droplets.pdf}
}

@unpublished{maheswaranathanGuidedEvolutionaryStrategies2018,
  title       = {Guided Evolutionary Strategies: Escaping the Curse of Dimensionality in Random Search},
  shorttitle  = {Guided Evolutionary Strategies},
  author      = {Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and Sohl-Dickstein, Jascha},
  date        = {2018-06-26},
  eprint      = {1806.10230},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1806.10230},
  urldate     = {2019-03-22},
  abstract    = {Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications, or when using synthetic gradients). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a guiding subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and we use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and a synthetic gradient problem, demonstrating improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient. We provide a demo of Guided ES at: github.com/brain-research/guided-evolutionary-strategies.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation  = {00000}
}

@unpublished{mairalOnlineLearningMatrix2009,
  title       = {Online {{Learning}} for {{Matrix Factorization}} and {{Sparse Coding}}},
  author      = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  date        = {2009-08-01},
  eprint      = {0908.0050},
  eprinttype  = {arXiv},
  eprintclass = {cs, math, stat},
  url         = {http://arxiv.org/abs/0908.0050},
  urldate     = {2019-03-22},
  abstract    = {Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{makhzaniWinnerTakeAllAutoencoders,
  title      = {Winner-{{Take-All Autoencoders}}},
  author     = {Makhzani, Alireza and Frey, Brendan},
  pages      = {11},
  abstract   = {In this paper, we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion. We first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units. We then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations. We describe a way to train convolutional autoencoders layer by layer, where in addition to lifetime sparsity, a spatial sparsity within each feature map is achieved using winner-take-all activation functions. We will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the MNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets, and achieve competitive classification performance.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Makhzani et Frey - Winner-Take-All Autoencoders.pdf}
}

@article{maloneModelingSampleVariables2010,
  title   = {Modeling sample variables with an Experimental Factor Ontology},
  author  = {Malone, James and others},
  journal = {Bioinformatics},
  volume  = {26},
  pages   = {1112--1118},
  year    = {2010},
  doi     = {10.1093/bioinformatics/btq099}
}

@article{manganInferringBiologicalNetworks2016,
  title        = {Inferring {{Biological Networks}} by {{Sparse Identification}} of {{Nonlinear Dynamics}}},
  author       = {Mangan, Niall M. and Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
  date         = {2016-06},
  journaltitle = {IEEE Transactions on Molecular, Biological and Multi-Scale Communications},
  volume       = {2},
  number       = {1},
  pages        = {52--63},
  issn         = {2372-2061, 2332-7804},
  doi          = {10/gfxbdn},
  url          = {http://ieeexplore.ieee.org/document/7809160/},
  urldate      = {2018-04-11},
  abstract     = {Inferring the structure and dynamics of network models is critical to understanding the functionality and control of complex systems, such as metabolic and regulatory biological networks. The increasing quality and quantity of experimental data enable statistical approaches based on information theory for model selection and goodness-of-fit metrics. We propose an alternative data-driven method to infer networked nonlinear dynamical systems by using sparsity-promoting optimization to select a subset of nonlinear interactions representing dynamics on a network. In contrast to standard model selection methodsbased upon information content for a finite number of heuristic models (order 10 or less), our model selection procedure discovers a parsimonious model from a combinatorially large set of models, without an exhaustive search. Our particular innovation is appropriate for many biological networks, where the governing dynamical systems have rational function nonlinearities with cross terms, thus requiring an implicit formulation and the equations to be identified in the null-space of a library of mixed nonlinearities, including the state and derivative terms. This method, implicit-SINDy, succeeds in inferring three canonical biological models: 1) Michaelis–Menten enzyme kinetics; 2) the regulatory network for competence in bacteria; and 3) the metabolic network for yeast glycolysis.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Mangan et al. - 2016 - Inferring Biological Networks by Sparse Identifica.pdf}
}

@article{mansourOncogenicSuperenhancerFormed2014,
  title        = {An Oncogenic Super-Enhancer Formed through Somatic Mutation of a Noncoding Intergenic Element},
  author       = {Mansour, M. R. and Abraham, B. J. and Anders, L. and Berezovskaya, A. and Gutierrez, A. and Durbin, A. D. and Etchin, J. and Lawton, L. and Sallan, S. E. and Silverman, L. B. and Loh, M. L. and Hunger, S. P. and Sanda, T. and Young, R. A. and Look, A. T.},
  date         = {2014-12-12},
  journaltitle = {Science},
  volume       = {346},
  number       = {6215},
  pages        = {1373--1377},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/f726hr},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.1259037},
  urldate      = {2019-03-22},
  langid       = {english}
}

@online{ManuscriptScprint,
  title        = {manuscript scprint},
  url          = {https://docs.google.com/document/d/17KnEwIntVMdnLwvH6ix8Ep59bjQKxoW7kG9_3SgG3o4/edit?usp=embed_facebook},
  urldate      = {2024-07-26},
  abstract     = {First version of manuscript Accepted by Laura Intro Part 1 Part 2 Part 3 Part 4 Part 5 Conclusion Abstract Supp Figures methods Accepted by Gabriel Intro Part 1 Part 2 Part 3 Part 4 Part 5 Conclusion Abstract Supp Figures methods First version of methods Methods with LateX equations citations Add...},
  langid       = {french},
  organization = {Google Docs},
  file         = {/Users/jkobject/Zotero/storage/8NA4DX4G/edit.html}
}

@article{maPancancerGenomeTranscriptome2018,
  title        = {Pan-Cancer Genome and Transcriptome Analyses of 1,699 Paediatric Leukaemias and Solid Tumours},
  author       = {Ma, Xiaotu and Liu, Yu and Liu, Yanling and Alexandrov, Ludmil B. and Edmonson, Michael N. and Gawad, Charles and Zhou, Xin and Li, Yongjin and Rusch, Michael C. and Easton, John and Huether, Robert and Gonzalez-Pena, Veronica and Wilkinson, Mark R. and Hermida, Leandro C. and Davis, Sean and Sioson, Edgar and Pounds, Stanley and Cao, Xueyuan and Ries, Rhonda E. and Wang, Zhaoming and Chen, Xiang and Dong, Li and Diskin, Sharon J. and Smith, Malcolm A. and Guidry Auvil, Jaime M. and Meltzer, Paul S. and Lau, Ching C. and Perlman, Elizabeth J. and Maris, John M. and Meshinchi, Soheil and Hunger, Stephen P. and Gerhard, Daniela S. and Zhang, Jinghui},
  date         = {2018-03},
  journaltitle = {Nature},
  volume       = {555},
  number       = {7696},
  pages        = {371--376},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gc3sx2},
  url          = {http://www.nature.com/articles/nature25795},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{marinovChIPpingBranchesTree2018,
  title        = {{{ChIP-ping}} the Branches of the Tree: Functional Genomics and the Evolution of Eukaryotic Gene Regulation},
  shorttitle   = {{{ChIP-ping}} the Branches of the Tree},
  author       = {Marinov, Georgi K and Kundaje, Anshul},
  date         = {2018-03-01},
  journaltitle = {Briefings in Functional Genomics},
  volume       = {17},
  number       = {2},
  pages        = {116--137},
  issn         = {2041-2649, 2041-2657},
  doi          = {10/gfxbdk},
  url          = {https://academic.oup.com/bfg/article/17/2/116/4909806},
  urldate      = {2018-04-11},
  abstract     = {Advances in the methods for detecting protein–DNA interactions have played a key role in determining the directions of research into the mechanisms of transcriptional regulation. The most recent major technological transformation happened a decade ago, with the move from using tiling arrays [chromatin immunoprecipitation (ChIP)-on-Chip] to high-throughput sequencing (ChIP-seq) as a readout for ChIP assays. In addition to the numerous other ways in which it is superior to arrays, by eliminating the need to design and manufacture them, sequencing also opened the door to carrying out comparative analyses of genome-wide transcription factor occupancy across species and studying chromatin biology in previously less accessible model and nonmodel organisms, thus allowing us to understand the evolution and diversity of regulatory mechanisms in unprecedented detail. Here, we review the biological insights obtained from such studies in recent years and discuss anticipated future developments in the field.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Marinov et Kundaje - 2018 - ChIP-ping the branches of the tree functional gen 2.pdf}
}

@article{marshallHighresolutionSlideseqV2Spatial2022,
  title     = {High-Resolution {{Slide-seqV2}} Spatial Transcriptomics Enables Discovery of Disease-Specific Cell Neighborhoods and Pathways},
  author    = {Marshall, Jamie L. and Noel, Teia and Wang, Qingbo S. and Chen, Haiqi and Murray, Evan and Subramanian, Ayshwarya and Vernon, Katherine A. and {Bazua-Valenti}, Silvana and Liguori, Katie and Keller, Keith and Stickels, Robert R. and McBean, Breanna and Heneghan, Rowan M. and Weins, Astrid and Macosko, Evan Z. and Chen, Fei and Greka, Anna},
  year      = {2022},
  month     = apr,
  journal   = {iScience},
  volume    = {25},
  number    = {4},
  publisher = {Elsevier},
  issn      = {2589-0042},
  doi       = {10.1016/j.isci.2022.104097},
  urldate   = {2024-07-23},
  langid    = {english},
  pmid      = {35372810},
  keywords  = {Cell biology,Pathophysiology,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/LBJ3F598/Marshall et al. - 2022 - High-resolution Slide-seqV2 spatial transcriptomic.pdf}
}

@article{masuhoMolecularDeconvolutionPlatform2018,
  title        = {Molecular {{Deconvolution Platform}} to {{Establish Disease Mechanisms}} by {{Surveying GPCR Signaling}}},
  author       = {Masuho, Ikuo and Chavali, Sreenivas and Muntean, Brian S. and Skamangas, Nickolas K. and Simonyan, Kristina and Patil, Dipak N. and Kramer, Grant M. and Ozelius, Laurie and Babu, M. Madan and Martemyanov, Kirill A.},
  date         = {2018-07},
  journaltitle = {Cell Reports},
  volume       = {24},
  number       = {3},
  pages        = {557-568.e5},
  issn         = {22111247},
  doi          = {10/gdvmpv},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S2211124718310076},
  urldate      = {2019-03-22},
  abstract     = {Despite the wealth of genetic information available, mechanisms underlying pathological effects of disease-associated mutations in components of G protein-coupled receptor (GPCR) signaling cascades remain elusive. In this study, we developed a scalable approach for the functional analysis of clinical variants in GPCR pathways along with a complete analytical framework. We applied the strategy to evaluate an extensive set of dystonia-causing mutations in G protein Gaolf. Our quantitative analysis revealed diverse mechanisms by which pathogenic variants disrupt GPCR signaling, leading to a mechanism-based classification of dystonia. In light of significant clinical heterogeneity, the mechanistic analysis of individual disease-associated variants permits tailoring personalized intervention strategies, which makes it superior to the current phenotype-based approach. We propose that the platform developed in this study can be universally applied to evaluate disease mechanisms for conditions associated with genetic variation in all components of GPCR signaling.},
  langid       = {english}
}

@article{mausIronAccumulationDrives2023,
  title        = {Iron Accumulation Drives Fibrosis, Senescence and the Senescence-Associated Secretory Phenotype},
  author       = {Maus, Mate and López-Polo, Vanessa and Mateo, Lidia and Lafarga, Miguel and Aguilera, Mònica and De Lama, Eugenia and Meyer, Kathleen and Sola, Anna and Lopez-Martinez, Cecilia and López-Alonso, Ines and Guasch-Piqueras, Marc and Hernandez-Gonzalez, Fernanda and Chaib, Selim and Rovira, Miguel and Sanchez, Mayka and Faner, Rosa and Agusti, Alvar and Diéguez-Hurtado, Rodrigo and Ortega, Sagrario and Manonelles, Anna and Engelhardt, Stefan and Monteiro, Freddy and Stephan-Otto Attolini, Camille and Prats, Neus and Albaiceta, Guillermo and Cruzado, Josep M. and Serrano, Manuel},
  date         = {2023-12},
  journaltitle = {Nature Metabolism},
  shortjournal = {Nat Metab},
  volume       = {5},
  number       = {12},
  pages        = {2111--2130},
  publisher    = {Nature Publishing Group},
  issn         = {2522-5812},
  doi          = {10.1038/s42255-023-00928-2},
  url          = {https://www.nature.com/articles/s42255-023-00928-2},
  urldate      = {2024-07-26},
  abstract     = {Fibrogenesis is part of a normal protective response to tissue injury that can become irreversible and progressive, leading to fatal diseases. Senescent cells are a main driver of fibrotic diseases through their secretome, known as senescence-associated secretory phenotype (SASP). Here, we report that cellular senescence, and multiple types of fibrotic diseases in mice and humans are characterized by the accumulation of iron. We show that vascular and hemolytic injuries are efficient in triggering iron accumulation, which in turn can cause senescence and promote fibrosis. Notably, we find that senescent cells persistently accumulate iron, even when the surge of extracellular iron has subdued. Indeed, under normal conditions of extracellular iron, cells exposed to different types of senescence-inducing insults accumulate abundant ferritin-bound iron, mostly within lysosomes, and present high levels of labile iron, which fuels the generation of reactive oxygen species and the SASP. Finally, we demonstrate that detection of iron by magnetic resonance imaging might allow non-invasive assessment of fibrotic burden in the kidneys of mice and in patients with renal fibrosis. Our findings suggest that iron accumulation plays a central role in senescence and fibrosis, even when the initiating events may be independent of iron, and identify iron metabolism as a potential therapeutic target for senescence-associated diseases.},
  langid       = {english},
  keywords     = {Ageing,Iron,Mechanisms of disease,Metabolism,Senescence},
  file         = {/Users/jkobject/Zotero/storage/Y32BHB8F/Maus et al. - 2023 - Iron accumulation drives fibrosis, senescence and .pdf}
}

@article{mccallaIdentifyingStrengthsWeaknesses2023,
  title    = {Identifying Strengths and Weaknesses of Methods for Computational Network Inference from Single-Cell {{RNA-seq}} Data},
  author   = {McCalla, Sunnie Grace and Fotuhi Siahpirani, Alireza and Li, Jiaxin and Pyne, Saptarshi and Stone, Matthew and Periyasamy, Viswesh and Shin, Junha and Roy, Sushmita},
  year     = {2023},
  month    = mar,
  journal  = {G3 Genes{\textbar}Genomes{\textbar}Genetics},
  volume   = {13},
  number   = {3},
  pages    = {jkad004},
  issn     = {2160-1836},
  doi      = {10.1093/g3journal/jkad004},
  urldate  = {2024-04-18},
  abstract = {Single-cell RNA-sequencing (scRNA-seq) offers unparalleled insight into the transcriptional programs of different cellular states by measuring the transcriptome of thousands of individual cells. An emerging problem in the analysis of scRNA-seq is the inference of transcriptional gene regulatory networks and a number of methods with different learning frameworks have been developed to address this problem. Here, we present an expanded benchmarking study of eleven recent network inference methods on seven published scRNA-seq datasets in human, mouse, and yeast considering different types of gold standard networks and evaluation metrics. We evaluate methods based on their computing requirements as well as on their ability to recover the network structure. We find that, while most methods have a modest recovery of experimentally derived interactions based on global metrics such as Area Under the Precision Recall curve, methods are able to capture targets of regulators that are relevant to the system under study. Among the top performing methods that use only expression were SCENIC, PIDC, MERLIN or Correlation. Addition of prior biological knowledge and the estimation of transcription factor activities resulted in the best overall performance with the Inferelator and MERLIN methods that use prior knowledge outperforming methods that use expression alone. We found that imputation for network inference did not improve network inference accuracy and could be detrimental. Comparisons of inferred networks for comparable bulk conditions showed that the networks inferred from scRNA-seq datasets are often better or at par with the networks inferred from bulk datasets. Our analysis should be beneficial in selecting methods for network inference. At the same time, this highlights the need for improved methods and better gold standards for regulatory network inference from scRNAseq datasets.},
  file     = {/Users/jkobject/Zotero/storage/MB6A2FBI/McCalla et al. - 2023 - Identifying strengths and weaknesses of methods fo.pdf}
}

@article{mccarthyHumanDiseaseGenomics2017,
  title        = {Human Disease Genomics: From Variants to Biology},
  shorttitle   = {Human Disease Genomics},
  author       = {McCarthy, Mark I. and MacArthur, Daniel G.},
  date         = {2017-12},
  journaltitle = {Genome Biology},
  volume       = {18},
  number       = {1},
  issn         = {1474-760X},
  doi          = {10/gfxbg3},
  url          = {http://genomebiology.biomedcentral.com/articles/10.1186/s13059-017-1160-z},
  urldate      = {2019-03-22},
  abstract     = {We summarize the remarkable progress that has been made in the identification and functional characterization of DNA sequence variants associated with disease.},
  langid       = {english}
}

@article{mcdonaldProjectDRIVECompendium2017,
  title        = {Project {{DRIVE}}: {{A Compendium}} of {{Cancer Dependencies}} and {{Synthetic Lethal Relationships Uncovered}} by {{Large-Scale}}, {{Deep RNAi Screening}}},
  shorttitle   = {Project {{DRIVE}}},
  author       = {McDonald, E. Robert and family=Weck, given=Antoine, prefix=de, useprefix=true and Schlabach, Michael R. and Billy, Eric and Mavrakis, Konstantinos J. and Hoffman, Gregory R. and Belur, Dhiren and Castelletti, Deborah and Frias, Elizabeth and Gampa, Kalyani and Golji, Javad and Kao, Iris and Li, Li and Megel, Philippe and Perkins, Thomas A. and Ramadan, Nadire and Ruddy, David A. and Silver, Serena J. and Sovath, Sosathya and Stump, Mark and Weber, Odile and Widmer, Roland and Yu, Jianjun and Yu, Kristine and Yue, Yingzi and Abramowski, Dorothee and Ackley, Elizabeth and Barrett, Rosemary and Berger, Joel and Bernard, Julie L. and Billig, Rebecca and Brachmann, Saskia M. and Buxton, Frank and Caothien, Roger and Caushi, Justina X. and Chung, Franklin S. and Cortés-Cros, Marta and {deBeaumont}, Rosalie S. and Delaunay, Clara and Desplat, Aurore and Duong, William and Dwoske, Donald A. and Eldridge, Richard S. and Farsidjani, Ali and Feng, Fei and Feng, JiaJia and Flemming, Daisy and Forrester, William and Galli, Giorgio G. and Gao, Zhenhai and Gauter, François and Gibaja, Veronica and Haas, Kristy and Hattenberger, Marc and Hood, Tami and Hurov, Kristen E. and Jagani, Zainab and Jenal, Mathias and Johnson, Jennifer A. and Jones, Michael D. and Kapoor, Avnish and Korn, Joshua and Liu, Jilin and Liu, Qiumei and Liu, Shumei and Liu, Yue and Loo, Alice T. and Macchi, Kaitlin J. and Martin, Typhaine and McAllister, Gregory and Meyer, Amandine and Mollé, Sandra and Pagliarini, Raymond A. and Phadke, Tanushree and Repko, Brian and Schouwey, Tanja and Shanahan, Frances and Shen, Qiong and Stamm, Christelle and Stephan, Christine and Stucke, Volker M. and Tiedt, Ralph and Varadarajan, Malini and Venkatesan, Kavitha and Vitari, Alberto C. and Wallroth, Marco and Weiler, Jan and Zhang, Jing and Mickanin, Craig and Myer, Vic E. and Porter, Jeffery A. and Lai, Albert and Bitter, Hans and Lees, Emma and Keen, Nicholas and Kauffmann, Audrey and Stegmeier, Frank and Hofmann, Francesco and Schmelzle, Tobias and Sellers, William R.},
  date         = {2017-07},
  journaltitle = {Cell},
  volume       = {170},
  number       = {3},
  pages        = {577-592.e10},
  issn         = {00928674},
  doi          = {10/gbrqc3},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867417308127},
  urldate      = {2019-03-22},
  abstract     = {Elucidation of the mutational landscape of human cancer has progressed rapidly and been accompanied by the development of therapeutics targeting mutant oncogenes. However, a comprehensive mapping of cancer dependencies has lagged behind and the discovery of therapeutic targets for counteracting tumor suppressor gene loss is needed. To identify vulnerabilities relevant to specific cancer subtypes, we conducted a large-scale RNAi screen in which viability effects of mRNA knockdown were assessed for 7,837 genes using an average of 20 shRNAs per gene in 398 cancer cell lines. We describe findings of this screen, outlining the classes of cancer dependency genes and their relationships to genetic, expression, and lineage features. In addition, we describe robust gene-interaction networks recapitulating both protein complexes and functional cooperation among complexes and pathways. This dataset along with a web portal is provided to the community to assist in the discovery and translation of new therapeutic approaches for cancer.},
  langid       = {english}
}

@article{mcfarlandImprovedEstimationCancer2018,
  title        = {Improved Estimation of Cancer Dependencies from Large-Scale {{RNAi}} Screens Using Model-Based Normalization and Data Integration},
  author       = {McFarland, James M. and Ho, Zandra V. and Kugener, Guillaume and Dempster, Joshua M. and Montgomery, Phillip G. and Bryan, Jordan G. and Krill-Burger, John M. and Green, Thomas M. and Vazquez, Francisca and Boehm, Jesse S. and Golub, Todd R. and Hahn, William C. and Root, David E. and Tsherniak, Aviad},
  date         = {2018-12},
  journaltitle = {Nature Communications},
  volume       = {9},
  number       = {1},
  issn         = {2041-1723},
  doi          = {10/gfmtn2},
  url          = {http://www.nature.com/articles/s41467-018-06916-5},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{mcgrathBiochemicalMachinesInterconversion2017,
  title        = {Biochemical {{Machines}} for the {{Interconversion}} of {{Mutual Information}} and {{Work}}},
  author       = {McGrath, Thomas and Jones, Nick S. and family=Wolde, given=Pieter Rein, prefix=ten, useprefix=true and Ouldridge, Thomas E.},
  date         = {2017-01-10},
  journaltitle = {Physical Review Letters},
  volume       = {118},
  number       = {2},
  issn         = {0031-9007, 1079-7114},
  doi          = {10/f9n2bn},
  url          = {https://link.aps.org/doi/10.1103/PhysRevLett.118.028101},
  urldate      = {2018-04-11},
  langid       = {english},
  keywords     = {Condensed Matter - Soft Condensed Matter,Condensed Matter - Statistical Mechanics,Quantitative Biology - Biomolecules},
  annotation   = {00000}
}

@misc{mcinnesUMAPUniformManifold2020,
  title  = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year   = {2020},
  doi    = {10.48550/arXiv.1802.03426}
}

@article{mcleanGREATImprovesFunctional2010,
  title        = {{{GREAT}} Improves Functional Interpretation of Cis-Regulatory Regions},
  author       = {McLean, Cory Y and Bristor, Dave and Hiller, Michael and Clarke, Shoa L and Schaar, Bruce T and Lowe, Craig B and Wenger, Aaron M and Bejerano, Gill},
  date         = {2010-05},
  journaltitle = {Nature Biotechnology},
  volume       = {28},
  number       = {5},
  pages        = {495--501},
  issn         = {1087-0156, 1546-1696},
  doi          = {10.1038/nbt.1630},
  url          = {http://www.nature.com/articles/nbt.1630},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{mcveanIntegratedMapGenetic2012,
  title        = {An Integrated Map of Genetic Variation from 1,092 Human Genomes},
  author       = {McVean, Gil A. and Altshuler (Co-Chair), David M. and Durbin (Co-Chair), Richard M. and Abecasis, Gonçalo R. and Bentley, David R. and Chakravarti, Aravinda and Clark, Andrew G. and Donnelly, Peter and Eichler, Evan E. and Flicek, Paul and Gabriel, Stacey B. and Gibbs, Richard A. and Green, Eric D. and Hurles, Matthew E. and Knoppers, Bartha M. and Korbel, Jan O. and Lander, Eric S. and Lee, Charles and Lehrach, Hans and Mardis, Elaine R. and Marth, Gabor T. and McVean, Gil A. and Nickerson, Deborah A. and Schmidt, Jeanette P. and Sherry, Stephen T. and Wang, Jun and Wilson, Richard K. and Gibbs (Principal Investigator), Richard A. and Dinh, Huyen and Kovar, Christie and Lee, Sandra and Lewis, Lora and Muzny, Donna and Reid, Jeff and Wang, Min and Wang (Principal Investigator), Jun and Fang, Xiaodong and Guo, Xiaosen and Jian, Min and Jiang, Hui and Jin, Xin and Li, Guoqing and Li, Jingxiang and Li, Yingrui and Li, Zhuo and Liu, Xiao and Lu, Yao and Ma, Xuedi and Su, Zhe and Tai, Shuaishuai and Tang, Meifang and Wang, Bo and Wang, Guangbiao and Wu, Honglong and Wu, Renhua and Yin, Ye and Zhang, Wenwei and Zhao, Jiao and Zhao, Meiru and Zheng, Xiaole and Zhou, Yan and Lander (Principal Investigator), Eric S. and Altshuler, David M. and Gabriel (Co-Chair), Stacey B. and Gupta, Namrata and Flicek (Principal Investigator), Paul and Clarke, Laura and Leinonen, Rasko and Smith, Richard E. and Zheng-Bradley, Xiangqun and Bentley (Principal Investigator), David R. and Grocock, Russell and Humphray, Sean and James, Terena and Kingsbury, Zoya and Lehrach (Principal Investigator), Hans and Sudbrak (Project Leader), Ralf and Albrecht, Marcus W. and Amstislavskiy, Vyacheslav S. and Borodina, Tatiana A. and Lienhard, Matthias and Mertes, Florian and Sultan, Marc and Timmermann, Bernd and Yaspo, Marie-Laure and Sherry (Principal Investigator), Stephen T. and McVean (Principal Investigator), Gil A. and Mardis (Co-Principal Investigator) (Co-Chair), Elaine R. and Wilson (Co-Principal Investigator), Richard K. and Fulton, Lucinda and Fulton, Robert and Weinstock, George M. and Durbin (Principal Investigator), Richard M. and Balasubramaniam, Senduran and Burton, John and Danecek, Petr and Keane, Thomas M. and Kolb-Kokocinski, Anja and McCarthy, Shane and Stalker, James and Quail, Michael and Schmidt (Principal Investigator), Jeanette P. and Davies, Christopher J. and Gollub, Jeremy and Webster, Teresa and Wong, Brant and Zhan, Yiping and Auton (Principal Investigator), Adam and Gibbs (Principal Investigator), Richard A. and Yu (Project Leader), Fuli and Bainbridge, Matthew and Challis, Danny and Evani, Uday S. and Lu, James and Muzny, Donna and Nagaswamy, Uma and Reid, Jeff and Sabo, Aniko and Wang, Yi and Yu, Jin and Wang (Principal Investigator), Jun and Coin, Lachlan J. M. and Fang, Lin and Guo, Xiaosen and Jin, Xin and Li, Guoqing and Li, Qibin and Li, Yingrui and Li, Zhenyu and Lin, Haoxiang and Liu, Binghang and Luo, Ruibang and Qin, Nan and Shao, Haojing and Wang, Bingqiang and Xie, Yinlong and Ye, Chen and Yu, Chang and Zhang, Fan and Zheng, Hancheng and Zhu, Hongmei and Marth (Principal Investigator), Gabor T. and Garrison, Erik P. and Kural, Deniz and Lee, Wan-Ping and Fung Leong, Wen and Ward, Alistair N. and Wu, Jiantao and Zhang, Mengyao and Lee (Principal Investigator), Charles and Griffin, Lauren and Hsieh, Chih-Heng and Mills, Ryan E. and Shi, Xinghua and family=Grotthuss, given=Marcin, prefix=von, useprefix=true and Zhang, Chengsheng and Daly (Principal Investigator), Mark J. and DePristo (Project Leader), Mark A. and Altshuler, David M. and Banks, Eric and Bhatia, Gaurav and Carneiro, Mauricio O. and family=Angel, given=Guillermo, prefix=del, useprefix=true and Gabriel, Stacey B. and Genovese, Giulio and Gupta, Namrata and Handsaker, Robert E. and Hartl, Chris and Lander, Eric S. and McCarroll, Steven A. and Nemesh, James C. and Poplin, Ryan E. and Schaffner, Stephen F. and Shakir, Khalid and Yoon (Principal Investigator), Seungtai C. and Lihm, Jayon and Makarov, Vladimir and Jin (Principal Investigator), Hanjun and Kim, Wook and Cheol Kim, Ki and Korbel (Principal Investigator), Jan O. and Rausch, Tobias and Flicek (Principal Investigator), Paul and Beal, Kathryn and Clarke, Laura and Cunningham, Fiona and Herrero, Javier and McLaren, William M. and Ritchie, Graham R. S. and Smith, Richard E. and Zheng-Bradley, Xiangqun and Clark (Principal Investigator), Andrew G. and Gottipati, Srikanth and Keinan, Alon and Rodriguez-Flores, Juan L. and Sabeti (Principal Investigator), Pardis C. and Grossman, Sharon R. and Tabrizi, Shervin and Tariyal, Ridhi and Cooper (Principal Investigator), David N. and Ball, Edward V. and Stenson, Peter D. and Bentley (Principal Investigator), David R. and Barnes, Bret and Bauer, Markus and Keira Cheetham, R. and Cox, Tony and Eberle, Michael and Humphray, Sean and Kahn, Scott and Murray, Lisa and Peden, John and Shaw, Richard and Ye (Principal Investigator), Kai and Batzer (Principal Investigator), Mark A. and Konkel, Miriam K. and Walker, Jerilyn A. and MacArthur (Principal Investigator), Daniel G. and Lek, Monkol and {Sudbrak (Project Leader)} and Amstislavskiy, Vyacheslav S. and Herwig, Ralf and Shriver (Principal Investigator), Mark D. and Bustamante (Principal Investigator), Carlos D. and Byrnes, Jake K. and De La Vega, Francisco M. and Gravel, Simon and Kenny, Eimear E. and Kidd, Jeffrey M. and Lacroute, Phil and Maples, Brian K. and Moreno-Estrada, Andres and Zakharia, Fouad and Halperin (Principal Investigator), Eran and Baran, Yael and Craig (Principal Investigator), David W. and Christoforides, Alexis and Homer, Nils and Izatt, Tyler and Kurdoglu, Ahmet A. and Sinari, Shripad A. and Squire, Kevin and Sherry (Principal Investigator), Stephen T. and Xiao, Chunlin and Sebat (Principal Investigator), Jonathan and Bafna, Vineet and Ye, Kenny and Burchard (Principal Investigator), Esteban G. and Hernandez (Principal Investigator), Ryan D. and Gignoux, Christopher R. and Haussler (Principal Investigator), David and Katzman, Sol J. and James Kent, W. and Howie, Bryan and Ruiz-Linares (Principal Investigator), Andres and Dermitzakis (Principal Investigator), Emmanouil T. and Lappalainen, Tuuli and Devine (Principal Investigator), Scott E. and Liu, Xinyue and Maroo, Ankit and Tallon, Luke J. and Rosenfeld (Principal Investigator), Jeffrey A. and Michelson, Leslie P. and Abecasis (Principal Investigator) (Co-Chair), Gonçalo R. and Min Kang (Project Leader), Hyun and Anderson, Paul and Angius, Andrea and Bigham, Abigail and Blackwell, Tom and Busonero, Fabio and Cucca, Francesco and Fuchsberger, Christian and Jones, Chris and Jun, Goo and Li, Yun and Lyons, Robert and Maschio, Andrea and Porcu, Eleonora and Reinier, Fred and Sanna, Serena and Schlessinger, David and Sidore, Carlo and Tan, Adrian and Kate Trost, Mary and Awadalla (Principal Investigator), Philip and Hodgkinson, Alan and Lunter (Principal Investigator), Gerton and McVean (Principal Investigator) (Co-Chair), Gil A. and Marchini (Principal Investigator), Jonathan L. and Myers (Principal Investigator), Simon and Churchhouse, Claire and Delaneau, Olivier and Gupta-Hinch, Anjali and Iqbal, Zamin and Mathieson, Iain and Rimmer, Andy and Xifara, Dionysia K. and Oleksyk (Principal Investigator), Taras K. and Fu (Principal Investigator), Yunxin and Liu, Xiaoming and Xiong, Momiao and Jorde (Principal Investigator), Lynn and Witherspoon, David and Xing, Jinchuan and Eichler (Principal Investigator), Evan E. and Browning (Principal Investigator), Brian L. and Alkan, Can and Hajirasouliha, Iman and Hormozdiari, Fereydoun and Ko, Arthur and Sudmant, Peter H. and Mardis (Co-Principal Investigator), Elaine R. and Chen, Ken and Chinwalla, Asif and Ding, Li and Dooling, David and Koboldt, Daniel C. and McLellan, Michael D. and Wallis, John W. and Wendl, Michael C. and Zhang, Qunyuan and Durbin (Principal Investigator), Richard M. and Hurles (Principal Investigator), Matthew E. and Albers, Cornelis A. and Ayub, Qasim and Balasubramaniam, Senduran and Chen, Yuan and Coffey, Alison J. and Colonna, Vincenza and Danecek, Petr and Huang, Ni and Jostins, Luke and Keane, Thomas M. and Li, Heng and McCarthy, Shane and Scally, Aylwyn and Stalker, James and Walter, Klaudia and Xue, Yali and Zhang, Yujun and Gerstein (Principal Investigator), Mark B. and Abyzov, Alexej and Balasubramanian, Suganthi and Chen, Jieming and Clarke, Declan and Fu, Yao and Habegger, Lukas and Harmanci, Arif O. and Jin, Mike and Khurana, Ekta and Jasmine Mu, Xinmeng and Sisu, Cristina and Li, Yingrui and Luo, Ruibang and Zhu, Hongmei and Lee (Principal Investigator) (Co-Chair), Charles and Griffin, Lauren and Hsieh, Chih-Heng and Mills, Ryan E. and Shi, Xinghua and family=Grotthuss, given=Marcin, prefix=von, useprefix=true and Zhang, Chengsheng and Marth (Principal Investigator), Gabor T. and Garrison, Erik P. and Kural, Deniz and Lee, Wan-Ping and Ward, Alistair N. and Wu, Jiantao and Zhang, Mengyao and McCarroll (Project Leader), Steven A. and Altshuler, David M. and Banks, Eric and family=Angel, given=Guillermo, prefix=del, useprefix=true and Genovese, Giulio and Handsaker, Robert E. and Hartl, Chris and Nemesh, James C. and Shakir, Khalid and Yoon (Principal Investigator), Seungtai C. and Lihm, Jayon and Makarov, Vladimir and Degenhardt, Jeremiah and Flicek (Principal Investigator), Paul and Clarke, Laura and Smith, Richard E. and Zheng-Bradley, Xiangqun and Korbel (Principal Investigator) (Co-Chair), Jan O. and Rausch, Tobias and Stütz, Adrian M. and Bentley (Principal Investigator), David R. and Barnes, Bret and Keira Cheetham, R. and Eberle, Michael and Humphray, Sean and Kahn, Scott and Murray, Lisa and Shaw, Richard and Ye (Principal Investigator), Kai and Batzer (Principal Investigator), Mark A. and Konkel, Miriam K. and Walker, Jerilyn A. and Lacroute, Phil and Craig (Principal Investigator), David W. and Homer, Nils and Church, Deanna and Xiao, Chunlin and Sebat (Principal Investigator), Jonathan and Bafna, Vineet and Michaelson, Jacob J. and Ye, Kenny and Devine (Principal Investigator), Scott E. and Liu, Xinyue and Maroo, Ankit and Tallon, Luke J. and Lunter (Principal Investigator), Gerton and McVean (Principal Investigator), Gil A. and Iqbal, Zamin and Witherspoon, David and Xing, Jinchuan and Eichler (Principal Investigator) (Co-Chair), Evan E. and Alkan, Can and Hajirasouliha, Iman and Hormozdiari, Fereydoun and Ko, Arthur and Sudmant, Peter H. and Chen, Ken and Chinwalla, Asif and Ding, Li and McLellan, Michael D. and Wallis, John W. and Hurles (Principal Investigator) (Co-Chair), Matthew E. and Blackburne, Ben and Li, Heng and Lindsay, Sarah J. and Ning, Zemin and Scally, Aylwyn and Walter, Klaudia and Zhang, Yujun and Gerstein (Principal Investigator), Mark B. and Abyzov, Alexej and Chen, Jieming and Clarke, Declan and Khurana, Ekta and Jasmine Mu, Xinmeng and Sisu, Cristina and Gibbs (Principal Investigator) (Co-Chair), Richard A. and Yu (Project Leader), Fuli and Bainbridge, Matthew and Challis, Danny and Evani, Uday S. and Kovar, Christie and Lewis, Lora and Lu, James and Muzny, Donna and Nagaswamy, Uma and Reid, Jeff and Sabo, Aniko and Yu, Jin and Guo, Xiaosen and Li, Yingrui and Wu, Renhua and Marth (Principal Investigator) (Co-Chair), Gabor T. and Garrison, Erik P. and Fung Leong, Wen and Ward, Alistair N. and family=Angel, given=Guillermo, prefix=del, useprefix=true and DePristo, Mark A. and Gabriel, Stacey B. and Gupta, Namrata and Hartl, Chris and Poplin, Ryan E. and Clark (Principal Investigator), Andrew G. and Rodriguez-Flores, Juan L. and Flicek (Principal Investigator), Paul and Clarke, Laura and Smith, Richard E. and Zheng-Bradley, Xiangqun and MacArthur (Principal Investigator), Daniel G. and Bustamante (Principal Investigator), Carlos D. and Gravel, Simon and Craig (Principal Investigator), David W. and Christoforides, Alexis and Homer, Nils and Izatt, Tyler and Sherry (Principal Investigator), Stephen T. and Xiao, Chunlin and Dermitzakis (Principal Investigator), Emmanouil T. and Abecasis (Principal Investigator), Gonçalo R. and Min Kang, Hyun and McVean (Principal Investigator), Gil A. and Mardis (Principal Investigator), Elaine R. and Dooling, David and Fulton, Lucinda and Fulton, Robert and Koboldt, Daniel C. and Durbin (Principal Investigator), Richard M. and Balasubramaniam, Senduran and Keane, Thomas M. and McCarthy, Shane and Stalker, James and Gerstein (Principal Investigator), Mark B. and Balasubramanian, Suganthi and Habegger, Lukas and Garrison, Erik P. and Gibbs (Principal Investigator), Richard A. and Bainbridge, Matthew and Muzny, Donna and Yu, Fuli and Yu, Jin and family=Angel, given=Guillermo, prefix=del, useprefix=true and Handsaker, Robert E. and Makarov, Vladimir and Rodriguez-Flores, Juan L. and Jin (Principal Investigator), Hanjun and Kim, Wook and Cheol Kim, Ki and Flicek (Principal Investigator), Paul and Beal, Kathryn and Clarke, Laura and Cunningham, Fiona and Herrero, Javier and McLaren, William M. and Ritchie, Graham R. S. and Zheng-Bradley, Xiangqun and Tabrizi, Shervin and MacArthur (Principal Investigator), Daniel G. and Lek, Monkol and Bustamante (Principal Investigator), Carlos D. and De La Vega, Francisco M. and Craig (Principal Investigator), David W. and Kurdoglu, Ahmet A. and Lappalainen, Tuuli and Rosenfeld (Principal Investigator), Jeffrey A. and Michelson, Leslie P. and Awadalla (Principal Investigator), Philip and Hodgkinson, Alan and McVean (Principal Investigator), Gil A. and Chen, Ken and Chen, Yuan and Colonna, Vincenza and Frankish, Adam and Harrow, Jennifer and Xue, Yali and Gerstein (Principal Investigator) (Co-Chair), Mark B. and Abyzov, Alexej and Balasubramanian, Suganthi and Chen, Jieming and Clarke, Declan and Fu, Yao and Harmanci, Arif O. and Jin, Mike and Khurana, Ekta and Jasmine Mu, Xinmeng and Sisu, Cristina and Gibbs (Principal Investigator), Richard A. and Fowler, Gerald and Hale, Walker and Kalra, Divya and Kovar, Christie and Muzny, Donna and Reid, Jeff and Wang (Principal Investigator), Jun and Guo, Xiaosen and Li, Guoqing and Li, Yingrui and Zheng, Xiaole and Altshuler, David M. and Flicek (Principal Investigator) (Co-Chair), Paul and Clarke (Project Leader), Laura and Barker, Jonathan and Kelman, Gavin and Kulesha, Eugene and Leinonen, Rasko and McLaren, William M. and Radhakrishnan, Rajesh and Roa, Asier and Smirnov, Dmitriy and Smith, Richard E. and Streeter, Ian and Toneva, Iliana and Vaughan, Brendan and Zheng-Bradley, Xiangqun and Bentley (Principal Investigator), David R. and Cox, Tony and Humphray, Sean and Kahn, Scott and Sudbrak (Project Leader), Ralf and Albrecht, Marcus W. and Lienhard, Matthias and Craig (Principal Investigator), David W. and Izatt, Tyler and Kurdoglu, Ahmet A. and Sherry (Principal Investigator) (Co-Chair), Stephen T. and Ananiev, Victor and Belaia, Zinaida and Beloslyudtsev, Dimitriy and Bouk, Nathan and Chen, Chao and Church, Deanna and Cohen, Robert and Cook, Charles and Garner, John and Hefferon, Timothy and Kimelman, Mikhail and Liu, Chunlei and Lopez, John and Meric, Peter and O’Sullivan, Chris and Ostapchuk, Yuri and Phan, Lon and Ponomarov, Sergiy and Schneider, Valerie and Shekhtman, Eugene and Sirotkin, Karl and Slotta, Douglas and Xiao, Chunlin and Zhang, Hua and Haussler (Principal Investigator), David and Abecasis (Principal Investigator), Gonçalo R. and McVean (Principal Investigator), Gil A. and Alkan, Can and Ko, Arthur and Dooling, David and Durbin (Principal Investigator), Richard M. and Balasubramaniam, Senduran and Keane, Thomas M. and McCarthy, Shane and Stalker, James and Chakravarti (Co-Chair), Aravinda and Knoppers (Co-Chair), Bartha M. and Abecasis, Gonçalo R. and Barnes, Kathleen C. and Beiswanger, Christine and Burchard, Esteban G. and Bustamante, Carlos D. and Cai, Hongyu and Cao, Hongzhi and Durbin, Richard M. and Gharani, Neda and Gibbs, Richard A. and Gignoux, Christopher R. and Gravel, Simon and Henn, Brenna and Jones, Danielle and Jorde, Lynn and Kaye, Jane S. and Keinan, Alon and Kent, Alastair and Kerasidou, Angeliki and Li, Yingrui and Mathias, Rasika and McVean, Gil A. and Moreno-Estrada, Andres and Ossorio, Pilar N. and Parker, Michael and Reich, David and Rotimi, Charles N. and Royal, Charmaine D. and Sandoval, Karla and Su, Yeyang and Sudbrak, Ralf and Tian, Zhongming and Timmermann, Bernd and Tishkoff, Sarah and Toji, Lorraine H. and Tyler Smith, Chris and Via, Marc and Wang, Yuhong and Yang, Huanming and Yang, Ling and Zhu, Jiayong and Bodmer, Walter and Bedoya, Gabriel and Ruiz-Linares, Andres and Zhi Ming, Cai and Yang, Gao and Jia You, Chu and Peltonen, Leena and Garcia-Montero, Andres and Orfao, Alberto and Dutil, Julie and Martinez-Cruzado, Juan C. and Oleksyk, Taras K. and Brooks, Lisa D. and Felsenfeld, Adam L. and McEwen, Jean E. and Clemm, Nicholas C. and Duncanson, Audrey and Dunn, Michael and Green, Eric D. and Guyer, Mark S. and Peterson, Jane L. and Abecasis, Goncalo R. and Auton, Adam and Brooks, Lisa D. and DePristo, Mark A. and Durbin, Richard M. and Handsaker, Robert E. and Min Kang, Hyun and Marth, Gabor T. and McVean, Gil A.},
  date         = {2012-10-31},
  journaltitle = {Nature},
  volume       = {491},
  number       = {7422},
  pages        = {56--65},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/f4k2v2},
  url          = {http://www.nature.com/doifinder/10.1038/nature11632},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@online{MechanismPrognosticMarker,
  title   = {Mechanism of Prognostic Marker {{SPOCK3}} Affecting Malignant Progression of Prostate Cancer and Construction of Prognostic Model - {{PMC}}},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10416445/},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/ZWM7ARIW/PMC10416445.html}
}

@article{memmesheimerLearningPreciselyTimed2014,
  title        = {Learning {{Precisely Timed Spikes}}},
  author       = {Memmesheimer, Raoul-Martin and Rubin, Ran and Ölveczky, Bence~P. and Sompolinsky, Haim},
  date         = {2014-05},
  journaltitle = {Neuron},
  volume       = {82},
  number       = {4},
  pages        = {925--938},
  issn         = {08966273},
  doi          = {10/f53466},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0896627314002566},
  urldate      = {2018-04-11},
  abstract     = {To signal the onset of salient sensory features or execute well-timed motor sequences, neuronal circuits must transform streams of incoming spike trains into precisely timed firing. To address the efficiency and fidelity with which neurons can perform such computations, we developed a theory to characterize the capacity of feedforward networks to generate desired spike sequences. We find the maximum number of desired output spikes a neuron can implement to be 0.1–0.3 per synapse. We further present a biologically plausible learning rule that allows feedforward and recurrent networks to learn multiple mappings between inputs and desired spike sequences. We apply this framework to reconstruct synaptic weights from spiking activity and study the precision with which the temporal structure of ongoing behavior can be inferred from the spiking of premotor neurons. This work provides a powerful approach for characterizing the computational and learning capacities of single neurons and neuronal circuits.},
  langid       = {english},
  annotation   = {00072},
  file         = {/Users/jeremie/Documents/science/ML/NN/spiking/Memmesheimer et al. - 2014 - Learning Precisely Timed Spikes.pdf}
}

@article{mendez-lucioMolEFoundationModel2024,
  title      = {{{MolE}}: A Foundation Model for Molecular Graphs Using Disentangled Attention},
  shorttitle = {{{MolE}}},
  author     = {{M{\'e}ndez-Lucio}, Oscar and Nicolaou, Christos A. and Earnshaw, Berton},
  year       = {2024},
  month      = nov,
  journal    = {Nature Communications},
  volume     = {15},
  number     = {1},
  pages      = {9431},
  publisher  = {Nature Publishing Group},
  issn       = {2041-1723},
  doi        = {10.1038/s41467-024-53751-y},
  urldate    = {2025-02-25},
  abstract   = {Models that accurately predict properties based on chemical structure are valuable tools in the chemical sciences. However, for many properties, public and private training sets are typically small, making it difficult for models to generalize well outside of the training data. Recently, this lack of generalization has been mitigated by using self-supervised pretraining on large unlabeled datasets, followed by finetuning on smaller, labeled datasets. Inspired by these advances, we report MolE, a Transformer architecture adapted for molecular graphs together with a two-step pretraining strategy. The first step of pretraining is a self-supervised approach focused on learning chemical structures trained on {\textasciitilde}842 million molecular graphs, and the second step is a massive multi-task approach to learn biological information. We show that finetuning models that were pretrained in this way perform better than the best published results on 10 of the 22 ADMET (absorption, distribution, metabolism, excretion and toxicity) tasks included in the Therapeutic Data Commons leaderboard (c. September 2023).},
  copyright  = {2024 The Author(s)},
  langid     = {english},
  keywords   = {Computational models,Machine learning},
  file       = {/Users/jkobject/Zotero/storage/EXSLB73N/Méndez-Lucio et al. - 2024 - MolE a foundation model for molecular graphs usin.pdf}
}

@misc{mentzerFiniteScalarQuantization2023,
  title         = {Finite {{Scalar Quantization}}: {{VQ-VAE Made Simple}}},
  shorttitle    = {Finite {{Scalar Quantization}}},
  author        = {Mentzer, Fabian and Minnen, David and Agustsson, Eirikur and Tschannen, Michael},
  year          = {2023},
  month         = oct,
  number        = {arXiv:2309.15505},
  eprint        = {2309.15505},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2309.15505},
  urldate       = {2025-02-06},
  abstract      = {We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/QGE6WMAS/Mentzer et al. - 2023 - Finite Scalar Quantization VQ-VAE Made Simple.pdf;/Users/jkobject/Zotero/storage/3ABMC5HW/2309.html}
}

@article{mermelGISTIC20FacilitatesSensitive2011,
  title        = {{{GISTIC2}}.0 Facilitates Sensitive and Confident Localization of the Targets of Focal Somatic Copy-Number Alteration in Human Cancers},
  author       = {Mermel, Craig H and Schumacher, Steven E and Hill, Barbara and Meyerson, Matthew L and Beroukhim, Rameen and Getz, Gad},
  date         = {2011},
  journaltitle = {Genome Biology},
  volume       = {12},
  number       = {4},
  pages        = {R41},
  issn         = {1465-6906},
  doi          = {10.1186/gb-2011-12-4-r41},
  url          = {http://genomebiology.biomedcentral.com/articles/10.1186/gb-2011-12-4-r41},
  urldate      = {2019-03-22},
  abstract     = {We describe methods with enhanced power and specificity to identify genes targeted by somatic copy-number alterations (SCNAs) that drive cancer growth. By separating SCNA profiles into underlying arm-level and focal alterations, we improve the estimation of background rates for each category. We additionally describe a probabilistic method for defining the boundaries of selected-for SCNA regions with user-defined confidence. Here we detail this revised computational approach, GISTIC2.0, and validate its performance in real and simulated datasets.},
  langid       = {english}
}

@article{mesquitaHFerritinEssentialMacrophages2020,
  title   = {H-Ferritin is essential for macrophages' capacity to store or detoxify exogenously added iron},
  author  = {Mesquita, Gielifar and others},
  journal = {Scientific Reports},
  volume  = {10},
  pages   = {3061},
  year    = {2020},
  doi     = {10.1038/s41598-020-59898-0}
}

@article{meyersComputationalCorrectionCopy2017,
  title        = {Computational Correction of Copy Number Effect Improves Specificity of {{CRISPR}}–{{Cas9}} Essentiality Screens in Cancer Cells},
  author       = {Meyers, Robin M and Bryan, Jordan G and McFarland, James M and Weir, Barbara A and Sizemore, Ann E and Xu, Han and Dharia, Neekesh V and Montgomery, Phillip G and Cowley, Glenn S and Pantel, Sasha and Goodale, Amy and Lee, Yenarae and Ali, Levi D and Jiang, Guozhi and Lubonja, Rakela and Harrington, William F and Strickland, Matthew and Wu, Ting and Hawes, Derek C and Zhivich, Victor A and Wyatt, Meghan R and Kalani, Zohra and Chang, Jaime J and Okamoto, Michael and Stegmaier, Kimberly and Golub, Todd R and Boehm, Jesse S and Vazquez, Francisca and Root, David E and Hahn, William C and Tsherniak, Aviad},
  date         = {2017-12},
  journaltitle = {Nature Genetics},
  volume       = {49},
  number       = {12},
  pages        = {1779--1784},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gchc3x},
  url          = {http://www.nature.com/articles/ng.3984},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{miikkulainenEvolvingDeepNeural,
  title      = {Evolving {{Deep Neural Networks}}},
  author     = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak and Technologies, Sentient},
  pages      = {8},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/NE/Miikkulainen et al. - Evolving Deep Neural Networks.pdf}
}

@article{miklDissectingSplicingDecisions2018,
  title        = {Dissecting Splicing Decisions and Cell-to-Cell Variability with Designed Sequence Libraries},
  author       = {Mikl, Martin and Hamburg, Amit and Pilpel, Yitzhak and Segal, Eran},
  date         = {2018-08-16},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbf7},
  url          = {http://biorxiv.org/lookup/doi/10.1101/392605},
  urldate      = {2019-03-22},
  abstract     = {Most human genes are alternatively spliced, allowing for a large expansion of the proteome. The multitude of regulatory inputs to splicing limits the potential to infer general principles from investigating native sequences. Here, we created a rationally designed library of {$>$}32,000 splicing events to dissect the complexity of splicing regulation through systematic sequence alterations. Measuring RNA and protein splice isoforms allowed us to investigate both cause and effect of splicing decisions, quantify diverse regulatory inputs and accurately predict (R2=0.75-0.85) isoform ratios from sequence and secondary structure. By profiling individual cells, we measure the cell-to-cell variability of splicing decisions and show that it can be encoded in the DNA and influenced by regulatory inputs, opening the door for a novel, single-cell perspective on splicing regulation.},
  langid       = {english},
  annotation   = {00000}
}

@article{milacicReactomePathwayKnowledgebase2024,
  title        = {The {{Reactome Pathway Knowledgebase}} 2024},
  author       = {Milacic, Marija and Beavers, Deidre and Conley, Patrick and Gong, Chuqiao and Gillespie, Marc and Griss, Johannes and Haw, Robin and Jassal, Bijay and Matthews, Lisa and May, Bruce and Petryszak, Robert and Ragueneau, Eliot and Rothfels, Karen and Sevilla, Cristoffer and Shamovsky, Veronica and Stephan, Ralf and Tiwari, Krishna and Varusai, Thawfeek and Weiser, Joel and Wright, Adam and Wu, Guanming and Stein, Lincoln and Hermjakob, Henning and D’Eustachio, Peter},
  date         = {2024-01-05},
  journaltitle = {Nucleic Acids Research},
  shortjournal = {Nucleic Acids Research},
  volume       = {52},
  number       = {D1},
  pages        = {D672-D678},
  issn         = {0305-1048},
  doi          = {10.1093/nar/gkad1025},
  url          = {https://doi.org/10.1093/nar/gkad1025},
  urldate      = {2024-07-26},
  abstract     = {The Reactome Knowledgebase (https://reactome.org), an Elixir and GCBR core biological data resource, provides manually curated molecular details of a broad range of normal and disease-related biological processes. Processes are annotated as an ordered network of molecular transformations in a single consistent data model. Reactome thus functions both as a digital archive of manually curated human biological processes and as a tool for discovering functional relationships in data such as gene expression profiles or somatic mutation catalogs from tumor cells. Here we review progress towards annotation of the entire human proteome, targeted annotation of disease-causing genetic variants of proteins and of small-molecule drugs in a pathway context, and towards supporting explicit annotation of cell- and tissue-specific pathways. Finally, we briefly discuss issues involved in making Reactome more fully interoperable with other related resources such as the Gene Ontology and maintaining the resulting community resource network.},
  file         = {/Users/jkobject/Zotero/storage/YQ69U5PY/Milacic et al. - 2024 - The Reactome Pathway Knowledgebase 2024.pdf;/Users/jkobject/Zotero/storage/IEQR68P3/7369850.html}
}

@unpublished{milletariVNetFullyConvolutional2016,
  title       = {V-{{Net}}: {{Fully Convolutional Neural Networks}} for {{Volumetric Medical Image Segmentation}}},
  shorttitle  = {V-{{Net}}},
  author      = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  date        = {2016-06-15},
  eprint      = {1606.04797},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1606.04797},
  urldate     = {2019-03-22},
  abstract    = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation  = {00000}
}

@article{miloNetworkMotifsSimple2002,
  title        = {Network {{Motifs}}: {{Simple Building Blocks}} of {{Complex Networks}}},
  shorttitle   = {Network {{Motifs}}},
  author       = {Milo, R.},
  date         = {2002-10-25},
  journaltitle = {Science},
  volume       = {298},
  number       = {5594},
  pages        = {824--827},
  issn         = {00368075, 10959203},
  doi          = {10/dbddmt},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.298.5594.824},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@article{misekGermlineVariationContributes2024,
  title     = {Germline Variation Contributes to False Negatives in {{CRISPR-based}} Experiments with Varying Burden across Ancestries},
  author    = {Misek, Sean A. and Fultineer, Aaron and Kalfon, Jeremie and Noorbakhsh, Javad and Boyle, Isabella and Roy, Priyanka and Dempster, Joshua and Petronio, Lia and Huang, Katherine and Saadat, Alham and Green, Thomas and Brown, Adam and Doench, John G. and Root, David E. and McFarland, James M. and Beroukhim, Rameen and Boehm, Jesse S.},
  year      = 2024,
  month     = jun,
  journal   = {Nature Communications},
  volume    = {15},
  number    = {1},
  pages     = {4892},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-024-48957-z},
  urldate   = {2025-12-01},
  abstract  = {Reducing disparities is vital for equitable access to precision treatments in cancer. Socioenvironmental factors are a major driver of disparities, but differences in genetic variation likely also contribute. The impact of genetic ancestry on prioritization of cancer targets in drug discovery pipelines has not been systematically explored due to the absence of pre-clinical data at the appropriate scale. Here, we analyze data from 611 genome-scale CRISPR/Cas9 viability experiments in human cell line models to identify ancestry-associated genetic dependencies essential for cell survival. Surprisingly, we find that most putative associations between ancestry and dependency arise from artifacts related to germline variants. Our analysis suggests that for 1.2-2.5\% of guides, germline variants in sgRNA targeting sequences reduce cutting by the CRISPR/Cas9 nuclease, disproportionately affecting cell models derived from individuals of recent African descent. We propose three approaches to mitigate this experimental bias, enabling the scientific community to address these disparities.},
  copyright = {2024 The Author(s)},
  langid    = {english},
  keywords  = {Cancer genomics,Cancer models,CRISPR-Cas9 genome editing},
  file      = {/Users/jkobject/Zotero/storage/R2GQDPX6/Misek et al. - 2024 - Germline variation contributes to false negatives in CRISPR-based experiments with varying burden ac.pdf}
}

@article{mizusakiNeocortexLeanMean2016,
  title        = {Neocortex: A Lean Mean Memory Storage Machine},
  shorttitle   = {Neocortex},
  author       = {Mizusaki, Beatriz E P and Stepanyants, Armen and Chklovskii, Dmitri B and Sjöström, P Jesper},
  date         = {2016-04-26},
  journaltitle = {Nature Neuroscience},
  volume       = {19},
  number       = {5},
  pages        = {643--644},
  issn         = {1097-6256, 1546-1726},
  doi          = {10/gfxbdq},
  url          = {http://www.nature.com/doifinder/10.1038/nn.4292},
  urldate      = {2018-04-11},
  abstract     = {Connectivity patterns of neocortex exhibit several odd properties: for example, most neighboring excitatory neurons do not connect, which seems curiously wasteful. Brunel’s elegant theoretical treatment reveals how optimal information storage can naturally impose these peculiar properties.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/simulation/Mizusaki et al. - 2016 - Neocortex a lean mean memory storage machine.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title        = {Human-Level Control through Deep Reinforcement Learning},
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date         = {2015-02},
  journaltitle = {Nature},
  volume       = {518},
  number       = {7540},
  pages        = {529--533},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gc3h75},
  url          = {http://www.nature.com/articles/nature14236},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@unpublished{mnihPlayingAtariDeep2013,
  title       = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author      = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date        = {2013-12-19},
  eprint      = {1312.5602},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1312.5602},
  urldate     = {2019-03-22},
  abstract    = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning}
}

@article{mohammedIntegrativeApproachAnalyzing2016,
  title        = {An Integrative Approach for Analyzing Hundreds of Neurons in Task Performing Mice Using Wide-Field Calcium Imaging},
  author       = {Mohammed, Ali I. and Gritton, Howard J. and Tseng, Hua-an and Bucklin, Mark E. and Yao, Zhaojie and Han, Xue},
  date         = {2016-08},
  journaltitle = {Scientific Reports},
  volume       = {6},
  number       = {1},
  issn         = {2045-2322},
  doi          = {10/f78jj5},
  url          = {http://www.nature.com/articles/srep20986},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/imaging/Mohammed et al. - 2016 - An integrative approach for analyzing hundreds of .pdf}
}

@misc{moinfartUnsupervisedDeepDisentangled2025,
  title  = {Unsupervised Deep Disentangled Representation of Single-Cell Omics},
  author = {Moinfar, Amir Ali and Theis, Fabian J.},
  year   = {2025},
  eprint = {2024.11.06.622266},
  doi    = {10.1101/2024.11.06.622266}
}

@misc{moinfarUnsupervisedDeepDisentangled2025,
  title         = {Unsupervised {{Deep Disentangled Representation}} of {{Single-Cell Omics}}},
  author        = {Moinfar, Amir Ali and Theis, Fabian J.},
  year          = 2025,
  month         = aug,
  primaryclass  = {New Results},
  pages         = {2024.11.06.622266},
  publisher     = {bioRxiv},
  issn          = {2692-8205},
  doi           = {10.1101/2024.11.06.622266},
  urldate       = {2025-12-03},
  abstract      = {Single-cell genomics allows for the unbiased exploration of cellular heterogeneity. Representation learning methods summarize high-dimensional single-cell data into a manageable latent space in a typically nonlinear fashion, allowing cross-sample integration or generative modeling. However, these methods often produce entangled representations, limiting interpretability and downstream analyses. Existing disentanglement methods instead either require supervised information or impose sparsity and linearity, which may not capture the complexity of biological data. We, therefore, introduce Disentangled Representation Variational Inference (DRVI), an unsupervised deep generative model that learns nonlinear, disentangled representations of single-cell omics. This is achieved by combining recently introduced additive decoders with nonlinear pooling, for which we theoretically prove disentanglement under reasonable assumptions. We validate DRVI's disentanglement capabilities across diverse relevant biological problems, from development to perturbational studies and cell atlases, decomposing, for example, the Human Lung Cell Atlas into meaningful, interpretable latent dimensions. Moreover, we demonstrate that if applied to batch integration, DRVI's integration quality does not suffer from the disentanglement constraints and instead is on par with entangled integration methods. With its disentangled latent space, DRVI is inherently interpretable and facilitates the identification of rare cell types, provides novel insights into cellular heterogeneity beyond traditional cell types, and highlights developmental stages.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {\copyright{} 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/4FCRH3R2/Moinfar and Theis - 2025 - Unsupervised Deep Disentangled Representation of Single-Cell Omics.pdf}
}

@article{montagueFrameworkMesencephalicDopamine1996,
  title        = {A Framework for Mesencephalic Dopamine Systems Based on Predictive {{Hebbian}} Learning},
  author       = {Montague, Pr and Dayan, P and Sejnowski, Tj},
  date         = {1996-03-01},
  journaltitle = {The Journal of Neuroscience},
  volume       = {16},
  number       = {5},
  pages        = {1936--1947},
  issn         = {0270-6474, 1529-2401},
  doi          = {10/gfn79s},
  url          = {http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.16-05-01936.1996},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{moralesStandardizedFrameworkRepresentation2018,
  title    = {A Standardized Framework for Representation of Ancestry Data in Genomics Studies, with Application to the {{NHGRI-EBI GWAS Catalog}}},
  author   = {Morales, Joannella and Welter, Danielle and Bowler, Emily H. and Cerezo, Maria and Harris, Laura W. and McMahon, Aoife C. and Hall, Peggy and Junkins, Heather A. and Milano, Annalisa and Hastings, Emma and Malangone, Cinzia and Buniello, Annalisa and Burdett, Tony and Flicek, Paul and Parkinson, Helen and Cunningham, Fiona and Hindorff, Lucia A. and MacArthur, Jacqueline A. L.},
  year     = {2018},
  month    = feb,
  journal  = {Genome Biology},
  volume   = {19},
  number   = {1},
  pages    = {21},
  issn     = {1474-760X},
  doi      = {10.1186/s13059-018-1396-2},
  urldate  = {2024-07-19},
  abstract = {The accurate description of ancestry is essential to interpret, access, and integrate human genomics data, and to ensure that these benefit individuals from all ancestral backgrounds. However, there are no established guidelines for the representation of ancestry information. Here we describe a framework for the accurate and standardized description of sample ancestry, and validate it by application to the NHGRI-EBI GWAS Catalog. We confirm known biases and gaps in diversity, and find that African and Hispanic or Latin American ancestry populations contribute a disproportionately high number of associations. It is our hope that widespread adoption of this framework will lead to improved analysis, interpretation, and integration of human genomics data.},
  keywords = {Ancestry,Diversity,Genome-wide association studies,Genomics,GWAS Catalog,Population genetics},
  file     = {/Users/jkobject/Zotero/storage/2L34KQLD/Morales et al. - 2018 - A standardized framework for representation of anc.pdf;/Users/jkobject/Zotero/storage/DJZH5LC4/s13059-018-1396-2.html}
}

@article{moriartyFormingNeuralNetworks,
  title      = {Forming {{Neural Networks}} through {{E}} Cient and {{Adaptive Coevolution}}},
  author     = {Moriarty, David E and Miikkulainen, Risto},
  pages      = {28},
  abstract   = {This article demonstrates the advantages of a cooperative, coevolutionary search in di cult control problems. The SANE system coevolves a population of neurons that cooperate to form a functioning neural network. In this process, neurons assume di erent but overlapping roles, resulting in a robust encoding of control behavior. SANE is shown to be more e cient, more adaptive, and maintain higher levels of diversity than the more common network-based population approaches. Further empirical studies illustrate the emergent neuron specializations and the di erent roles the neurons assume in the population.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/NE/Moriarty et Miikkulainen - Forming Neural Networks through E cient and Adapti.pdf}
}

@article{movvaDecipheringRegulatoryDNA2018,
  title        = {Deciphering Regulatory {{DNA}} Sequences and Noncoding Genetic Variants Using Neural Network Models of Massively Parallel Reporter Assays},
  author       = {Movva, Rajiv and Greenside, Peyton and Shrikumar, Avanti and Kundaje, Anshul},
  date         = {2018-08-17},
  journaltitle = {bioRxiv},
  doi          = {10/gd3w6z},
  url          = {http://biorxiv.org/lookup/doi/10.1101/393926},
  urldate      = {2019-03-22},
  abstract     = {The relationship between noncoding DNA sequence and gene expression is not well-understood. Massively parallel reporter assays (MPRAs), which quantify the regulatory activity of large libraries of DNA sequences in parallel, are a powerful approach to characterize this relationship. We present SNPpet, a convolutional neural network (CNN)-based framework to predict and interpret the regulatory activity of DNA sequences as measured by MPRAs. While our method is generally applicable to a variety of MPRA designs, here we trained SNPpet on the Sharpr-MPRA dataset that measures the activity of ∼500,000 constructs tiling 15,720 regulatory regions in human K562 and HepG2 cell lines. SNPpet’s predictions were moderately correlated (Spearman ρ = 0.28) with measured activity and were within range of replicate concordance of the assay. State-of-the-art model interpretation methods revealed high-resolution predictive regulatory sequence features that overlapped transcription factor (TF) binding motifs. We used the model to investigate the cell type and chromatin state preferences of predictive TF motifs. We explored the ability of SNPpet to predict the allelic effects of regulatory variants in an independent MPRA experiment and fine map putative functional SNPs in loci associated with lipid traits. Our results suggest that interpretable deep learning models trained on MPRA data have the potential to reveal meaningful patterns in regulatory DNA sequences and prioritize regulatory genetic variants, especially as larger, higher-quality datasets are produced.},
  langid       = {english},
  annotation   = {00000}
}

@article{muller-dottExpandingCoverageRegulons2023,
  title    = {Expanding the Coverage of Regulons from High-Confidence Prior Knowledge for Accurate Estimation of Transcription Factor Activities},
  author   = {{M{\"u}ller-Dott}, Sophia and Tsirvouli, Eirini and Vazquez, Miguel and Ramirez~Flores, Ricardo~O and {Badia-i-Mompel}, Pau and Fallegger, Robin and T{\"u}rei, D{\'e}nes and L{\ae}greid, Astrid and {Saez-Rodriguez}, Julio},
  year     = {2023},
  month    = nov,
  journal  = {Nucleic Acids Research},
  volume   = {51},
  number   = {20},
  pages    = {10934--10949},
  issn     = {0305-1048},
  doi      = {10.1093/nar/gkad841},
  urldate  = {2024-04-18},
  abstract = {Gene regulation plays a critical role in the cellular processes that underlie human health and disease. The regulatory relationship between transcription factors (TFs), key regulators of gene expression, and their target genes, the so called TF regulons, can be coupled with computational algorithms to estimate the activity of TFs. However, to interpret these findings accurately, regulons of high reliability and coverage are needed. In this study, we present and evaluate a collection of regulons created using the CollecTRI meta-resource containing signed TF--gene interactions for 1186 TFs. In this context, we introduce a workflow to integrate information from multiple resources and assign the sign of regulation to TF--gene interactions that could be applied to other comprehensive knowledge bases. We find that the signed CollecTRI-derived regulons outperform other public collections of regulatory interactions in accurately inferring changes in TF activities in perturbation experiments. Furthermore, we showcase the value of the regulons by examining TF activity profiles in three different cancer types and exploring TF activities at the level of single-cells. Overall, the CollecTRI-derived TF regulons enable the accurate and comprehensive estimation of TF activities and thereby help to interpret transcriptomics data.},
  file     = {/Users/jkobject/Zotero/storage/C9LNPCZ2/Müller-Dott et al. - 2023 - Expanding the coverage of regulons from high-confi.pdf}
}

@article{nairXRayStructuresMycMax2003,
  title      = {X-{{Ray Structures}} of {{Myc-Max}} and {{Mad-Max Recognizing DNA}}: {{Molecular Bases}} of {{Regulation}} by {{Proto-Oncogenic Transcription Factors}}},
  shorttitle = {X-{{Ray Structures}} of {{Myc-Max}} and {{Mad-Max Recognizing DNA}}},
  author     = {Nair, Satish K. and Burley, Stephen K.},
  year       = {2003},
  month      = jan,
  journal    = {Cell},
  volume     = {112},
  number     = {2},
  pages      = {193--205},
  issn       = {0092-8674},
  doi        = {10.1016/S0092-8674(02)01284-9},
  urldate    = {2025-02-17},
  abstract   = {X-ray structures of the basic/helix-loop-helix/leucine zipper (bHLHZ) domains of Myc-Max and Mad-Max heterodimers bound to their common DNA target (Enhancer or E box hexanucleotide, 5{$\prime$}-CACGTG-3{$\prime$}) have been determined at 1.9 {\AA} and 2.0 {\AA} resolution, respectively. E box recognition by these two structurally similar transcription factor pairs determines whether a cell will divide and proliferate (Myc-Max) or differentiate and become quiescent (Mad-Max). Deregulation of Myc has been implicated in the development of many human cancers, including Burkitt's lymphoma, neuroblastomas, and small cell lung cancers. Both quasisymmetric heterodimers resemble the symmetric Max homodimer, albeit with marked structural differences in the coiled-coil leucine zipper regions that explain preferential homo- and heteromeric dimerization of these three evolutionarily related DNA-binding proteins. The Myc-Max heterodimer, but not its Mad-Max counterpart, dimerizes to form a bivalent heterotetramer, which explains how Myc can upregulate expression of genes with promoters bearing widely separated E boxes.}
}

@article{nanayakkaraCharacterisingRiskInhospital2018,
  title        = {Characterising Risk of In-Hospital Mortality Following Cardiac Arrest Using Machine Learning: {{A}} Retrospective International Registry Study},
  shorttitle   = {Characterising Risk of In-Hospital Mortality Following Cardiac Arrest Using Machine Learning},
  author       = {Nanayakkara, Shane and Fogarty, Sam and Tremeer, Michael and Ross, Kelvin and Richards, Brent and Bergmeir, Christoph and Xu, Sheng and Stub, Dion and Smith, Karen and Tacey, Mark and Liew, Danny and Pilcher, David and Kaye, David M.},
  editor       = {Saria, Suchi},
  date         = {2018-11-30},
  journaltitle = {PLOS Medicine},
  volume       = {15},
  number       = {11},
  pages        = {e1002709},
  issn         = {1549-1676},
  doi          = {10/gfxbfw},
  url          = {http://dx.plos.org/10.1371/journal.pmed.1002709},
  urldate      = {2019-03-22},
  abstract     = {Background Resuscitated cardiac arrest is associated with high mortality; however, the ability to estimate risk of adverse outcomes using existing illness severity scores is limited. Using in-hospital data available within the first 24 hours of admission, we aimed to develop more accurate models of risk prediction using both logistic regression (LR) and machine learning (ML) techniques, with a combination of demographic, physiologic, and biochemical information.},
  langid       = {english},
  annotation   = {00000}
}

@article{narangBLOCKSPARSERECURRENTNEURAL,
  title      = {{{BLOCK-SPARSE RECURRENT NEURAL NETWORKS}}},
  author     = {Narang, Sharan and Diamos, Gregory and Undersander, Eric},
  pages      = {12},
  abstract   = {Recurrent Neural Networks (RNNs) are used in state-of-the-art models in domains such as speech recognition, machine translation, and language modelling. Sparsity is a technique to reduce compute and memory requirements of deep learning models. Sparse RNNs are easier to deploy on devices and high-end server processors. Even though sparse operations need less compute and memory relative to their dense counterparts, the speed-up observed by using sparse operations is less than expected on different hardware platforms. In order to address this issue, we investigate two different approaches to induce block sparsity in RNNs: pruning blocks of weights in a layer and using group lasso regularization to create blocks of weights with zeros. Using these techniques, we demonstrate that we can create block-sparse RNNs with sparsity ranging from 80\% to 90\% with small loss in accuracy. This allows us to reduce the model size by roughly 10×. Additionally, we can prune a larger dense network to recover this loss in accuracy while maintaining high block sparsity and reducing the overall parameter count. Our technique works with a variety of block sizes up to 32×32. Block-sparse RNNs eliminate overheads related to data storage and irregular memory accesses while increasing hardware efficiency compared to unstructured sparsity.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/Narang et al. - BLOCK-SPARSE RECURRENT NEURAL NETWORKS.pdf}
}

@book{nealBayesianLearningNeural1996,
  title      = {Bayesian {{Learning}} for {{Neural Networks}}},
  author     = {Neal, Radford M.},
  editor     = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  editortype = {redactor},
  date       = {1996},
  series     = {Lecture {{Notes}} in {{Statistics}}},
  volume     = {118},
  publisher  = {Springer New York},
  location   = {New York, NY},
  doi        = {10.1007/978-1-4612-0745-0},
  url        = {http://link.springer.com/10.1007/978-1-4612-0745-0},
  urldate    = {2018-04-11},
  abstract   = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties | the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
  isbn       = {978-0-387-94724-2 978-1-4612-0745-0},
  langid     = {english},
  annotation = {00000}
}

@unpublished{neelakantanNeuralProgrammerInducing2015,
  title       = {Neural {{Programmer}}: {{Inducing Latent Programs}} with {{Gradient Descent}}},
  shorttitle  = {Neural {{Programmer}}},
  author      = {Neelakantan, Arvind and Le, Quoc V. and Sutskever, Ilya},
  date        = {2015-11-16},
  eprint      = {1511.04834},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1511.04834},
  urldate     = {2019-03-22},
  abstract    = {Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, a neural network augmented with a small set of basic arithmetic and logic operations that can be trained end-to-end using backpropagation. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@misc{nguyen2023hyenadnalongrangegenomicsequence,
  title         = {HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution},
  author        = {Eric Nguyen and Michael Poli and Marjan Faizi and Armin Thomas and Callum Birch-Sykes and Michael Wornow and Aman Patel and Clayton Rabideau and Stefano Massaroli and Yoshua Bengio and Stefano Ermon and Stephen A. Baccus and Chris Ré},
  year          = {2023},
  eprint        = {2306.15794},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2306.15794}
}

@article{nguyenTransformersTearsImproving2019,
  title         = {Transformers without {{Tears}}: {{Improving}} the {{Normalization}} of {{Self-Attention}}},
  shorttitle    = {Transformers without {{Tears}}},
  author        = {Nguyen, Toan Q. and Salazar, Julian},
  year          = {2019},
  month         = nov,
  eprint        = {1910.05895},
  primaryclass  = {cs, stat},
  doi           = {10.5281/zenodo.3525484},
  urldate       = {2024-07-19},
  abstract      = {We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose \${\textbackslash}ell\_2\$ normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/G8W7AIPD/Nguyen and Salazar - 2019 - Transformers without Tears Improving the Normaliz.pdf;/Users/jkobject/Zotero/storage/6PWFLPMG/1910.html}
}

@misc{nichaniHowTransformersLearn2024,
  title         = {How {{Transformers Learn Causal Structure}} with {{Gradient Descent}}},
  author        = {Nichani, Eshaan and Damian, Alex and Lee, Jason D.},
  year          = {2024},
  month         = feb,
  number        = {arXiv:2402.14735},
  eprint        = {2402.14735},
  primaryclass  = {cs, math, stat},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2402.14735},
  urldate       = {2024-07-23},
  abstract      = {The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/I369Q8HA/Nichani et al. - 2024 - How Transformers Learn Causal Structure with Gradi.pdf;/Users/jkobject/Zotero/storage/EWNED6NB/2402.html}
}

@misc{nichaniHowTransformersLearn2024a,
  title         = {How {{Transformers Learn Causal Structure}} with {{Gradient Descent}}},
  author        = {Nichani, Eshaan and Damian, Alex and Lee, Jason D.},
  year          = 2024,
  month         = aug,
  number        = {arXiv:2402.14735},
  eprint        = {2402.14735},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2402.14735},
  urldate       = {2025-12-12},
  abstract      = {The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/QX4F6RQ6/Nichani et al. - 2024 - How Transformers Learn Causal Structure with Gradient Descent.pdf;/Users/jkobject/Zotero/storage/XMGLU4IQ/2402.html}
}

@misc{nourisaGeneRNIBLivingBenchmark2025,
  title  = {geneRNIB: a living benchmark for gene regulatory network inference},
  author = {Nourisa, Jalil and others},
  year   = {2025},
  eprint = {2025.02.25.640181},
  doi    = {10.1101/2025.02.25.640181}
}

@article{oksuzTranscriptionFactorsInteract2023,
  title    = {Transcription Factors Interact with {{RNA}} to Regulate Genes},
  author   = {Oksuz, Ozgur and Henninger, Jonathan E. and {Warneford-Thomson}, Robert and Zheng, Ming M. and Erb, Hailey and Vancura, Adrienne and Overholt, Kalon J. and Hawken, Susana Wilson and Banani, Salman F. and Lauman, Richard and Reich, Lauren N. and Robertson, Anne L. and Hannett, Nancy M. and Lee, Tong I. and Zon, Leonard I. and Bonasio, Roberto and Young, Richard A.},
  year     = {2023},
  month    = jul,
  journal  = {Molecular Cell},
  volume   = {83},
  number   = {14},
  pages    = {2449-2463.e13},
  issn     = {1097-2765},
  doi      = {10.1016/j.molcel.2023.06.012},
  urldate  = {2024-04-19},
  abstract = {Transcription factors (TFs) orchestrate the gene expression programs that define each cell's identity. The canonical TF accomplishes this with two domains, one that binds specific DNA sequences and the other that binds protein coactivators or corepressors. We find that at least half of TFs also bind RNA, doing so through a previously unrecognized domain with sequence and functional features analogous to the arginine-rich motif of the HIV transcriptional activator Tat. RNA binding contributes to TF function by promoting the dynamic association between DNA, RNA, and TF on chromatin. TF-RNA interactions are a conserved feature important for vertebrate development and disrupted in disease. We propose that the ability to bind DNA, RNA, and protein is a general property of many TFs and is fundamental to their gene regulatory function.},
  keywords = {arginine-rich motif,chromatin,development,gene regulation,RNA,RNA-binding proteins,single-molecule imaging,transcription factor,zebrafish},
  file     = {/Users/jkobject/Zotero/storage/HQMCKV45/Oksuz et al. - 2023 - Transcription factors interact with RNA to regulat.pdf;/Users/jkobject/Zotero/storage/FMZXDBLS/S1097276523004343.html}
}

@online{oordRepresentationLearningContrastive2019,
  title       = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author      = {family=Oord, given=Aaron, prefix=van den, useprefix=false and Li, Yazhe and Vinyals, Oriol},
  date        = {2019-01-22},
  eprint      = {1807.03748},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.1807.03748},
  url         = {http://arxiv.org/abs/1807.03748},
  urldate     = {2025-02-27},
  abstract    = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  pubstate    = {prepublished},
  version     = {2},
  keywords    = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file        = {/Users/jkobject/Zotero/storage/FUVXSCFX/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf;/Users/jkobject/Zotero/storage/YLN6GT9T/1807.html}
}

@misc{openaiGPT4TechnicalReport2024,
  title  = {GPT-4 Technical Report},
  author = {{OpenAI} and others},
  year   = {2024},
  doi    = {10.48550/arXiv.2303.08774}
}

@misc{OpenproblemsbioOpenproblemsv22024,
  title        = {Openproblems-Bio/Openproblems-V2},
  year         = {2024},
  month        = jul,
  urldate      = {2024-07-15},
  abstract     = {Formalizing and benchmarking open problems in single-cell genomics},
  copyright    = {MIT},
  howpublished = {Open Problems in Single-Cell Analysis},
  keywords     = {benchmarking,nextflow,single-cell,viash}
}

@misc{oquabDINOv2LearningRobust2024,
  title         = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle    = {{{DINOv2}}},
  author        = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year          = {2024},
  month         = feb,
  number        = {arXiv:2304.07193},
  eprint        = {2304.07193},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2304.07193},
  urldate       = {2025-02-25},
  abstract      = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {/Users/jkobject/Zotero/storage/828W5HFF/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Su.pdf;/Users/jkobject/Zotero/storage/UN2JZ3T6/2304.html}
}

@unpublished{osokinGANsBiologicalImage2017,
  title       = {{{GANs}} for {{Biological Image Synthesis}}},
  author      = {Osokin, Anton and Chessel, Anatole and Salas, Rafael E. Carazo and Vaggi, Federico},
  date        = {2017-08-15},
  eprint      = {1708.04692},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1708.04692},
  urldate     = {2019-03-22},
  abstract    = {In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multichannel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{ouyangMaximumMeanDiscrepancy2022,
  title  = {Maximum Mean Discrepancy for Generalization in the Presence of Distribution and Missingness Shift},
  author = {Ouyang, Long and Key, Alex},
  year   = {2022},
  doi    = {10.48550/arXiv.2111.10344}
}

@misc{pagliardiniFasterCausalAttention2023,
  title         = {Faster {{Causal Attention Over Large Sequences Through Sparse Flash Attention}}},
  author        = {Pagliardini, Matteo and Paliotta, Daniele and Jaggi, Martin and Fleuret, Fran{\c c}ois},
  year          = {2023},
  month         = jun,
  number        = {arXiv:2306.01160},
  eprint        = {2306.01160},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2306.01160},
  urldate       = {2024-04-18},
  abstract      = {Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by \$2.0{\textbackslash}times\$ and \$3.3{\textbackslash}times\$ for sequences of respectively \$8k\$ and \$16k\$ tokens.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/F4P22JYF/Pagliardini et al. - 2023 - Faster Causal Attention Over Large Sequences Throu.pdf;/Users/jkobject/Zotero/storage/ZM53X2M2/2306.html}
}

@article{pallaSquidpyScalableFramework2022,
  title      = {Squidpy: A Scalable Framework for Spatial Omics Analysis},
  shorttitle = {Squidpy},
  author     = {Palla, Giovanni and Spitzer, Hannah and Klein, Michal and Fischer, David and Schaar, Anna Christina and Kuemmerle, Louis Benedikt and Rybakov, Sergei and Ibarra, Ignacio L. and Holmberg, Olle and Virshup, Isaac and Lotfollahi, Mohammad and Richter, Sabrina and Theis, Fabian J.},
  year       = 2022,
  month      = feb,
  journal    = {Nature Methods},
  volume     = {19},
  number     = {2},
  pages      = {171--178},
  publisher  = {Nature Publishing Group},
  issn       = {1548-7105},
  doi        = {10.1038/s41592-021-01358-2},
  urldate    = {2025-12-02},
  abstract   = {Spatial omics data are advancing the study of tissue organization and cellular communication at an unprecedented scale. Flexible tools are required to store, integrate and visualize the large diversity of spatial omics data. Here, we present Squidpy, a Python framework that brings together tools from omics and image analysis to enable scalable description of spatial molecular data, such as transcriptome or multivariate proteins. Squidpy provides efficient infrastructure and numerous analysis methods that allow to efficiently store, manipulate and interactively visualize spatial omics data. Squidpy is extensible and can be interfaced with a variety of already existing libraries for the scalable analysis of spatial omics data.},
  copyright  = {2022 The Author(s)},
  langid     = {english},
  keywords   = {Data integration,Imaging,Software,Transcriptomics},
  file       = {/Users/jkobject/Zotero/storage/7MJK6M7N/Palla et al. - 2022 - Squidpy a scalable framework for spatial omics analysis.pdf}
}

@article{panDeepSurveyingAlternative2008,
  title        = {Deep Surveying of Alternative Splicing Complexity in the Human Transcriptome by High-Throughput Sequencing},
  author       = {Pan, Qun and Shai, Ofer and Lee, Leo J and Frey, Brendan J and Blencowe, Benjamin J},
  date         = {2008-12},
  journaltitle = {Nature Genetics},
  volume       = {40},
  number       = {12},
  pages        = {1413--1415},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/dsvbfh},
  url          = {http://www.nature.com/articles/ng.259},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{panzeriCrackingNeuralCode2017,
  title        = {Cracking the {{Neural Code}} for {{Sensory Perception}} by {{Combining Statistics}}, {{Intervention}}, and {{Behavior}}},
  author       = {Panzeri, Stefano and Harvey, Christopher D. and Piasini, Eugenio and Latham, Peter E. and Fellin, Tommaso},
  date         = {2017-02},
  journaltitle = {Neuron},
  volume       = {93},
  number       = {3},
  pages        = {491--507},
  issn         = {08966273},
  doi          = {10/f9tkvt},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0896627316310091},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Panzeri et al. - 2017 - Cracking the Neural Code for Sensory Perception by.pdf}
}

@misc{PapersCodeDeep,
  title        = {Papers with {{Code}} - {{Deep Networks}} with {{Stochastic Depth}}},
  urldate      = {2024-07-19},
  abstract     = {\#20 best model for Image Classification on SVHN (Percentage error metric)},
  howpublished = {https://paperswithcode.com/paper/deep-networks-with-stochastic-depth},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/MWAVFSJW/deep-networks-with-stochastic-depth.html}
}

@article{pardo-de-santayanaBenefitsTraditionalKnowledge,
  title  = {The Benefits of Traditional Knowledge},
  author = {Pardo-De-Santayana, Manuel and Macía, Manuel J},
  pages  = {2},
  doi    = {10/gfxbg5},
  langid = {english}
}

@article{parisiLifelongLearningHuman2017,
  title        = {Lifelong Learning of Human Actions with Deep Neural Network Self-Organization},
  author       = {Parisi, German I. and Tani, Jun and Weber, Cornelius and Wermter, Stefan},
  date         = {2017-12},
  journaltitle = {Neural Networks},
  volume       = {96},
  pages        = {137--149},
  issn         = {08936080},
  doi          = {10/gcgr4m},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0893608017302034},
  urldate      = {2018-04-11},
  abstract     = {Lifelong learning is fundamental in autonomous robotics for the acquisition and fine-tuning of knowledge through experience. However, conventional deep neural models for action recognition from videos do not account for lifelong learning but rather learn a batch of training data with a predefined number of action classes and samples. Thus, there is the need to develop learning systems with the ability to incrementally process available perceptual cues and to adapt their responses over time. We propose a selforganizing neural architecture for incrementally learning to classify human actions from video sequences. The architecture comprises growing self-organizing networks equipped with recurrent neurons for processing time-varying patterns. We use a set of hierarchically arranged recurrent networks for the unsupervised learning of action representations with increasingly large spatiotemporal receptive fields. Lifelong learning is achieved in terms of prediction-driven neural dynamics in which the growth and the adaptation of the recurrent networks are driven by their capability to reconstruct temporally ordered input sequences. Experimental results on a classification task using two action benchmark datasets show that our model is competitive with state-of-the-art methods for batch learning also when a significant number of sample labels are missing or corrupted during training sessions. Additional experiments show the ability of our model to adapt to non-stationary input avoiding catastrophic interference.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/Parisi et al. - 2017 - Lifelong learning of human actions with deep neura.pdf}
}

@article{parkBayesianManifoldLearning,
  title      = {Bayesian {{Manifold Learning}}: {{The Locally Linear Latent Variable Model}}},
  author     = {Park, Mijung and Jitkrittum, Wittawat and Qamar, Ahmad and Szabo, Zoltan and Buesing, Lars and Sahani, Maneesh},
  pages      = {9},
  abstract   = {We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{parkChIPSeqAdvantages2009,
  title      = {{{ChIP}}--Seq: Advantages and Challenges of a Maturing Technology},
  shorttitle = {{{ChIP}}--Seq},
  author     = {Park, Peter J.},
  year       = {2009},
  month      = oct,
  journal    = {Nature Reviews Genetics},
  volume     = {10},
  number     = {10},
  pages      = {669--680},
  publisher  = {Nature Publishing Group},
  issn       = {1471-0064},
  doi        = {10.1038/nrg2641},
  urldate    = {2024-07-19},
  abstract   = {Chromatin immunoprecipitation followed by sequencing (ChIP--seq) can be used to map DNA-binding proteins and histone modifications in a genome-wide manner at base-pair resolution.ChIP--seq offers superior data quality to chromatin immunoprecipitation followed by microarray (ChIP--chip), and its advantages include higher resolution, less noise, higher genome coverage and wider dynamic range.To eliminate bias in fragmentation and sequencing, a control sample (generally input DNA) should also be sequenced. Other issues to consider in experimental design include the quality of the antibodies and the depth of sequencing.Genome alignment and the identification of enriched regions present challenges for data analysis, and there are several strategies available.Owing to increased genome coverage, a substantial fraction of the repetitive regions in the genome can now be examined.Increased sensitivity and specificity in the mapping of transcription factor binding sites has facilitated motif discovery and target identification.Detailed profiling of histone modifications and nucleosome positions enables greater understanding of epigenetic mechanisms in development and differentiation.As the cost of sequencing continues to decrease, ChIP--seq will be the method of choice over array-based approaches in nearly all cases.},
  copyright  = {2009 Springer Nature Limited},
  langid     = {english},
  keywords   = {Agriculture,Animal Genetics and Genomics,Biomedicine,Cancer Research,Gene Function,general,Human Genetics},
  file       = {/Users/jkobject/Zotero/storage/3DZDACNC/Park - 2009 - ChIP–seq advantages and challenges of a maturing .pdf}
}

@unpublished{pathakCuriositydrivenExplorationSelfsupervised2017,
  title       = {Curiosity-Driven {{Exploration}} by {{Self-supervised Prediction}}},
  author      = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date        = {2017-05-15},
  eprint      = {1705.05363},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1705.05363},
  urldate     = {2019-03-22},
  abstract    = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  annotation  = {00000}
}

@inproceedings{pathakCuriosityDrivenExplorationSelfSupervised2017a,
  title      = {Curiosity-{{Driven Exploration}} by {{Self-Supervised Prediction}}},
  author     = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date       = {2017-07},
  pages      = {488--489},
  publisher  = {IEEE},
  doi        = {10.1109/CVPRW.2017.70},
  url        = {http://ieeexplore.ieee.org/document/8014804/},
  urldate    = {2018-04-11},
  abstract   = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  isbn       = {978-1-5386-0733-6},
  langid     = {english},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf}
}

@article{pavlopoulosUsingGraphTheory2011,
  title        = {Using Graph Theory to Analyze Biological Networks},
  author       = {Pavlopoulos, Georgios A and Secrier, Maria and Moschopoulos, Charalampos N and Soldatos, Theodoros G and Kossida, Sophia and Aerts, Jan and Schneider, Reinhard and Bagos, Pantelis G},
  date         = {2011-12},
  journaltitle = {BioData Mining},
  volume       = {4},
  number       = {1},
  issn         = {1756-0381},
  doi          = {10/bggzw9},
  url          = {http://biodatamining.biomedcentral.com/articles/10.1186/1756-0381-4-10},
  urldate      = {2018-04-11},
  abstract     = {Understanding complex systems often requires a bottom-up analysis towards a systems biology approach. The need to investigate a system, not only as individual components but as a whole, emerges. This can be done by examining the elementary constituents individually and then how these are connected. The myriad components of a system and their interactions are best characterized as networks and they are mainly represented as graphs where thousands of nodes are connected with thousands of vertices. In this article we demonstrate approaches, models and methods from the graph theory universe and we discuss ways in which they can be used to reveal hidden properties and features of a network. This network profiling combined with knowledge extraction will help us to better understand the biological significance of the system.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Pavlopoulos et al. - 2011 - Using graph theory to analyze biological networks.pdf}
}

@misc{pearceTranscriptFormerCrossSpeciesGenerative2025,
  title  = {A Cross-Species Generative Cell Atlas Across 1.5 Billion Years of Evolution: The TranscriptFormer Single-cell Model},
  author = {Pearce, Joshua D. and others},
  year   = {2025},
  eprint = {2025.04.25.650731},
  doi    = {10.1101/2025.04.25.650731}
}

@unpublished{pearlTheoreticalImpedimentsMachine2018,
  title       = {Theoretical {{Impediments}} to {{Machine Learning With Seven Sparks}} from the {{Causal Revolution}}},
  author      = {Pearl, Judea},
  date        = {2018-01-11},
  eprint      = {1801.04016},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1801.04016},
  urldate     = {2019-03-22},
  abstract    = {Current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title      = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author     = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year       = {2011},
  journal    = {Journal of Machine Learning Research},
  volume     = {12},
  number     = {85},
  pages      = {2825--2830},
  issn       = {1533-7928},
  urldate    = {2024-07-19},
  abstract   = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file       = {/Users/jkobject/Zotero/storage/UB9W75TT/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@article{pehlevanHebbianAntiHebbianNeural2015,
  title        = {A {{Hebbian}}/{{Anti-Hebbian Neural Network}} for {{Linear Subspace Learning}}: {{A Derivation}} from {{Multidimensional Scaling}} of {{Streaming Data}}},
  shorttitle   = {A {{Hebbian}}/{{Anti-Hebbian Neural Network}} for {{Linear Subspace Learning}}},
  author       = {Pehlevan, Cengiz and Hu, Tao and Chklovskii, Dmitri B.},
  date         = {2015-07},
  journaltitle = {Neural Computation},
  volume       = {27},
  number       = {7},
  pages        = {1461--1495},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/f7f6dv},
  url          = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00745},
  urldate      = {2018-04-11},
  langid       = {english},
  keywords     = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/simulation/Pehlevan et al. - 2015 - A HebbianAnti-Hebbian Neural Network for Linear S.pdf}
}

@article{petersCellsRobertHooke2024,
  title      = {The Cells of {{Robert Hooke}}: Wombs, Brains and Ammonites},
  shorttitle = {The Cells of {{Robert Hooke}}},
  author     = {Peters, Winfried S.},
  year       = 2024,
  month      = may,
  journal    = {Notes and Records: the Royal Society Journal of the History of Science},
  issn       = {0035-9149},
  doi        = {10.1098/rsnr.2023.0081},
  urldate    = {2025-12-02},
  abstract   = {Robert Hooke (1635--1703) is commonly credited for introducing the term cell into biology when describing the microscopic structure of plant tissues in his Micrographia of 1665. This narrative ignores that, at the time, cell was an established term denoting linearly arranged elements of structures with functions in the storage, modification and transport of materials (e.g. the uterus, colon, brain, etc.). In analogy to the cells of these organs, Hooke interpreted plant cells as elements of continuous tubes for the storage and regulated movement of vital fluids. Hooke also was one of the few British natural philosophers who regarded `serpentine-stones', the fossilized chambered shells of cephalopods (e.g. ammonites) that seemed to lack living counterparts, as remnants of organisms. He considered Nautilus a living serpentine-stone and referred to the chambers in its shell as cells, postulating that gas and liquid were transported along and stored within these cavities for buoyancy regulation. Hooke published this theory in 1696, but probably developed it much earlier. In Micrographia, he described ammonite cells just before plant cells, suggesting an overlooked rhetoric function of his report of cells in plants. By visualizing microscopic equivalents of macroscopic cellular structures for the first time, Hooke reinforced the common notion that living matter generally was characterized by such structures. Consequently, the presence of cells in serpentine-stones implicitly supported his organismic interpretation of these fossils.},
  file       = {/Users/jkobject/Zotero/storage/NLLP4JUY/Peters - 2024 - The cells of Robert Hooke wombs, brains and ammonites.pdf;/Users/jkobject/Zotero/storage/IRYMLI97/rsnr.2023.html}
}

@misc{petersTuneNotTune2019,
  title         = {To {{Tune}} or {{Not}} to {{Tune}}? {{Adapting Pretrained Representations}} to {{Diverse Tasks}}},
  shorttitle    = {To {{Tune}} or {{Not}} to {{Tune}}?},
  author        = {Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.},
  year          = {2019},
  month         = jun,
  number        = {arXiv:1903.05987},
  eprint        = {1903.05987},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1903.05987},
  urldate       = {2025-02-25},
  abstract      = {While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/2YK5TV4H/Peters et al. - 2019 - To Tune or Not to Tune Adapting Pretrained Repres.pdf;/Users/jkobject/Zotero/storage/LV2P5NP5/1903.html}
}

@article{petrykMCM2PromotesSymmetric2018,
  title        = {{{MCM2}} Promotes Symmetric Inheritance of Modified Histones during {{DNA}} Replication},
  author       = {Petryk, Nataliya and Dalby, Maria and Wenger, Alice and Stromme, Caroline B. and Strandsby, Anne and Andersson, Robin and Groth, Anja},
  date         = {2018-09-28},
  journaltitle = {Science},
  volume       = {361},
  number       = {6409},
  pages        = {1389--1392},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/gd3cp6},
  url          = {http://www.sciencemag.org/lookup/doi/10.1126/science.aau0294},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@misc{peyreComputationalOptimalTransport2020,
  title         = {Computational {{Optimal Transport}}},
  author        = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year          = 2020,
  month         = mar,
  number        = {arXiv:1803.00567},
  eprint        = {1803.00567},
  primaryclass  = {stat},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1803.00567},
  urldate       = {2025-12-04},
  abstract      = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  keywords      = {Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/ZLL3KB2R/Peyré and Cuturi - 2020 - Computational Optimal Transport.pdf;/Users/jkobject/Zotero/storage/6RUAJGWE/1803.html}
}

@unpublished{phamEfficientNeuralArchitecture2018,
  title       = {Efficient {{Neural Architecture Search}} via {{Parameter Sharing}}},
  author      = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
  date        = {2018-02-09},
  eprint      = {1802.03268},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1802.03268},
  urldate     = {2019-03-22},
  abstract    = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{piatkevichRoboticMultidimensionalDirected2018,
  title        = {A Robotic Multidimensional Directed Evolution Approach Applied to Fluorescent Voltage Reporters},
  author       = {Piatkevich, Kiryl D. and Jung, Erica E. and Straub, Christoph and Linghu, Changyang and Park, Demian and Suk, Ho-Jun and Hochbaum, Daniel R. and Goodwin, Daniel and Pnevmatikakis, Eftychios and Pak, Nikita and Kawashima, Takashi and Yang, Chao-Tsung and Rhoades, Jeffrey L. and Shemesh, Or and Asano, Shoh and Yoon, Young-Gyu and Freifeld, Limor and Saulnier, Jessica L. and Riegler, Clemens and Engert, Florian and Hughes, Thom and Drobizhev, Mikhail and Szabo, Balint and Ahrens, Misha B. and Flavell, Steven W. and Sabatini, Bernardo L. and Boyden, Edward S.},
  date         = {2018-02-26},
  journaltitle = {Nature Chemical Biology},
  issn         = {1552-4450, 1552-4469},
  doi          = {10/gc8zgd},
  url          = {http://www.nature.com/articles/s41589-018-0004-9},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@article{piersialaRegulatoryCellsProducing2023,
  title        = {Regulatory {{B}} Cells Producing {{IL-10}} Are Increased in Human Tumor Draining Lymph Nodes},
  author       = {Piersiala, Krzysztof and Hjalmarsson, Eric and family=Silva, given=Pedro Farrajota Neves, prefix=da, useprefix=true and Lagebro, Vilma and Kolev, Aeneas and Starkhammar, Magnus and Elliot, Alexandra and Marklund, Linda and Munck-Wikland, Eva and Margolin, Gregori and Georén, Susanna Kumlien and Cardell, Lars-Olaf},
  date         = {2023-08-15},
  journaltitle = {International Journal of Cancer},
  shortjournal = {Int J Cancer},
  volume       = {153},
  number       = {4},
  eprint       = {37144812},
  eprinttype   = {pmid},
  pages        = {854--866},
  issn         = {1097-0215},
  doi          = {10.1002/ijc.34555},
  abstract     = {The contribution of different immune cell subsets, especially T cells, in anti-tumor immune response is well established. In contrast to T cells, the anti-tumor contribution of B cells has been scarcely investigated. B-cells are often overlooked, even though they are important players in a fully integrated immune response and constitute a substantial fraction of tumor draining lymph nodes (TDLNs) known also as Sentinel Nodes. In this project, samples including TDLNs, non-TDLNs (nTDLNs) and metastatic lymph nodes from 21 patients with oral squamous cell carcinoma were analyzed by flow cytometry. TDLNs were characterized by a significantly higher proportion of B cells compared with nTDLNs (P\,=\,.0127). TDLNs-associated B cells contained high percentages of naïve B cells, in contrary to nTDLNs which contained significantly higher percentages of memory B cells. Patients having metastases in TDLNs showed a significantly higher presence of immunosuppressive B regulatory cells compared with metastasis-free patients (P\,=\,.0008). Elevated levels of regulatory B cells in TDLNs were associated with the advancement of the disease. B cells in TDLNs were characterized by significantly higher expression of an immunosuppressive cytokine-IL-10 compared with nTDLNs (P\,=\,.0077). Our data indicate that B cells in human TDLNs differ from B cells in nTDLNs and exhibit more naïve and immunosuppressive phenotypes. We identified a high accumulation of regulatory B cells within TDLNs which may be a potential obstacle in achieving response to novel cancer immunotherapies (ICIs) in head and neck cancer.},
  langid       = {english},
  keywords     = {B regulatory cells,B-Lymphocytes Regulatory,Carcinoma Squamous Cell,Humans,Interleukin-10,Lymph Nodes,Mouth Neoplasms,oral cancer,tumor-draining lymph nodes},
  file         = {/Users/jkobject/Zotero/storage/4RRI3WWW/Piersiala et al. - 2023 - Regulatory B cells producing IL-10 are increased i.pdf}
}

@article{pillowSpatiotemporalCorrelationsVisual2008,
  title        = {Spatio-Temporal Correlations and Visual Signalling in a Complete Neuronal Population},
  author       = {Pillow, Jonathan W. and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J. and Simoncelli, Eero P.},
  date         = {2008-08},
  journaltitle = {Nature},
  volume       = {454},
  number       = {7207},
  pages        = {995--999},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/dzvdm3},
  url          = {http://www.nature.com/articles/nature07140},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Dropbox/Journal Club/Pillow et al. - 2008 - Spatio-temporal correlations and visual signalling.pdf}
}

@article{pintoHighThroughputScreeningApproach2009,
  title        = {A {{High-Throughput Screening Approach}} to {{Discovering Good Forms}} of {{Biologically Inspired Visual Representation}}},
  author       = {Pinto, Nicolas and Doukhan, David and DiCarlo, James J. and Cox, David D.},
  editor       = {Friston, Karl J.},
  date         = {2009-11-26},
  journaltitle = {PLoS Computational Biology},
  volume       = {5},
  number       = {11},
  pages        = {e1000579},
  issn         = {1553-7358},
  doi          = {10/bg6jbh},
  url          = {http://dx.plos.org/10.1371/journal.pcbi.1000579},
  urldate      = {2019-03-22},
  abstract     = {While many models of biological object recognition share a common set of ‘‘broad-stroke’’ properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model—e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct ‘‘parts’’ have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3’s IBM Cell Processor). In analogy to highthroughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.},
  langid       = {english},
  annotation   = {00000}
}

@article{piranDisentanglementSinglecellData2024,
  title     = {Disentanglement of Single-Cell Data with Biolord},
  author    = {Piran, Zoe and Cohen, Niv and Hoshen, Yedid and Nitzan, Mor},
  year      = {2024},
  month     = jan,
  journal   = {Nature Biotechnology},
  pages     = {1--6},
  publisher = {Nature Publishing Group},
  issn      = {1546-1696},
  doi       = {10.1038/s41587-023-02079-x},
  urldate   = {2024-10-25},
  abstract  = {Biolord is a deep generative method for disentangling single-cell multi-omic data to known and unknown attributes, including spatial, temporal and disease states, used to reveal the decoupled biological signatures over diverse single-cell modalities and biological systems. By virtually shifting cells across states, biolord generates experimentally inaccessible samples, outperforming state-of-the-art methods in predictions of cellular response to unseen drugs and genetic perturbations. Biolord is available at https://github.com/nitzanlab/biolord.},
  copyright = {2024 The Author(s)},
  langid    = {english},
  keywords  = {Computational models,Machine learning,Software},
  file      = {/Users/jkobject/Zotero/storage/MQGUUAEF/Piran et al. - 2024 - Disentanglement of single-cell data with biolord.pdf}
}

@article{plazaLargescaleConnectomeReconstructions2014,
  title        = {Toward Large-Scale Connectome Reconstructions},
  author       = {Plaza, Stephen M and Scheffer, Louis K and Chklovskii, Dmitri B},
  date         = {2014-04},
  journaltitle = {Current Opinion in Neurobiology},
  volume       = {25},
  pages        = {201--210},
  issn         = {09594388},
  doi          = {10/gfxbdr},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S095943881400035X},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/connectionism/Plaza et al. - 2014 - Toward large-scale connectome reconstructions.pdf}
}

@article{pnevmatikakisNoRMCorreOnlineAlgorithm2017,
  title        = {{{NoRMCorre}}: {{An}} Online Algorithm for Piecewise Rigid Motion Correction of Calcium Imaging Data},
  shorttitle   = {{{NoRMCorre}}},
  author       = {Pnevmatikakis, Eftychios A and Giovannucci, Andrea},
  date         = {2017-02-17},
  journaltitle = {bioRxiv},
  doi          = {10/gfphx4},
  url          = {http://biorxiv.org/lookup/doi/10.1101/108514},
  urldate      = {2019-03-22},
  abstract     = {Background:. Motion correction is a challenging pre-processing problem that arises early in the analysis pipeline of calcium imaging data sequences. The motion artifacts in two-photon microscopy recordings can be non-rigid, arising from the finite time of raster scanning and non-uniform deformations of the brain medium.},
  langid       = {english},
  annotation   = {00000}
}

@article{pnevmatikakisSimultaneousDenoisingDeconvolution2016,
  title        = {Simultaneous {{Denoising}}, {{Deconvolution}}, and {{Demixing}} of {{Calcium Imaging Data}}},
  author       = {Pnevmatikakis, Eftychios~A. and Soudry, Daniel and Gao, Yuanjun and Machado, Timothy A. and Merel, Josh and Pfau, David and Reardon, Thomas and Mu, Yu and Lacefield, Clay and Yang, Weijian and Ahrens, Misha and Bruno, Randy and Jessell, Thomas M. and Peterka, Darcy~S. and Yuste, Rafael and Paninski, Liam},
  date         = {2016-01},
  journaltitle = {Neuron},
  volume       = {89},
  number       = {2},
  pages        = {285--299},
  issn         = {08966273},
  doi          = {10/f8g23x},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0896627315010843},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}


@article{poplinPredictionCardiovascularRisk2018,
  title        = {Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning},
  author       = {Poplin, Ryan and Varadarajan, Avinash V. and Blumer, Katy and Liu, Yun and McConnell, Michael V. and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
  date         = {2018-03},
  journaltitle = {Nature Biomedical Engineering},
  volume       = {2},
  number       = {3},
  pages        = {158--164},
  issn         = {2157-846X},
  doi          = {10/gdq7wp},
  url          = {http://www.nature.com/articles/s41551-018-0195-0},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{pottWhatAreSuperenhancers2015,
  title        = {What Are Super-Enhancers?},
  author       = {Pott, Sebastian and Lieb, Jason D},
  date         = {2015-01},
  journaltitle = {Nature Genetics},
  volume       = {47},
  number       = {1},
  pages        = {8--12},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gfxbgp},
  url          = {http://www.nature.com/articles/ng.3167},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{pratapaBenchmarkingAlgorithmsGene2020,
  title     = {Benchmarking Algorithms for Gene Regulatory Network Inference from Single-Cell Transcriptomic Data},
  author    = {Pratapa, Aditya and Jalihal, Amogh P. and Law, Jeffrey N. and Bharadwaj, Aditya and Murali, T. M.},
  year      = {2020},
  month     = feb,
  journal   = {Nature Methods},
  volume    = {17},
  number    = {2},
  pages     = {147--154},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-019-0690-6},
  urldate   = {2024-07-10},
  abstract  = {We present a systematic evaluation of state-of-the-art algorithms for inferring gene regulatory networks from single-cell transcriptional data. As the ground truth for assessing accuracy, we use synthetic networks with predictable trajectories, literature-curated Boolean models and diverse transcriptional regulatory networks. We develop a strategy to simulate single-cell transcriptional data from synthetic and Boolean networks that avoids pitfalls of previously used methods. Furthermore, we collect networks from multiple experimental single-cell RNA-seq datasets. We develop an evaluation framework called BEELINE. We find that the area under the precision-recall curve and early precision of the algorithms are moderate. The methods are better in recovering interactions in synthetic networks than Boolean models. The algorithms with the best early precision values for Boolean models also perform well on experimental datasets. Techniques that do not require pseudotime-ordered cells are generally more accurate. Based on these results, we present recommendations to end users. BEELINE will aid the development of gene regulatory network inference algorithms.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Computational biology and bioinformatics,Gene regulatory networks,Software},
  file      = {/Users/jkobject/Zotero/storage/IJZUU7TS/Pratapa et al. - 2020 - Benchmarking algorithms for gene regulatory networ.pdf}
}

@misc{predictingCellularResponsesPerturbation2025,
  title = {Predicting cellular responses to perturbation across diverse contexts with State},
  year  = {2025},
  url   = {https://www.biorxiv.org/content/10.1101/2025.06.26.661135v2}
}

@misc{PreviewDataFFPE,
  title        = {Preview {{Data}}: {{FFPE Human Skin Primary Dermal Melanoma}} with {{5K Human Pan Tissue}} and {{Pathways Panel}}},
  shorttitle   = {Preview {{Data}}},
  journal      = {10x Genomics},
  urldate      = {2025-12-05},
  howpublished = {https://www.10xgenomics.com/datasets/xenium-prime-ffpe-human-skin},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/4DZE3CA8/xenium-prime-ffpe-human-skin.html}
}

@misc{programCZCELLxGENEDiscover2023,
  title         = {{{CZ CELL}}{\texttimes}{{GENE Discover}}: {{A}} Single-Cell Data Platform for Scalable Exploration, Analysis and Modeling of Aggregated Data},
  shorttitle    = {{{CZ CELL}}{\texttimes}{{GENE Discover}}},
  author        = {Program, CZI Single-Cell Biology and Abdulla, Shibla and Aevermann, Brian and Assis, Pedro and Badajoz, Seve and Bell, Sidney M. and Bezzi, Emanuele and Cakir, Batuhan and Chaffer, Jim and Chambers, Signe and Cherry, J. Michael and Chi, Tiffany and Chien, Jennifer and Dorman, Leah and {Garcia-Nieto}, Pablo and Gloria, Nayib and Hastie, Mim and Hegeman, Daniel and Hilton, Jason and Huang, Timmy and Infeld, Amanda and Istrate, Ana-Maria and Jelic, Ivana and Katsuya, Kuni and Kim, Yang Joon and Liang, Karen and Lin, Mike and Lombardo, Maximilian and Marshall, Bailey and Martin, Bruce and McDade, Fran and Megill, Colin and Patel, Nikhil and Predeus, Alexander and Raymor, Brian and Robatmili, Behnam and Rogers, Dave and Rutherford, Erica and Sadgat, Dana and Shin, Andrew and Small, Corinn and Smith, Trent and Sridharan, Prathap and Tarashansky, Alexander and Tavares, Norbert and Thomas, Harley and Tolopko, Andrew and Urisko, Meghan and Yan, Joyce and Yeretssian, Garabet and Zamanian, Jennifer and Mani, Arathi and Cool, Jonah and Carr, Ambrose},
  year          = {2023},
  month         = nov,
  primaryclass  = {New Results},
  pages         = {2023.10.30.563174},
  publisher     = {bioRxiv},
  doi           = {10.1101/2023.10.30.563174},
  urldate       = {2024-07-15},
  abstract      = {Hundreds of millions of single cells have been analyzed to date using high throughput transcriptomic methods, thanks to technological advances driving the increasingly rapid generation of single-cell data. This provides an exciting opportunity for unlocking new insights into health and disease, made possible by meta-analysis that span diverse datasets building on recent advances in large language models and other machine learning approaches. Despite the promise of these and emerging analytical tools for analyzing large amounts of data, a major challenge remains the sheer number of datasets and inconsistent format, data models and accessibility. Many datasets are available via unique portals platforms that often lack interoperability. Here, we present CZ CellxGene Discover (cellxgene.cziscience.com), a data platform that provides curated and interoperable data. This single-cell data resource, available via a free-to-use online data portal, hosts a growing corpus of community contributed data that spans more than 50 million unique cells. Curated, standardized, and associated with consistent cell-level metadata, this collection of interoperable single-cell transcriptomic data is the largest of its kind. A suite of tools and features enables accessibility and reusability of the data via both computational and visual interfaces to allow researchers to rapidly explore individual datasets and perform cross-corpus analysis. This functionality is enabling meta-analyses of tens of millions of cells across studies and tissues and providing global views of human cells at the resolution of single cells.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/ZMW8T8WM/Program et al. - 2023 - CZ CELL×GENE Discover A single-cell data platform.pdf}
}

@misc{PyTorch16Now,
  title        = {{{PyTorch}} 1.6 Now Includes {{Stochastic Weight Averaging}}},
  journal      = {PyTorch},
  urldate      = {2024-07-21},
  abstract     = {Do you use stochastic gradient descent (SGD) or Adam? Regardless of the procedure you use to train your neural network, you can likely achieve significantly better generalization at virtually no additional cost with a simple new technique now natively supported in PyTorch 1.6, Stochastic Weight Averaging (SWA) [1]. Even if you have already trained your model, it's easy to realize the benefits of SWA by running SWA for a small number of epochs starting with a pre-trained model. Again and again, researchers are  discovering that SWA improves the performance of well-tuned models in a wide array of practical applications with little cost or effort!},
  howpublished = {https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/FDZVLLXF/pytorch-1.6-now-includes-stochastic-weight-averaging.html}
}

@article{qianEstablishmentCancerassociatedFibroblastsrelated2023,
  title        = {Establishment of Cancer-Associated Fibroblasts-Related Subtypes and Prognostic Index for Prostate Cancer through Single-Cell and Bulk {{RNA}} Transcriptome},
  author       = {Qian, Youliang and Feng, Dechao and Wang, Jie and Wei, Wuran and Wei, Qiang and Han, Ping and Yang, Lu},
  date         = {2023-06-03},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume       = {13},
  number       = {1},
  pages        = {9016},
  publisher    = {Nature Publishing Group},
  issn         = {2045-2322},
  doi          = {10.1038/s41598-023-36125-0},
  url          = {https://www.nature.com/articles/s41598-023-36125-0},
  urldate      = {2024-07-26},
  abstract     = {Current evidence indicate that cancer-associated fibroblasts (CAFs) play an important role in prostate cancer (PCa) development and progression. In this study, we identified CAF-related molecular subtypes and prognostic index for PCa patients undergoing radical prostatectomy through integrating single-cell and bulk RNA sequencing data. We completed analyses using software R 3.6.3 and its suitable packages. Through single-cell and bulk RNA sequencing analysis, NDRG2, TSPAN1, PTN, APOE, OR51E2, P4HB, STEAP1 and ABCC4 were used to construct molecular subtypes and CAF-related gene prognostic index (CRGPI). These genes could clearly divide the PCa patients into two subtypes in TCGA database and the BCR risk of subtype 1 was 13.27 times higher than that of subtype 2 with statistical significance. Similar results were observed in MSKCC2010 and GSE46602 cohorts. In addtion, the molucular subtypes were the independent risk factor of PCa patients. We orchestrated CRGPI based on the above genes and divided 430 PCa patients in TCGA database into high- and low- risk groups according to the median value of this score. We found that high-risk group had significant higher risk of BCR than low-risk group (HR: 5.45). For functional analysis, protein secretion was highly enriched in subtype 2 while snare interactions in vesicular transport was highly enriched in subtype 1. In terms of tumor heterogeneity and stemness, subtype 1 showd higher levels of TMB than subtype 2. In addition, subtype 1 had significant higher activated dendritic cell score than subtype 2. Based on eight CAF-related genes, we developed two prognostic subtypes and constructed a gene prognostic index, which could predict the prognosis of PCa patients very well.},
  langid       = {english},
  keywords     = {Cancer,Cancer microenvironment,Tumour biomarkers},
  file         = {/Users/jkobject/Zotero/storage/6R4WTA3T/Qian et al. - 2023 - Establishment of cancer-associated fibroblasts-rel.pdf}
}

@article{qinEfficientBlockcoordinateDescent2013,
  title        = {Efficient Block-Coordinate Descent Algorithms for the {{Group Lasso}}},
  author       = {Qin, Zhiwei and Scheinberg, Katya and Goldfarb, Donald},
  date         = {2013-06},
  journaltitle = {Mathematical Programming Computation},
  volume       = {5},
  number       = {2},
  pages        = {143--169},
  issn         = {1867-2949, 1867-2957},
  doi          = {10/gfvz68},
  url          = {http://link.springer.com/10.1007/s12532-013-0051-x},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/Qin et al. - 2013 - Efficient block-coordinate descent algorithms for .pdf}
}

@article{qiSystematicPredictionHuman2009,
  title        = {Systematic Prediction of Human Membrane Receptor Interactions},
  author       = {Qi, Yanjun and Dhiman, Harpreet K. and Bhola, Neil and Budyak, Ivan and Kar, Siddhartha and Man, David and Dutta, Arpana and Tirupula, Kalyan and Carr, Brian I. and Grandis, Jennifer and Bar-Joseph, Ziv and Klein-Seetharaman, Judith},
  date         = {2009-12},
  journaltitle = {PROTEOMICS},
  volume       = {9},
  number       = {23},
  pages        = {5243--5255},
  issn         = {16159853, 16159861},
  doi          = {10/brqs84},
  url          = {http://doi.wiley.com/10.1002/pmic.200900259},
  urldate      = {2019-03-22},
  abstract     = {Membrane receptor-activated signal transduction pathways are integral to cellular functions and disease mechanisms in humans. Identification of the full set of proteins interacting with membrane receptors by high throughput experimental means is difficult because methods to directly identify protein interactions are largely not applicable to membrane proteins. Unlike prior approaches that attempted to predict the global human interactome we used a computational strategy that only focused on discovering the interacting partners of human membrane receptors leading to improved results for these proteins. We predict specific interactions based on statistical integration of biological data containing highly informative direct and indirect evidences together with feedback from experts. The predicted membrane receptor interactome provides a system-wide view, and generates new biological hypotheses regarding interactions between membrane receptors and other proteins. We have experimentally validated a number of these interactions. The results suggest that a framework of systematically integrating computational predictions, global analyses, biological experimentation and expert feedback is a feasible strategy to study the human membrane receptor interactome.},
  langid       = {english},
  annotation   = {00000}
}

@article{qiuEmbracingDropoutsSinglecell2020,
  title     = {Embracing the Dropouts in Single-Cell {{RNA-seq}} Analysis},
  author    = {Qiu, Peng},
  year      = {2020},
  month     = mar,
  journal   = {Nature Communications},
  volume    = {11},
  number    = {1},
  pages     = {1169},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-020-14976-9},
  urldate   = {2024-04-19},
  abstract  = {One primary reason that makes single-cell RNA-seq analysis challenging is dropouts, where the data only captures a small fraction of the transcriptome of each cell. Almost all computational algorithms developed for single-cell RNA-seq adopted gene selection, dimension reduction or imputation to address the dropouts. Here, an opposite view is explored. Instead of treating dropouts as a problem to be fixed, we embrace it as a useful signal. We represent the dropout pattern by binarizing single-cell RNA-seq count data, and present a co-occurrence clustering algorithm to cluster cells based on the dropout pattern. We demonstrate in multiple published datasets that the binary dropout pattern is as informative as the quantitative expression of highly variable genes for the purpose of identifying cell types. We expect that recognizing the utility of dropouts provides an alternative direction for developing computational algorithms for single-cell RNA-seq analysis.},
  copyright = {2020 The Author(s)},
  langid    = {english},
  keywords  = {Biotechnology,Computational biology and bioinformatics},
  file      = {/Users/jkobject/Zotero/storage/SBKX36PM/Qiu - 2020 - Embracing the dropouts in single-cell RNA-seq anal.pdf}
}


@misc{qlora,
  title         = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle    = {{{QLoRA}}},
  author        = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year          = {2023},
  month         = may,
  number        = {arXiv:2305.14314},
  eprint        = {2305.14314},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2305.14314},
  urldate       = {2025-02-06},
  abstract      = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/9GYE3RAG/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf;/Users/jkobject/Zotero/storage/JWJSXHDV/2305.html}
}

@article{quangDanQHybridConvolutional2016,
  title        = {{{DanQ}}: A Hybrid Convolutional and Recurrent Deep Neural Network for Quantifying the Function of {{DNA}} Sequences},
  shorttitle   = {{{DanQ}}},
  author       = {Quang, Daniel and Xie, Xiaohui},
  date         = {2016-06-20},
  journaltitle = {Nucleic Acids Research},
  volume       = {44},
  number       = {11},
  pages        = {e107-e107},
  issn         = {0305-1048, 1362-4962},
  doi          = {10/f8v4wj},
  url          = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkw226},
  urldate      = {2018-04-11},
  abstract     = {Modeling the properties and functions of DNA sequences is an important, but challenging task in the broad field of genomics. This task is particularly difficult for non-coding DNA, the vast majority of which is still poorly understood in terms of function. A powerful predictive model for the function of non-coding DNA can have enormous benefit for both basic science and translational research because over 98\% of the human genome is non-coding and 93\% of disease-associated variants lie in these regions. To address this need, we propose DanQ, a novel hybrid convolutional and bi-directional long short-term memory recurrent neural network framework for predicting non-coding function de novo from sequence. In the DanQ model, the convolution layer captures regulatory motifs, while the recurrent layer captures long-term dependencies between the motifs in order to learn a regulatory ‘grammar’ to improve predictions. DanQ improves considerably upon other models across several metrics. For some regulatory markers, DanQ can achieve over a 50\% relative improvement in the area under the precision-recall curve metric compared to related models. We have made the source code available at the github repository http://github.com/uci-cbcl/DanQ.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Quang et Xie - 2016 - DanQ a hybrid convolutional and recurrent deep ne.pdf}
}

@article{raganSerialTwophotonTomography2012,
  title        = {Serial Two-Photon Tomography for Automated Ex Vivo Mouse Brain Imaging},
  author       = {Ragan, Timothy and Kadiri, Lolahon R and Venkataraju, Kannan Umadevi and Bahlmann, Karsten and Sutin, Jason and Taranda, Julian and Arganda-Carreras, Ignacio and Kim, Yongsoo and Seung, H Sebastian and Osten, Pavel},
  date         = {2012-03},
  journaltitle = {Nature Methods},
  volume       = {9},
  number       = {3},
  pages        = {255--258},
  issn         = {1548-7091, 1548-7105},
  doi          = {10/fxjm35},
  url          = {http://www.nature.com/articles/nmeth.1854},
  urldate      = {2018-04-11},
  abstract     = {Here we describe an automated method, which we call serial two-photon (STP) tomography, that achieves high-throughput fluorescence imaging of mouse brains by integrating two-photon microscopy and tissue sectioning. STP tomography generates high-resolution datasets that are free of distortions and can be readily warped in 3D, for example, for comparing multiple anatomical tracings. This method opens the door to routine systematic studies of neuroanatomy in mouse models of human brain disorders.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/imaging/Ragan et al. - 2012 - Serial two-photon tomography for automated ex vivo.pdf}
}

@article{raharinirinaInferringGeneRegulatory2021,
  title    = {Inferring Gene Regulatory Networks from Single-Cell {{RNA-seq}} Temporal Snapshot Data Requires Higher-Order Moments},
  author   = {Raharinirina, N. Alexia and Peppert, Felix and {von Kleist}, Max and Sch{\"u}tte, Christof and Sunkara, Vikram},
  year     = {2021},
  month    = sep,
  journal  = {Patterns},
  volume   = {2},
  number   = {9},
  pages    = {100332},
  issn     = {2666-3899},
  doi      = {10.1016/j.patter.2021.100332},
  urldate  = {2024-07-10},
  abstract = {Single-cell RNA sequencing (scRNA-seq) has become ubiquitous in biology. Recently, there has been a push for using scRNA-seq snapshot data to infer the underlying gene regulatory networks (GRNs) steering cellular function. To date, this aspiration remains unrealized due to technical and computational challenges. In this work we focus on the latter, which is under-represented in the literature. We took a systemic approach by subdividing the GRN inference into three fundamental components: data pre-processing, feature extraction, and inference. We observed that the regulatory signature is captured in the statistical moments of scRNA-seq data and requires computationally intensive minimization solvers to extract it. Furthermore, current data pre-processing might not conserve these statistical moments. Although our moment-based approach is a didactic tool for understanding the different compartments of GRN inference, this line of thinking---finding computationally feasible multi-dimensional statistics of data---is imperative for designing GRN inference methods.},
  keywords = {chemical master equation,Markov chains,moment equations,RNA sequencing,single cell,time-course snapshots},
  file     = {/Users/jkobject/Zotero/storage/MFHXBKUQ/Raharinirina et al. - 2021 - Inferring gene regulatory networks from single-cel.pdf;/Users/jkobject/Zotero/storage/AJFGCVHZ/S266638992100180X.html}
}

@unpublished{raikoIterativeNeuralAutoregressive2014,
  title       = {Iterative {{Neural Autoregressive Distribution Estimator}} ({{NADE-k}})},
  author      = {Raiko, Tapani and Yao, Li and Cho, Kyunghyun and Bengio, Yoshua},
  date        = {2014-06-05},
  eprint      = {1406.1485},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1406.1485},
  urldate     = {2019-03-22},
  abstract    = {Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in k steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-prediction training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{ramachandranUncoveringRobustPatterns2017,
  title        = {Uncovering Robust Patterns of {{microRNA}} Co-Expression across Cancers Using {{Bayesian Relevance Networks}}},
  author       = {Ramachandran, Parameswaran and family=Taltavull, given=Daniel Sa, prefix=nchez-, useprefix=true and Perkins, Theodore J},
  date         = {2017},
  journaltitle = {PLOS ONE},
  pages        = {25},
  langid       = {english},
  keywords     = {❓ Multiple DOI},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Ramachandran et al. - 2017 - Uncovering robust patterns of microRNA co-expressi.pdf}
}

@article{reesCorrelatingChemicalSensitivity2016,
  title        = {Correlating Chemical Sensitivity and Basal Gene Expression Reveals Mechanism of Action},
  author       = {Rees, Matthew G and Seashore-Ludlow, Brinton and Cheah, Jaime H and Adams, Drew J and Price, Edmund V and Gill, Shubhroz and Javaid, Sarah and Coletti, Matthew E and Jones, Victor L and Bodycombe, Nicole E and Soule, Christian K and Alexander, Benjamin and Li, Ava and Montgomery, Philip and Kotz, Joanne D and Hon, C Suk-Yee and Munoz, Benito and Liefeld, Ted and Dančík, Vlado and Haber, Daniel A and Clish, Clary B and Bittker, Joshua A and Palmer, Michelle and Wagner, Bridget K and Clemons, Paul A and Shamji, Alykhan F and Schreiber, Stuart L},
  date         = {2016-02},
  journaltitle = {Nature Chemical Biology},
  volume       = {12},
  number       = {2},
  pages        = {109--116},
  issn         = {1552-4450, 1552-4469},
  doi          = {10/94z},
  url          = {http://www.nature.com/articles/nchembio.1986},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{regevHumanCellAtlas,
  title    = {The {{Human Cell Atlas}}},
  author   = {Regev, Aviv and Teichmann, Sarah A and Lander, Eric S and Amit, Ido and Benoist, Christophe and Birney, Ewan and Bodenmiller, Bernd and Campbell, Peter and Carninci, Piero and Clatworthy, Menna and Clevers, Hans and Deplancke, Bart and Dunham, Ian and Eberwine, James and Eils, Roland and Enard, Wolfgang and Farmer, Andrew and Fugger, Lars and G{\"o}ttgens, Berthold and Hacohen, Nir and Haniffa, Muzlifah and Hemberg, Martin and Kim, Seung and Klenerman, Paul and Kriegstein, Arnold and Lein, Ed and Linnarsson, Sten and Lundberg, Emma and Lundeberg, Joakim and Majumder, Partha and Marioni, John C and Merad, Miriam and Mhlanga, Musa and Nawijn, Martijn and Netea, Mihai and Nolan, Garry and Pe'er, Dana and Phillipakis, Anthony and Ponting, Chris P and Quake, Stephen and Reik, Wolf and {Rozenblatt-Rosen}, Orit and Sanes, Joshua and Satija, Rahul and Schumacher, Ton N and Shalek, Alex and Shapiro, Ehud and Sharma, Padmanee and Shin, Jay W and Stegle, Oliver and Stratton, Michael and Stubbington, Michael J T and Theis, Fabian J and Uhlen, Matthias and {van Oudenaarden}, Alexander and Wagner, Allon and Watt, Fiona and Weissman, Jonathan and Wold, Barbara and Xavier, Ramnik and Yosef, Nir},
  journal  = {eLife},
  volume   = {6},
  pages    = {e27041},
  issn     = {2050-084X},
  doi      = {10.7554/eLife.27041},
  urldate  = {2025-02-25},
  abstract = {The recent advent of methods for high-throughput single-cell molecular profiling has catalyzed a growing sense in the scientific community that the time is ripe to complete the 150-year-old effort to identify all cell types in the human body. The Human Cell Atlas Project is an international collaborative effort that aims to define all human cell types in terms of distinctive molecular profiles (such as gene expression profiles) and to connect this information with classical cellular descriptions (such as location and morphology). An open comprehensive reference map of the molecular state of cells in healthy human tissues would propel the systematic study of physiological states, developmental trajectories, regulatory circuitry and interactions of cells, and also provide a framework for understanding cellular dysregulation in human disease. Here we describe the idea, its potential utility, early proofs-of-concept, and some design considerations for the Human Cell Atlas, including a commitment to open data, code, and community.},
  pmcid    = {PMC5762154},
  pmid     = {29206104},
  file     = {/Users/jkobject/Zotero/storage/VJDYXCCE/Regev et al. - The Human Cell Atlas.pdf}
}

@article{replogleMappingInformationrichGenotypephenotype2022,
  title    = {Mapping Information-Rich Genotype-Phenotype Landscapes with Genome-Scale {{Perturb-seq}}},
  author   = {Replogle, Joseph M. and Saunders, Reuben A. and Pogson, Angela N. and Hussmann, Jeffrey A. and Lenail, Alexander and Guna, Alina and Mascibroda, Lauren and Wagner, Eric J. and Adelman, Karen and {Lithwick-Yanai}, Gila and Iremadze, Nika and Oberstrass, Florian and Lipson, Doron and Bonnar, Jessica L. and Jost, Marco and Norman, Thomas M. and Weissman, Jonathan S.},
  year     = {2022},
  month    = jul,
  journal  = {Cell},
  volume   = {185},
  number   = {14},
  pages    = {2559-2575.e28},
  issn     = {1097-4172},
  doi      = {10.1016/j.cell.2022.05.013},
  abstract = {A central goal of genetics is to define the relationships between genotypes and phenotypes. High-content phenotypic screens such as Perturb-seq (CRISPR-based screens with single-cell RNA-sequencing readouts) enable massively parallel functional genomic mapping but, to date, have been used at limited scales. Here, we perform genome-scale Perturb-seq targeting all expressed genes with CRISPR interference (CRISPRi) across {$>$}2.5 million human cells. We use transcriptional phenotypes to predict the function of poorly characterized genes, uncovering new regulators of ribosome biogenesis (including CCDC86, ZNF236, and SPATA5L1), transcription (C7orf26), and mitochondrial respiration (TMEM242). In addition to assigning gene function, single-cell transcriptional phenotypes allow for in-depth dissection of complex cellular phenomena-from RNA processing to differentiation. We leverage this ability to systematically identify genetic drivers and consequences of aneuploidy and to discover an unanticipated layer of stress-specific regulation of the mitochondrial genome. Our information-rich genotype-phenotype map reveals a multidimensional portrait of gene and cellular function.},
  langid   = {english},
  pmcid    = {PMC9380471},
  pmid     = {35688146},
  keywords = {cell biology,chromosomal instability,Chromosome Mapping,CRISPR,CRISPR-Cas Systems,genetic screens,Genomics,Genotype,genotype-phenotype map,Integrator complex,mitochondrial genome stress response,Perturb-seq,Phenotype,Single-Cell Analysis,single-cell RNA sequencing},
  file     = {/Users/jkobject/Zotero/storage/J6XBVGYS/Replogle et al. - 2022 - Mapping information-rich genotype-phenotype landsc.pdf}
}

@online{reusabilityReportExploringSpatial2025,
  title   = {Reusability report: Exploring the transferability of self-supervised learning models from single-cell to spatial transcriptomics},
  journal = {Nature Machine Intelligence},
  year    = {2025},
  url     = {https://www.nature.com/articles/s42256-025-01097-5}
}

@misc{rhodes2025orbv3atomisticsimulationscale,
  title         = {Orb-v3: atomistic simulation at scale},
  author        = {Benjamin Rhodes and Sander Vandenhaute and Vaidotas Šimkus and James Gin and Jonathan Godwin and Tim Duignan and Mark Neumann},
  year          = {2025},
  eprint        = {2504.06231},
  archiveprefix = {arXiv},
  primaryclass  = {cond-mat.mtrl-sci},
  url           = {https://arxiv.org/abs/2504.06231}
}

@article{richardsonAtlasPolygenicRisk2018,
  title        = {An Atlas of Polygenic Risk Score Associations to Highlight Putative Causal Relationships across the Human Phenome},
  author       = {Richardson, Tom G and Harrison, Sean and Hemani, Gibran and Davey Smith, George},
  date         = {2018-11-11},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbfs},
  url          = {http://biorxiv.org/lookup/doi/10.1101/467910},
  urldate      = {2019-03-22},
  abstract     = {The age of large-scale genome-wide association studies (GWAS) has provided us with an unprecedented opportunity to evaluate the genetic liability of complex disease using polygenic risk scores (PRS). In this study, we have analysed 162 PRS (P\&lt;5x10-05) derived from GWAS and 551 heritable traits from the UK Biobank study (N=334,398). Findings can be investigated using a web application (http://mrcieu.mrsoftware.org/PRS\_atlas/), which we envisage will help uncover both known and novel mechanisms which contribute towards disease susceptibility. To demonstrate this, we have investigated the results from a phenome-wide evaluation of schizophrenia genetic liability. Amongst findings were inverse associations with measures of cognitive function which extensive follow-up analyses using Mendelian randomization (MR) provided evidence of a causal relationship. We have also investigated the effect of multiple risk factors on disease using mediation and multivariable MR frameworks. Our atlas provides a resource for future endeavours seeking to unravel the causal determinants of complex disease.},
  langid       = {english},
  annotation   = {00000}
}

@article{richardsonStatisticalMethodsIntegrative2016,
  title        = {Statistical {{Methods}} in {{Integrative Genomics}}},
  author       = {Richardson, Sylvia and Tseng, George C. and Sun, Wei},
  date         = {2016-06},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume       = {3},
  number       = {1},
  pages        = {181--209},
  issn         = {2326-8298, 2326-831X},
  doi          = {10/gfkrz7},
  url          = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-041715-033506},
  urldate      = {2019-03-22},
  abstract     = {Statistical methods in integrative genomics aim to answer important biology questions by jointly analyzing multiple types of genomic data (vertical integration) or aggregating the same type of data across multiple studies (horizontal integration). In this article, we introduce different types of genomic data and data resources, and then we review statistical methods of integrative genomics with emphasis on the motivation and rationale of these methods. We conclude with some summary points and future research directions.},
  langid       = {english}
}

@unpublished{riedmillerLearningPlayingSolving2018,
  title       = {Learning by {{Playing}} - {{Solving Sparse Reward Tasks}} from {{Scratch}}},
  author      = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Van de Wiele, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
  date        = {2018-02-28},
  eprint      = {1802.10567},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1802.10567},
  urldate     = {2019-03-22},
  abstract    = {We propose Scheduled Auxiliary Control (SACX), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors – from scratch – in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment – enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach. A video of the rich set of learned behaviours can be found at https://youtu.be/mPKyvocNe M.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  annotation  = {00000}
}

@article{risiEnhancedHypercubeBasedEncoding2012,
  title        = {An {{Enhanced Hypercube-Based Encoding}} for {{Evolving}} the {{Placement}}, {{Density}}, and {{Connectivity}} of {{Neurons}}},
  author       = {Risi, Sebastian and Stanley, Kenneth O.},
  date         = {2012-10},
  journaltitle = {Artificial Life},
  volume       = {18},
  number       = {4},
  pages        = {331--363},
  issn         = {1064-5462, 1530-9185},
  doi          = {10/f4dgpc},
  url          = {http://www.mitpressjournals.org/doi/10.1162/ARTL_a_00071},
  urldate      = {2018-04-11},
  abstract     = {Intelligence in nature is the product of living brains, which are themselves the product of natural evolution. Although researchers in the field of neuroevolution (NE) attempt to recapitulate this process, artificial neural networks (ANNs) so far evolved through NE algorithms do not match the distinctive capabilities of biological brains. The recently-introduced Hypercubebased NeuroEvolution of Augmenting Topologies (HyperNEAT) approach narrowed this gap by demonstrating that the pattern of weights across the connectivity of an ANN can be generated as a function of its geometry, thereby allowing large ANNs to be evolved for high-dimensional problems. Yet the positions and number of the neurons connected through this approach must be decided a priori by the user and, unlike in living brains, cannot change during evolution. Evolvable-substrate HyperNEAT (ES-HyperNEAT), introduced in this paper, addresses this limitation by automatically deducing node geometry based on implicit information in the pattern of weights encoded by HyperNEAT, thereby avoiding the need to evolve explicit placement. This approach not only can evolve the location of every neuron in the network, but also can represent regions of varying density, which means resolution can increase holistically over evolution. ES-HyperNEAT is demonstrated through multi-task, maze navigation and modular retina domains, revealing that the ANNs generated by this new approach assume natural properties such as neural topography and geometric regularity. Also importantly, ES-HyperNEAT’s compact indirect encoding can be seeded to begin with a bias towards a desired class of ANN topographies, which facilitates the evolutionary search. The main conclusion is that ES-HyperNEAT significantly expands the scope of neural structures that evolution can discover.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/NE/Risi et Stanley - 2012 - An Enhanced Hypercube-Based Encoding for Evolving .pdf}
}

@article{rivesBiologicalStructureFunction2021,
  title     = {Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences},
  author    = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
  year      = {2021},
  month     = apr,
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {118},
  number    = {15},
  pages     = {e2016239118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi       = {10.1073/pnas.2016239118},
  urldate   = {2025-02-25},
  abstract  = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
  file      = {/Users/jkobject/Zotero/storage/P7LWZL6T/Rives et al. - 2021 - Biological structure and function emerge from scal.pdf}
}

@article{RNAWorld2024,
  title     = {The {{RNA}} World},
  year      = 2024,
  month     = may,
  journal   = {Nature Structural \& Molecular Biology},
  volume    = {31},
  number    = {5},
  pages     = {729--729},
  publisher = {Nature Publishing Group},
  issn      = {1545-9985},
  doi       = {10.1038/s41594-024-01327-1},
  urldate   = {2025-12-02},
  abstract  = {This issue of Nature Structural \& Molecular Biology presents studies investigating RNA processing, including mechanisms of splicing, biogenesis of the splicing machinery, decoding of mRNA by the ribosome, and deadenylation of mRNA for degradation. We are also delighted to be publishing News \& Views and Comment pieces that reflect on these exciting advances in the field.},
  copyright = {2024 Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Biochemistry,Biological Microscopy,general,Life Sciences,Membrane Biology,Protein Structure},
  file      = {/Users/jkobject/Zotero/storage/VCVPIPYN/2024 - The RNA world.pdf}
}

@article{robbinsRemarkStirlingsFormula1955,
  title      = {A {{Remark}} on {{Stirling}}'s {{Formula}}},
  author     = {Robbins, Herbert},
  year       = {1955},
  journal    = {The American Mathematical Monthly},
  volume     = {62},
  number     = {1},
  eprint     = {2308012},
  eprinttype = {jstor},
  pages      = {26--29},
  publisher  = {[Taylor \& Francis, Ltd., Mathematical Association of America]},
  issn       = {0002-9890},
  doi        = {10.2307/2308012},
  urldate    = {2024-07-19},
  file       = {/Users/jkobject/Zotero/storage/FKRA7X29/Robbins - 1955 - A Remark on Stirling's Formula.pdf}
}

@online{ROLEBIOMARKERMACROPHAGE,
  title   = {{{THE ROLE OF BIOMARKER MACROPHAGE MIGRATION INHIBITORY FACTOR IN CARDIAC REMODELING PREDICTION IN PATIENTS WITH ST-SEGMENT ELEVATION MYOCARDIAL INFARCTION}} - {{PubMed}}},
  url     = {https://pubmed.ncbi.nlm.nih.gov/37326070/},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/MJRBJYIJ/37326070.html}
}

@unpublished{ronnebergerUNetConvolutionalNetworks2015,
  title       = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle  = {U-{{Net}}},
  author      = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date        = {2015-05-18},
  eprint      = {1505.04597},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1505.04597},
  urldate     = {2019-03-22},
  abstract    = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation  = {00000}
}

@article{roohaniPredictingTranscriptionalOutcomes2024,
  title     = {Predicting Transcriptional Outcomes of Novel Multigene Perturbations with {{GEARS}}},
  author    = {Roohani, Yusuf and Huang, Kexin and Leskovec, Jure},
  year      = {2024},
  month     = jun,
  journal   = {Nature Biotechnology},
  volume    = {42},
  number    = {6},
  pages     = {927--935},
  publisher = {Nature Publishing Group},
  issn      = {1546-1696},
  doi       = {10.1038/s41587-023-01905-6},
  urldate   = {2025-02-25},
  abstract  = {Understanding cellular responses to genetic perturbation is central to numerous biomedical applications, from identifying genetic interactions involved in cancer to developing methods for regenerative medicine. However, the combinatorial explosion in the number of possible multigene perturbations severely limits experimental interrogation. Here, we present graph-enhanced gene activation and repression simulator (GEARS), a method that integrates deep learning with a knowledge graph of gene--gene relationships to predict transcriptional responses to both single and multigene perturbations using single-cell RNA-sequencing data from perturbational screens. GEARS is able to predict outcomes of perturbing combinations consisting of genes that were never experimentally perturbed. GEARS exhibited 40\% higher precision than existing approaches in predicting four distinct genetic interaction subtypes in a combinatorial perturbation screen and identified the strongest interactions twice as well as prior approaches. Overall, GEARS can predict phenotypically distinct effects of multigene perturbations and thus guide the design of perturbational experiments.},
  copyright = {2023 The Author(s)},
  langid    = {english},
  keywords  = {Gene expression profiling,Gene regulatory networks,Genomic engineering},
  file      = {/Users/jkobject/Zotero/storage/ZGH24GM6/Roohani et al. - 2024 - Predicting transcriptional outcomes of novel multi.pdf}
}

@misc{rosenUniversalCellEmbeddings2023,
  title         = {Universal {{Cell Embeddings}}: {{A Foundation Model}} for {{Cell Biology}}},
  shorttitle    = {Universal {{Cell Embeddings}}},
  author        = {Rosen, Yanay and Roohani, Yusuf and Agarwal, Ayush and Samotor{\v c}an, Leon and Consortium, Tabula Sapiens and Quake, Stephen R. and Leskovec, Jure},
  year          = {2023},
  month         = nov,
  primaryclass  = {New Results},
  pages         = {2023.11.28.568918},
  publisher     = {bioRxiv},
  doi           = {10.1101/2023.11.28.568918},
  urldate       = {2024-04-18},
  abstract      = {Developing a universal representation of cells which encompasses the tremendous molecular diversity of cell types within the human body and more generally, across species, would be transformative for cell biology. Recent work using single-cell transcriptomic approaches to create molecular definitions of cell types in the form of cell atlases has provided the necessary data for such an endeavor. Here, we present the Universal Cell Embedding (UCE) foundation model. UCE was trained on a corpus of cell atlas data from human and other species in a completely self-supervised way without any data annotations. UCE offers a unified biological latent space that can represent any cell, regardless of tissue or species. This universal cell embedding captures important biological variation despite the presence of experimental noise across diverse datasets. An important aspect of UCE's universality is that any new cell from any organism can be mapped to this embedding space with no additional data labeling, model training or fine-tuning. We applied UCE to create the Integrated Mega-scale Atlas, embedding 36 million cells, with more than 1,000 uniquely named cell types, from hundreds of experiments, dozens of tissues and eight species. We uncovered new insights about the organization of cell types and tissues within this universal cell embedding space, and leveraged it to infer function of newly discovered cell types. UCE's embedding space exhibits emergent behavior, uncovering new biology that it was never explicitly trained for, such as identifying developmental lineages and embedding data from novel species not included in the training set. Overall, by enabling a universal representation for every cell state and type, UCE provides a valuable tool for analysis, annotation and hypothesis generation as the scale and diversity of single cell datasets continues to grow.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/M2PHDH5I/Rosen et al. - 2023 - Universal Cell Embeddings A Foundation Model for .pdf}
}

@article{rossLargescaleChemicalLanguage2022,
  title     = {Large-Scale Chemical Language Representations Capture Molecular Structure and Properties},
  author    = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and Mroueh, Youssef and Das, Payel},
  year      = {2022},
  month     = dec,
  journal   = {Nature Machine Intelligence},
  volume    = {4},
  number    = {12},
  pages     = {1256--1264},
  publisher = {Nature Publishing Group},
  issn      = {2522-5839},
  doi       = {10.1038/s42256-022-00580-7},
  urldate   = {2025-02-25},
  abstract  = {Models based on machine learning can enable accurate and fast molecular property predictions, which is of interest in drug discovery and material design. Various supervised machine learning models have demonstrated promising performance, but the vast chemical space and the limited availability of property labels make supervised learning challenging. Recently, unsupervised transformer-based language models pretrained on a large unlabelled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, we present molecular embeddings obtained by training an efficient transformer encoder model, MoLFormer, which uses rotary positional embeddings. This model employs a linear attention mechanism, coupled with highly distributed training, on SMILES sequences of 1.1 billion unlabelled molecules from the PubChem and ZINC datasets. We show that the learned molecular representation outperforms existing baselines, including supervised and self-supervised graph neural networks and language models, on several downstream tasks from ten benchmark datasets. They perform competitively on two others. Further analyses, specifically through the lens of attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns the spatial relationships between atoms within a molecule. These results provide encouraging evidence that large-scale molecular language models can capture sufficient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid    = {english},
  keywords  = {Computational methods,Computer science,Method development},
  file      = {/Users/jkobject/Zotero/storage/4J7LQT8T/Ross et al. - 2022 - Large-scale chemical language representations capt.pdf}
}

@online{S100A6MolecularFunction,
  title   = {{{S100A6}}: Molecular Function and Biomarker Role | {{Biomarker Research}} | {{Full Text}}},
  url     = {https://biomarkerres.biomedcentral.com/articles/10.1186/s40364-023-00515-3},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/C229MWVG/s40364-023-00515-3.html}
}

@unpublished{sabourDynamicRoutingCapsules2017,
  title       = {Dynamic {{Routing Between Capsules}}},
  author      = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  date        = {2017-10-26},
  eprint      = {1710.09829},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1710.09829},
  urldate     = {2019-03-22},
  abstract    = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation  = {00000}
}

@article{saint-andreModelsHumanCore2016,
  title        = {Models of Human Core Transcriptional Regulatory Circuitries},
  author       = {Saint-André, Violaine and Federation, Alexander J. and Lin, Charles Y. and Abraham, Brian J. and Reddy, Jessica and Lee, Tong Ihn and Bradner, James E. and Young, Richard A.},
  date         = {2016-03},
  journaltitle = {Genome Research},
  volume       = {26},
  number       = {3},
  pages        = {385--396},
  issn         = {1088-9051, 1549-5469},
  doi          = {10/f8cpjr},
  url          = {http://genome.cshlp.org/lookup/doi/10.1101/gr.197590.115},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{salakhutdinovLearningDeepGenerative2015,
  title        = {Learning {{Deep Generative Models}}},
  author       = {Salakhutdinov, Ruslan},
  date         = {2015-04-10},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume       = {2},
  number       = {1},
  pages        = {361--385},
  issn         = {2326-8298, 2326-831X},
  doi          = {10.1146/annurev-statistics-010814-020120},
  url          = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-010814-020120},
  urldate      = {2018-04-11},
  abstract     = {Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many artificial intelligence–related tasks, including object recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires models with deep architectures that involve many layers of nonlinear processing. In this article, we review several popular deep learning models, including deep belief networks and deep Boltzmann machines. We show that (a) these deep generative models, which contain many layers of latent variables and millions of parameters, can be learned efficiently, and (b) the learned high-level feature representations can be successfully applied in many application domains, including visual object recognition, information retrieval, classification, and regression tasks.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/unsupervised/Salakhutdinov - 2015 - Learning Deep Generative Models.pdf}
}

@article{salmenHighthroughputTotalRNA2022,
  title     = {High-Throughput Total {{RNA}} Sequencing in Single Cells Using {{VASA-seq}}},
  author    = {Salmen, Fredrik and De Jonghe, Joachim and Kaminski, Tomasz S. and Alemany, Anna and Parada, Guillermo E. and {Verity-Legg}, Joe and Yanagida, Ayaka and Kohler, Timo N. and Battich, Nicholas and {van den Brekel}, Floris and Ellermann, Anna L. and Arias, Alfonso Martinez and Nichols, Jennifer and Hemberg, Martin and Hollfelder, Florian and {van Oudenaarden}, Alexander},
  year      = 2022,
  month     = dec,
  journal   = {Nature Biotechnology},
  volume    = {40},
  number    = {12},
  pages     = {1780--1793},
  publisher = {Nature Publishing Group},
  issn      = {1546-1696},
  doi       = {10.1038/s41587-022-01361-8},
  urldate   = {2025-12-13},
  abstract  = {Most methods for single-cell transcriptome sequencing amplify the termini of polyadenylated transcripts, capturing only a small fraction of the total cellular transcriptome. This precludes the detection of many long non-coding, short non-coding and non-polyadenylated protein-coding transcripts and hinders alternative splicing analysis. We, therefore, developed VASA-seq to detect the total transcriptome in single cells, which is enabled by fragmenting and tailing all RNA molecules subsequent to cell lysis. The method is compatible with both plate-based formats and droplet microfluidics. We applied VASA-seq to more than 30,000 single cells in the developing mouse embryo during gastrulation and early organogenesis. Analyzing the dynamics of the total single-cell transcriptome, we discovered cell type markers, many based on non-coding RNA, and performed in vivo cell cycle analysis via detection of non-polyadenylated histone genes. RNA velocity characterization was improved, accurately retracing blood maturation trajectories. Moreover, our VASA-seq data provide a comprehensive analysis of alternative splicing during mammalian development, which highlighted substantial rearrangements during blood development and heart morphogenesis.},
  copyright = {2022 The Author(s)},
  langid    = {english},
  keywords  = {Databases,Gastrulation,Non-coding RNAs,RNA splicing,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/ZHKNA2HL/Salmen et al. - 2022 - High-throughput total RNA sequencing in single cells using VASA-seq.pdf}
}

@article{mercatelliGeneRegulatoryNetwork2020,
  title = {Gene Regulatory Network Inference Resources: {{A}} Practical Overview},
  shorttitle = {Gene Regulatory Network Inference Resources},
  author = {Mercatelli, Daniele and Scalambra, Laura and Triboli, Luca and Ray, Forest and Giorgi, Federico M.},
  year = 2020,
  month = jun,
  journal = {Biochimica Et Biophysica Acta. Gene Regulatory Mechanisms},
  volume = {1863},
  number = {6},
  pages = {194430},
  issn = {1876-4320},
  doi = {10.1016/j.bbagrm.2019.194430},
  abstract = {Transcriptional regulation is a fundamental molecular mechanism involved in almost every aspect of life, from homeostasis to development, from metabolism to behavior, from reaction to stimuli to disease progression. In recent years, the concept of Gene Regulatory Networks (GRNs) has grown popular as an effective applied biology approach for describing the complex and highly dynamic set of transcriptional interactions, due to its easy-to-interpret features. Since cataloguing, predicting and understanding every GRN connection in all species and cellular contexts remains a great challenge for biology, researchers have developed numerous tools and methods to infer regulatory processes. In this review, we catalogue these methods in six major areas, based on the dominant underlying information leveraged to infer GRNs: Coexpression, Sequence Motifs, Chromatin Immunoprecipitation (ChIP), Orthology, Literature and Protein-Protein Interaction (PPI) specifically focused on transcriptional complexes. The methods described here cover a wide range of user-friendliness: from web tools that require no prior computational expertise to command line programs and algorithms for large scale GRN inferences. Each method for GRN inference described herein effectively illustrates a type of transcriptional relationship, with many methods being complementary to others. While a truly holistic approach for inferring and displaying GRNs remains one of the greatest challenges in the field of systems biology, we believe that the integration of multiple methods described herein provides an effective means with which experimental and computational biologists alike may obtain the most complete pictures of transcriptional relationships. This article is part of a Special Issue entitled: Transcriptional Profiles and Regulatory Gene Networks edited by Dr. Federico Manuel Giorgi and Dr. Shaun Mahony.},
  langid = {english},
  pmid = {31678629},
  keywords = {Binding Sites,Chromatin Immunoprecipitation,Databases Nucleic Acid,Gene Regulatory Networks,Humans,Nucleotide Motifs,Protein Interaction Mapping,Sequence Analysis DNA,Software,Transcription Factors}
}


@article{sandaCoreTranscriptionalRegulatory2012,
  title        = {Core {{Transcriptional Regulatory Circuit Controlled}} by the {{TAL1 Complex}} in {{Human T Cell Acute Lymphoblastic Leukemia}}},
  author       = {Sanda, Takaomi and Lawton, Lee~N. and Barrasa, M.~Inmaculada and Fan, Zi~Peng and Kohlhammer, Holger and Gutierrez, Alejandro and Ma, Wenxue and Tatarek, Jessica and Ahn, Yebin and Kelliher, Michelle~A. and Jamieson, Catriona~H.M. and Staudt, Louis~M. and Young, Richard~A. and Look, A.~Thomas},
  date         = {2012-08},
  journaltitle = {Cancer Cell},
  volume       = {22},
  number       = {2},
  pages        = {209--221},
  issn         = {15356108},
  doi          = {10.1016/j.ccr.2012.06.007},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1535610812002565},
  urldate      = {2019-03-22},
  abstract     = {The oncogenic transcription factor TAL1/SCL is aberrantly expressed in over 40\% of cases of human T cell acute lymphoblastic leukemia (T-ALL), emphasizing its importance in the molecular pathogenesis of T-ALL. Here we identify the core transcriptional regulatory circuit controlled by TAL1 and its regulatory partners HEB, E2A, LMO1/2, GATA3, and RUNX1. We show that TAL1 forms a positive interconnected autoregulatory loop with GATA3 and RUNX1 and that the TAL1 complex directly activates the MYB oncogene, forming a positive feed-forward regulatory loop that reinforces and stabilizes the TAL1-regulated oncogenic program. One of the critical downstream targets in this circuitry is the TRIB2 gene, which is oppositely regulated by TAL1 and E2A/HEB and is essential for the survival of T-ALL cells.},
  langid       = {english},
  annotation   = {00000}
}


@article{sangerDNASequencingChainterminating1977,
  title    = {{{DNA}} Sequencing with Chain-Terminating Inhibitors},
  author   = {Sanger, F. and Nicklen, S. and Coulson, A. R.},
  year     = 1977,
  month    = dec,
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  volume   = {74},
  number   = {12},
  pages    = {5463--5467},
  issn     = {0027-8424},
  doi      = {10.1073/pnas.74.12.5463},
  abstract = {A new method for determining nucleotide sequences in DNA is described. It is similar to the "plus and minus" method [Sanger, F. \& Coulson, A. R. (1975) J. Mol. Biol. 94, 441-448] but makes use of the 2',3'-dideoxy and arabinonucleoside analogues of the normal deoxynucleoside triphosphates, which act as specific chain-terminating inhibitors of DNA polymerase. The technique has been applied to the DNA of bacteriophage varphiX174 and is more rapid and more accurate than either the plus or the minus method.},
  langid   = {english},
  pmcid    = {PMC431765},
  pmid     = {271968},
  keywords = {Base Sequence,Coliphages,Deoxyribonucleotides,DNA Polymerase I,DNA Restriction Enzymes,DNA Viral,Methods},
  file     = {/Users/jkobject/Zotero/storage/7XP4QL29/Sanger et al. - 1977 - DNA sequencing with chain-terminating inhibitors.pdf}
}


@article{saudiImmuneActivatedCellsAre2023,
  title    = {Immune-{{Activated B Cells Are Dominant}} in {{Prostate Cancer}}},
  author   = {Saudi, Aws and Banday, Viqar and Zirakzadeh, A. Ali and Selinger, Martin and Forsberg, Jon and Holmbom, Martin and Henriksson, Johan and Wald{\'e}n, Mauritz and Alamdari, Farhood and Aljabery, Firas and Winqvist, Ola and Sherif, Amir},
  year     = {2023},
  month    = feb,
  journal  = {Cancers},
  volume   = {15},
  number   = {3},
  pages    = {920},
  issn     = {2072-6694},
  doi      = {10.3390/cancers15030920},
  urldate  = {2024-07-23},
  abstract = {Simple Summary Contrary to the common belief that prostate cancer is an immune desert, our study shows tumor-associated B-cell responses in prostate cancer. We demonstrate mature and activated phenotypes of B cells with an increased frequency of effector plasmablasts in tumor-draining sentinel lymph nodes. These findings indicate a B-cell-specific antitumor immune response, emphasizing the importance of further trials targeting B cells in prostate cancer immunotherapy. Abstract B cells are multifaceted immune cells responding robustly during immune surveillance against tumor antigens by presentation to T cells and switched immunoglobulin production. However, B cells are unstudied in prostate cancer (PCa). We used flow cytometry to analyze B-cell subpopulations in peripheral blood and lymph nodes from intermediate--high risk PCa patients. B-cell subpopulations were related to clinicopathological factors. B-cell-receptor single-cell sequencing and VDJ analysis identified clonal B-cell expansion in blood and lymph nodes. Pathological staging was pT2 in 16\%, pT3a in 48\%, and pT3b in 36\%. Lymph node metastases occurred in 5/25 patients (20\%). Compared to healthy donors, the peripheral blood CD19+ B-cell compartment was significantly decreased in PCa patients and dominated by na{\"i}ve B cells. The nodal B-cell compartment had significantly increased fractions of CD19+ B cells and switched memory B cells. Plasmablasts were observed in tumor-draining sentinel lymph nodes (SNs). VDJ analysis revealed clonal expansion in lymph nodes. Thus, activated B cells are increased in SNs from PCa patients. The increased fraction of switched memory cells and plasmablasts together with the presence of clonally expanded B cells indicate tumor-specific T-cell-dependent responses from B cells, supporting an important role for B cells in the protection against tumors.},
  pmcid    = {PMC9913271},
  pmid     = {36765877},
  file     = {/Users/jkobject/Zotero/storage/72V8SRPI/Saudi et al. - 2023 - Immune-Activated B Cells Are Dominant in Prostate .pdf}
}


@article{saxtonMTORSignalingGrowth2017,
  title        = {{{mTOR Signaling}} in {{Growth}}, {{Metabolism}}, and {{Disease}}},
  author       = {Saxton, Robert A. and Sabatini, David M.},
  date         = {2017-03},
  journaltitle = {Cell},
  volume       = {168},
  number       = {6},
  pages        = {960--976},
  issn         = {00928674},
  doi          = {10.1016/j.cell.2017.02.004},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867417301824},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/GRHE4UMQ/Saxton and Sabatini - 2017 - mTOR Signaling in Growth, Metabolism, and Disease.pdf}
}



@article{scarches,
  title     = {Mapping Single-Cell Data to Reference Atlases by Transfer Learning},
  author    = {Lotfollahi, Mohammad and Naghipourfar, Mohsen and Luecken, Malte D. and Khajavi, Matin and B{\"u}ttner, Maren and Wagenstetter, Marco and Avsec, {\v Z}iga and Gayoso, Adam and Yosef, Nir and Interlandi, Marta and Rybakov, Sergei and Misharin, Alexander V. and Theis, Fabian J.},
  year      = {2022},
  month     = jan,
  journal   = {Nature Biotechnology},
  volume    = {40},
  number    = {1},
  pages     = {121--130},
  publisher = {Nature Publishing Group},
  issn      = {1546-1696},
  doi       = {10.1038/s41587-021-01001-7},
  urldate   = {2025-02-06},
  abstract  = {Large single-cell atlases are now routinely generated to serve as references for analysis of smaller-scale studies. Yet learning from reference data is complicated by batch effects between datasets, limited availability of computational resources and sharing restrictions on raw data. Here we introduce a deep learning strategy for mapping query datasets on top of a reference called single-cell architectural surgery (scArches). scArches uses transfer learning and parameter optimization to enable efficient, decentralized, iterative reference building and contextualization of new datasets with existing references without sharing raw data. Using examples from mouse brain, pancreas, immune and whole-organism atlases, we show that scArches preserves biological state information while removing batch effects, despite using four orders of magnitude fewer parameters than de novo integration. scArches generalizes to multimodal reference mapping, allowing imputation of missing modalities. Finally, scArches retains coronavirus disease 2019 (COVID-19) disease variation when mapping to a healthy reference, enabling the discovery of disease-specific cell states. scArches will facilitate collaborative projects by enabling iterative construction, updating, sharing and efficient use of reference atlases.},
  copyright = {2021 The Author(s)},
  langid    = {english},
  keywords  = {Data integration,Machine learning},
  file      = {/Users/jkobject/Zotero/storage/8496H6S2/Lotfollahi et al. - 2022 - Mapping single-cell data to reference atlases by t.pdf}
}

@article{schafferMultimodalCellMaps2025,
  title   = {Multimodal cell maps as a foundation for structural and functional genomics},
  author  = {Schaffer, Leah V. and others},
  journal = {Nature},
  volume  = {642},
  pages   = {222--231},
  year    = {2025},
  doi     = {10.1038/s41586-025-08623-y}
}


@misc{schockaertEmbeddingsEpistemicStates2023,
  title         = {Embeddings as {{Epistemic States}}: {{Limitations}} on the {{Use}} of {{Pooling Operators}} for {{Accumulating Knowledge}}},
  shorttitle    = {Embeddings as {{Epistemic States}}},
  author        = {Schockaert, Steven},
  year          = {2023},
  month         = jul,
  number        = {arXiv:2210.05723},
  eprint        = {2210.05723},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2210.05723},
  urldate       = {2025-03-27},
  abstract      = {Various neural network architectures rely on pooling operators to aggregate information coming from different sources. It is often implicitly assumed in such contexts that vectors encode epistemic states, i.e. that vectors capture the evidence that has been obtained about some properties of interest, and that pooling these vectors yields a vector that combines this evidence. We study, for a number of standard pooling operators, under what conditions they are compatible with this idea, which we call the epistemic pooling principle. While we find that all the considered pooling operators can satisfy the epistemic pooling principle, this only holds when embeddings are sufficiently high-dimensional and, for most pooling operators, when the embeddings satisfy particular constraints (e.g. having non-negative coordinates). We furthermore show that these constraints have important implications on how the embeddings can be used in practice. In particular, we find that when the epistemic pooling principle is satisfied, in most cases it is impossible to verify the satisfaction of propositional formulas using linear scoring functions, with two exceptions: (i) max-pooling with embeddings that are upper-bounded and (ii) Hadamard pooling with non-negative embeddings. This finding helps to clarify, among others, why Graph Neural Networks sometimes under-perform in reasoning tasks. Finally, we also study an extension of the epistemic pooling principle to weighted epistemic states, which are important in the context of non-monotonic reasoning, where max-pooling emerges as the most suitable operator.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/97M4DB87/Schockaert - 2023 - Embeddings as Epistemic States Limitations on the.pdf;/Users/jkobject/Zotero/storage/WYFTQNVB/2210.html}
}


@article{schreiberMultiscaleDeepTensor2018,
  title        = {Multi-Scale Deep Tensor Factorization Learns a Latent Representation of the Human Epigenome},
  author       = {Schreiber, Jacob and Durham, Timothy J and Bilmes, Jeffrey and Noble, William Stafford},
  date         = {2018-07-08},
  journaltitle = {bioRxiv},
  doi          = {10.1101/364976},
  url          = {http://biorxiv.org/lookup/doi/10.1101/364976},
  urldate      = {2019-03-22},
  abstract     = {The human epigenome has been experimentally characterized by measurements of protein binding, chromatin acessibility, methylation, and histone modification in hundreds of cell types. The result is a huge compendium of data, consisting of thousands of measurements for every basepair in the human genome. These data are difficult to make sense of, not only for humans, but also for computational methods that aim to detect genes and other functional elements, predict gene expression, characterize polymorphisms, etc. To address this challenge, we propose a deep neural network tensor factorization method, Avocado, that compresses epigenomic data into a dense, information-rich representation of the human genome. We use data from the Roadmap Epigenomics Consortium to demonstrate that this learned representation of the genome is broadly useful: first, by imputing epigenomic data more accurately than previous methods, and second, by showing that machine learning models that exploit this representation outperform those trained directly on epigenomic data on a variety of genomics tasks. These tasks include predicting gene expression, promoter-enhancer interactions, and elements of 3D chromatin architecture. Our findings suggest the broad utility of Avocado’s learned latent representation for computational genomics and epigenomics.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/EASL4J58/Schreiber et al. - 2018 - Multi-scale deep tensor factorization learns a lat.pdf}
}

@article{schultzNeuralSubstratePrediction1997,
  title        = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author       = {Schultz, W. and Dayan, P. and Montague, P. R.},
  date         = {1997-03-14},
  journaltitle = {Science},
  volume       = {275},
  number       = {5306},
  pages        = {1593--1599},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/bh8sst},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.275.5306.1593},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Dropbox/Journal Club/Schultz et al. - 1997 - A Neural Substrate of Prediction and Reward.pdf}
}


@article{schwartzNaturalSignalStatistics2001,
  title        = {Natural Signal Statistics and Sensory Gain Control},
  author       = {Schwartz, Odelia and Simoncelli, Eero P.},
  date         = {2001-08},
  journaltitle = {Nature Neuroscience},
  volume       = {4},
  number       = {8},
  pages        = {819--825},
  issn         = {1097-6256, 1546-1726},
  doi          = {10/bxn9b2},
  url          = {http://www.nature.com/articles/nn0801_819},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Dropbox/Journal Club/Schwartz et Simoncelli - 2001 - Natural signal statistics and sensory gain control.pdf}
}

@misc{scprint,
  title         = {{{scPRINT}}: Pre-Training on 50 Million Cells Allows Robust Gene Network Predictions},
  shorttitle    = {{{scPRINT}}},
  author        = {Kalfon, J{\'e}r{\'e}mie and Samaran, Jules and Peyr{\'e}, Gabriel and Cantini, Laura},
  year          = {2024},
  month         = jul,
  primaryclass  = {New Results},
  pages         = {2024.07.29.605556},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.07.29.605556},
  urldate       = {2025-02-06},
  abstract      = {A cell is governed by the interaction of myriads of macromolecules. Such a network of interaction has remained an elusive milestone in cellular biology. Building on recent advances in large foundation models and their ability to learn without supervision, we present scPRINT, a large cell model for the inference of gene networks pre-trained on more than 50M cells from the cellxgene database. Using novel pretraining methods and model architecture, scPRINT pushes large transformer models towards more interpretability and usability in uncovering the complex biology of the cell. Based on our atlas-level benchmarks, scPRINT demonstrates superior performance in gene network inference to the state of the art, as well as competitive zero-shot abilities in denoising, batch effect correction, and cell label prediction. On an atlas of benign prostatic hyperplasia, scPRINT highlights the profound connections between ion exchange, senescence, and chronic inflammation.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/59D4A3J8/Kalfon et al. - 2024 - scPRINT pre-training on 50 million cells allows r.pdf}
}

@article{scvi,
  title     = {Deep Generative Modeling for Single-Cell Transcriptomics},
  author    = {Lopez, Romain and Regier, Jeffrey and Cole, Michael B. and Jordan, Michael I. and Yosef, Nir},
  year      = {2018},
  month     = dec,
  journal   = {Nature Methods},
  volume    = {15},
  number    = {12},
  pages     = {1053--1058},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-018-0229-2},
  urldate   = {2024-07-15},
  abstract  = {Single-cell transcriptome measurements can reveal unexplored biological diversity, but they suffer from technical noise and bias that must be modeled to account for the resulting uncertainty in downstream analyses. Here we introduce single-cell variational inference (scVI), a ready-to-use scalable framework for the probabilistic representation and analysis of gene expression in single cells (https://github.com/YosefLab/scVI). scVI uses stochastic optimization and deep neural networks to aggregate information across similar cells and genes and to approximate the distributions that underlie observed expression values, while accounting for batch effects and limited sensitivity. We used scVI for a range of fundamental analysis tasks including batch correction, visualization, clustering, and differential expression, and achieved high accuracy for each task.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Computational biology and bioinformatics,Computational models},
  file      = {/Users/jkobject/Zotero/storage/VTAFJS9N/Lopez et al. - 2018 - Deep generative modeling for single-cell transcrip.pdf}
}

@unpublished{seglerGeneratingFocussedMolecule2017,
  title       = {Generating {{Focussed Molecule Libraries}} for {{Drug Discovery}} with {{Recurrent Neural Networks}}},
  author      = {Segler, Marwin H. S. and Kogej, Thierry and Tyrchan, Christian and Waller, Mark P.},
  date        = {2017-01-05},
  eprint      = {1701.01329},
  eprinttype  = {arXiv},
  eprintclass = {physics, stat},
  url         = {http://arxiv.org/abs/1701.01329},
  urldate     = {2019-03-22},
  abstract    = {In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14\% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28\% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Physics - Chemical Physics,Statistics - Machine Learning},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/YQVCFRU8/Segler et al. - 2017 - Generating Focussed Molecule Libraries for Drug Di.pdf}
}

@inproceedings{sejnowskiPredictiveHebbianLearning1995,
  title     = {Predictive {{Hebbian}} Learning},
  author    = {Sejnowski, Terrence J. and Dayan, Peter and Montague, P. Read},
  date      = {1995},
  pages     = {15--18},
  publisher = {ACM Press},
  doi       = {10.1145/225298.225300},
  url       = {http://portal.acm.org/citation.cfm?doid=225298.225300},
  urldate   = {2018-04-11},
  isbn      = {978-0-89791-723-0},
  langid    = {english},
  file      = {/Users/jeremie/Dropbox/Journal Club/Sejnowski et al. - 1995 - Predictive Hebbian learning.pdf}
}

@online{SelenoproteinDeficiencyAlters,
  title   = {Selenoprotein {{T}} Deficiency Alters Cell Adhesion and Elevates Selenoprotein {{W}} Expression in Murine Fibroblast Cells - {{PMC}}},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3471091/},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/NWXKHDRY/PMC3471091.html}
}

@article{senguptaGoingDeeperSpiking,
  title      = {Going {{Deeper}} in {{Spiking Neural Networks}}: {{VGG}} and {{Residual Architectures}}},
  author     = {Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu, Chiao and Roy, Kaushik},
  pages      = {10},
  abstract   = {Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00011},
  file       = {/Users/jeremie/Documents/science/ML/NN/spiking/Sengupta et al. - Going Deeper in Spiking Neural Networks VGG and R.pdf}
}

@article{senguptaManifoldtilingLocalizedReceptive2018,
  title        = {Manifold-Tiling {{Localized Receptive Fields}} Are {{Optimal}} in {{Similarity-preserving Neural Networks}}:},
  shorttitle   = {Manifold-Tiling {{Localized Receptive Fields}} Are {{Optimal}} in {{Similarity-preserving Neural Networks}}},
  author       = {Sengupta, Anirvan and Tepper, Mariano and Pehlevan, Cengiz and Genkin, Alexander and Chklovskii, Dmitri},
  date         = {2018-12-02},
  journaltitle = {bioRxiv},
  doi          = {10/gfwjwf},
  url          = {http://biorxiv.org/lookup/doi/10.1101/338947},
  urldate      = {2019-03-22},
  abstract     = {Many neurons in the brain, such as place cells in the rodent hippocampus, have localized receptive fields, i.e., they respond to a small neighborhood of stimulus space. What is the functional significance of such representations and how can they arise? Here, we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs. Numerical simulations of such networks on standard datasets yield manifold-tiling localized receptive fields. More generally, we show analytically that, for data lying on symmetric manifolds, optimal solutions of objectives, from which similarity-preserving networks are derived, have localized receptive fields. Therefore, nonnegative similarity-preserving mapping (NSM) implemented by neural networks can model representations of continuous manifolds in the brain.},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/9TWRCVLQ/Sengupta et al. - 2018 - Manifold-tiling Localized Receptive Fields are Opt.pdf}
}

@article{shahExplainingComplexCodon2011,
  title        = {Explaining Complex Codon Usage Patterns with Selection for Translational Efficiency, Mutation Bias, and Genetic Drift},
  author       = {Shah, P. and Gilchrist, M. A.},
  date         = {2011-06-21},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume       = {108},
  number       = {25},
  pages        = {10231--10236},
  issn         = {0027-8424, 1091-6490},
  doi          = {10/d6cn32},
  url          = {http://www.pnas.org/cgi/doi/10.1073/pnas.1016719108},
  urldate      = {2019-03-22},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/7PFZC98Q/Shah and Gilchrist - 2011 - Explaining complex codon usage patterns with selec.pdf}
}

@article{shalemGenomeScaleCRISPRCas9Knockout2014,
  title        = {Genome-{{Scale CRISPR-Cas9 Knockout Screening}} in {{Human Cells}}},
  author       = {Shalem, Ophir and Sanjana, Neville E. and Hartenian, Ella and Shi, Xi and Scott, David A. and Mikkelsen, Tarjei S. and Heckl, Dirk and Ebert, Benjamin L. and Root, David E. and Doench, John G. and Zhang, Feng},
  date         = {2014-01-03},
  journaltitle = {Science},
  volume       = {343},
  number       = {6166},
  pages        = {84--87},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/qhn},
  url          = {http://www.sciencemag.org/lookup/doi/10.1126/science.1247005},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{shaoATARiSComputationalQuantification2013,
  title        = {{{ATARiS}}: {{Computational}} Quantification of Gene Suppression Phenotypes from Multisample {{RNAi}} Screens},
  shorttitle   = {{{ATARiS}}},
  author       = {Shao, D. D. and Tsherniak, A. and Gopal, S. and Weir, B. A. and Tamayo, P. and Stransky, N. and Schumacher, S. E. and Zack, T. I. and Beroukhim, R. and Garraway, L. A. and Margolin, A. A. and Root, D. E. and Hahn, W. C. and Mesirov, J. P.},
  date         = {2013-04-01},
  journaltitle = {Genome Research},
  volume       = {23},
  number       = {4},
  pages        = {665--678},
  issn         = {1088-9051},
  doi          = {10/f4s4pb},
  url          = {http://genome.cshlp.org/cgi/doi/10.1101/gr.143586.112},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/BC79KS4K/Shao et al. - 2013 - ATARiS Computational quantification of gene suppr.pdf}
}

@misc{shawSelfAttentionRelativePosition2018,
  title         = {Self-{{Attention}} with {{Relative Position Representations}}},
  author        = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  year          = 2018,
  month         = apr,
  number        = {arXiv:1803.02155},
  eprint        = {1803.02155},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1803.02155},
  urldate       = {2025-12-12},
  abstract      = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/FZQEJSHQ/Shaw et al. - 2018 - Self-Attention with Relative Position Representations.pdf;/Users/jkobject/Zotero/storage/KEVSHYEZ/1803.html}
}

@article{shenSensitiveTumourDetection2018,
  title        = {Sensitive Tumour Detection and Classification Using Plasma Cell-Free {{DNA}} Methylomes},
  author       = {Shen, Shu Yi and Singhania, Rajat and Fehringer, Gordon and Chakravarthy, Ankur and Roehrl, Michael H. A. and Chadwick, Dianne and Zuzarte, Philip C. and Borgida, Ayelet and Wang, Ting Ting and Li, Tiantian and Kis, Olena and Zhao, Zhen and Spreafico, Anna and Medina, Tiago da Silva and Wang, Yadon and Roulois, David and Ettayebi, Ilias and Chen, Zhuo and Chow, Signy and Murphy, Tracy and Arruda, Andrea and O’Kane, Grainne M. and Liu, Jessica and Mansour, Mark and McPherson, John D. and O’Brien, Catherine and Leighl, Natasha and Bedard, Philippe L. and Fleshner, Neil and Liu, Geoffrey and Minden, Mark D. and Gallinger, Steven and Goldenberg, Anna and Pugh, Trevor J. and Hoffman, Michael M. and Bratman, Scott V. and Hung, Rayjean J. and De Carvalho, Daniel D.},
  date         = {2018-11},
  journaltitle = {Nature},
  volume       = {563},
  number       = {7732},
  pages        = {579--583},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gfjzjh},
  url          = {http://www.nature.com/articles/s41586-018-0703-0},
  urldate      = {2019-03-22},
  langid       = {english}
}

@misc{shimLayerwisePruningTransformer2021,
  title         = {Layer-Wise {{Pruning}} of {{Transformer Attention Heads}} for {{Efficient Language Modeling}}},
  author        = {Shim, Kyuhong and Choi, Iksoo and Sung, Wonyong and Choi, Jungwook},
  year          = {2021},
  month         = oct,
  number        = {arXiv:2110.03252},
  eprint        = {2110.03252},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2110.03252},
  urldate       = {2024-07-23},
  abstract      = {While Transformer-based models have shown impressive language modeling performance, the large computation cost is often prohibitive for practical use. Attention head pruning, which removes unnecessary attention heads in the multihead attention, is a promising technique to solve this problem. However, it does not evenly reduce the overall load because the heavy feedforward module is not affected by head pruning. In this paper, we apply layer-wise attention head pruning on All-attention Transformer so that the entire computation and the number of parameters can be reduced proportionally to the number of pruned heads. While the architecture has the potential to fully utilize head pruning, we propose three training methods that are especially helpful to minimize performance degradation and stabilize the pruning process. Our pruned model shows consistently lower perplexity within a comparable parameter size than Transformer-XL on WikiText-103 language modeling benchmark.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/2YQ68IX7/Shim et al. - 2021 - Layer-wise Pruning of Transformer Attention Heads .pdf;/Users/jkobject/Zotero/storage/5LLNXD2I/2110.html}
}

@article{shiponyLongrangeSinglemoleculeMapping2018,
  title        = {Long-Range Single-Molecule Mapping of Chromatin Accessibility in Eukaryotes: {{Supplemental Tables}} and {{Figures}}},
  shorttitle   = {Long-Range Single-Molecule Mapping of Chromatin Accessibility in Eukaryotes},
  author       = {Shipony, Zohar and Marinov, Georgi K and Swaffer, Matthew P and Sinott-Armstrong, Nasa A and Skotheim, Jan M and Kundaje, Anshul and Greenleaf, William J},
  date         = {2018-12-22},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbgk},
  url          = {http://biorxiv.org/lookup/doi/10.1101/504662},
  urldate      = {2019-03-22},
  abstract     = {Active regulatory elements in eukaryotes are typically characterized by an open, nucleosomedepleted chromatin structure; mapping areas of open chromatin has accordingly emerged as a widely used tool in the arsenal of modern functional genomics. However, existing approaches for profiling chromatin accessibility are limited by their reliance on DNA fragmentation and short read sequencing, which leaves them unable to provide information about the state of chromatin on larger scales or reveal coordination between the chromatin state of individual distal regulatory elements. To address these limitations, we have developed a method for profiling accessibility of individual chromatin fibers at multi-kilobase length scale (SMAC-seq, or Single-Molecule long-read Accessible Chromatin mapping sequencing assay), enabling the simultaneous, highresolution, single-molecule assessment of the chromatin state of distal genomic elements. Our strategy is based on combining the preferential methylation of open chromatin regions by DNA methyltransferases (CpG and GpC 5-methylcytosine (5mC) and N6-methyladenosine (m6A) enzymes) and the ability of long-read single-molecule nanopore sequencing to directly read out the methylation state of individual DNA bases. Applying SMAC-seq to the budding yeast Saccharomyces cerevisiae, we demonstrate that aggregate SMAC-seq signals match bulk-level accessibility measurements, observe single-molecule protection footprints of nucleosomes and transcription factors, and quantify the correlation between the chromatin states of distal genomic elements.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/FE3HBGU3/Shipony et al. - 2018 - Long-range single-molecule mapping of chromatin ac.pdf}
}

@article{shresthaRobustSpiketrainLearning2017,
  title        = {Robust Spike-Train Learning in Spike-Event Based Weight Update},
  author       = {Shrestha, Sumit Bam and Song, Qing},
  date         = {2017-12},
  journaltitle = {Neural Networks},
  volume       = {96},
  pages        = {33--46},
  issn         = {08936080},
  doi          = {10/gcgv8p},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S0893608017302009},
  urldate      = {2018-04-11},
  abstract     = {Supervised learning algorithms in a spiking neural network either learn a spike-train pattern for a single neuron receiving input spike-train from multiple input synapses or learn to output the first spike time in a feedforward network setting. In this paper, we build upon spike-event based weight update strategy to learn continuous spike-train in a spiking neural network with a hidden layer using a dead zone on–off based adaptive learning rate rule which ensures convergence of the learning process in the sense of weight convergence and robustness of the learning process to external disturbances. Based on different benchmark problems, we compare this new method with other relevant spike-train learning algorithms. The results show that the speed of learning is much improved and the rate of successful learning is also greatly improved.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/spiking/Shrestha et Song - 2017 - Robust spike-train learning in spike-event based w.pdf}
}

@article{shrikumarGkmexplainFastAccurate2018,
  title        = {Gkmexplain: {{Fast}} and {{Accurate Interpretation}} of {{Nonlinear Gapped}} k-Mer {{Support Vector Machines Using Integrated Gradients}}},
  shorttitle   = {Gkmexplain},
  author       = {Shrikumar, Avanti and Prakash, Eva and Kundaje, Anshul},
  date         = {2018-11-06},
  journaltitle = {bioRxiv},
  doi          = {10.1101/457606},
  url          = {http://biorxiv.org/lookup/doi/10.1101/457606},
  urldate      = {2019-03-22},
  abstract     = {Support Vector Machines with gapped k-mer kernels (gkm-SVMs) have been used to learn predictive models of regulatory DNA sequence. However, interpreting predictive sequence patterns learned by gkm-SVMs can be challenging. Existing interpretation methods such as deltaSVM, in-silico mutagenesis (ISM), or SHAP either do not scale well or make limiting assumptions about the model that can produce misleading results when the gkm kernel is combined with nonlinear kernels. Here, we propose gkmexplain: a novel approach inspired by the method of Integrated Gradients for interpreting gkm-SVM models. Using simulated regulatory DNA sequences, we show that gkmexplain identifies predictive patterns with high accuracy while avoiding pitfalls of deltaSVM and ISM and being orders of magnitude more computationally efficient than SHAP. We use a novel motif discovery method called TF-MoDISco to recover consolidated TF motifs from gkm-SVM models of in vivo TF binding by aggregating predictive patterns identified by gkmexplain. Finally, we find that mutation impact scores derived through gkmexplain using gkm-SVM models of chromatin accessibility in lymphoblastoid cell-lines consistently outperform deltaSVM and ISM at identifying regulatory genetic variants (dsQTLs). Code and example notebooks replicating the workflow are available at https://github.com/kundajelab/gkmexplain. Explanatory videos available at http://bit.ly/gkmexplainvids.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/A624H396/Shrikumar et al. - 2018 - Gkmexplain Fast and Accurate Interpretation of No.pdf}
}

@article{shuModelingGeneRegulatory2021,
  title     = {Modeling Gene Regulatory Networks Using Neural Network Architectures},
  author    = {Shu, Hantao and Zhou, Jingtian and Lian, Qiuyu and Li, Han and Zhao, Dan and Zeng, Jianyang and Ma, Jianzhu},
  year      = {2021},
  month     = jul,
  journal   = {Nature Computational Science},
  volume    = {1},
  number    = {7},
  pages     = {491--501},
  publisher = {Nature Publishing Group},
  issn      = {2662-8457},
  doi       = {10.1038/s43588-021-00099-8},
  urldate   = {2024-11-13},
  abstract  = {Gene regulatory networks (GRNs) encode the complex molecular interactions that govern cell identity. Here we propose DeepSEM, a deep generative model that can jointly infer GRNs and biologically meaningful representation of single-cell RNA sequencing (scRNA-seq) data. In particular, we developed a neural network version of the structural equation model (SEM) to explicitly model the regulatory relationships among genes. Benchmark results show that DeepSEM achieves comparable or better performance on a variety of single-cell computational tasks, such as GRN inference, scRNA-seq data visualization, clustering and simulation, compared with the state-of-the-art methods. In addition, the gene regulations predicted by DeepSEM on cell-type marker genes in the mouse cortex can be validated by epigenetic data, which further demonstrates the accuracy and efficiency of our method. DeepSEM can provide a useful and powerful tool to analyze scRNA-seq data and infer a GRN.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Computational biology and bioinformatics,Gene regulatory networks,Machine learning},
  file      = {/Users/jkobject/Zotero/storage/6SVP9RBY/Shu et al. - 2021 - Modeling gene regulatory networks using neural net.pdf}
}

@article{siFoundationModelsMolecular2024,
  title    = {Foundation Models in Molecular Biology},
  author   = {Si, Yunda and Zou, Jiawei and Gao, Yicheng and Chuai, Guohui and Liu, Qi and Chen, Luonan},
  year     = {2024},
  month    = jun,
  journal  = {Biophysics Reports},
  volume   = {10},
  number   = {3},
  pages    = {135--151},
  issn     = {2364-3439},
  doi      = {10.52601/bpr.2024.240006},
  urldate  = {2025-02-25},
  abstract = {Determining correlations between molecules at various levels is an important topic in molecular biology. Large language models have demonstrated a remarkable ability to capture correlations from large amounts of data in the field of natural language processing as well as image generation, and correlations captured from data using large language models can also be applicable to solving a wide range of specific tasks, hence large language models are also referred to as foundation models. The massive amount of data that exists in the field of molecular biology provides an excellent basis for the development of foundation models, and the recent emergence of foundation models in the field of molecular biology has really pushed the entire field forward. We summarize the foundation models developed based on RNA sequence data, DNA sequence data, protein sequence data, single-cell transcriptome data, and spatial transcriptome data respectively, and further discuss the research directions for the development of foundation models in molecular biology.},
  pmcid    = {PMC11252241},
  pmid     = {39027316},
  file     = {/Users/jkobject/Zotero/storage/IJS7CMKG/Si et al. - 2024 - Foundation models in molecular biology.pdf}
}

@article{sikkemaIntegratedCellAtlas2023,
  title     = {An Integrated Cell Atlas of the Lung in Health and Disease},
  author    = {Sikkema, Lisa and {Ram{\'i}rez-Su{\'a}stegui}, Ciro and Strobl, Daniel C. and Gillett, Tessa E. and Zappia, Luke and Madissoon, Elo and Markov, Nikolay S. and Zaragosi, Laure-Emmanuelle and Ji, Yuge and Ansari, Meshal and Arguel, Marie-Jeanne and Apperloo, Leonie and Banchero, Martin and B{\'e}cavin, Christophe and Berg, Marijn and Chichelnitskiy, Evgeny and Chung, Mei-i and Collin, Antoine and Gay, Aurore C. A. and {Gote-Schniering}, Janine and Hooshiar Kashani, Baharak and Inecik, Kemal and Jain, Manu and Kapellos, Theodore S. and Kole, Tessa M. and Leroy, Sylvie and Mayr, Christoph H. and Oliver, Amanda J. and {von Papen}, Michael and Peter, Lance and Taylor, Chase J. and Walzthoeni, Thomas and Xu, Chuan and Bui, Linh T. and De Donno, Carlo and Dony, Leander and Faiz, Alen and Guo, Minzhe and Gutierrez, Austin J. and Heumos, Lukas and Huang, Ni and Ibarra, Ignacio L. and Jackson, Nathan D. and Kadur Lakshminarasimha Murthy, Preetish and Lotfollahi, Mohammad and Tabib, Tracy and {Talavera-L{\'o}pez}, Carlos and Travaglini, Kyle J. and {Wilbrey-Clark}, Anna and Worlock, Kaylee B. and Yoshida, Masahiro and {van den Berge}, Maarten and Boss{\'e}, Yohan and Desai, Tushar J. and Eickelberg, Oliver and Kaminski, Naftali and Krasnow, Mark A. and Lafyatis, Robert and Nikolic, Marko Z. and Powell, Joseph E. and Rajagopal, Jayaraj and Rojas, Mauricio and {Rozenblatt-Rosen}, Orit and Seibold, Max A. and Sheppard, Dean and Shepherd, Douglas P. and Sin, Don D. and Timens, Wim and Tsankov, Alexander M. and Whitsett, Jeffrey and Xu, Yan and Banovich, Nicholas E. and Barbry, Pascal and Duong, Thu Elizabeth and Falk, Christine S. and Meyer, Kerstin B. and Kropski, Jonathan A. and Pe'er, Dana and Schiller, Herbert B. and Tata, Purushothama Rao and Schultze, Joachim L. and Teichmann, Sara A. and Misharin, Alexander V. and Nawijn, Martijn C. and Luecken, Malte D. and Theis, Fabian J.},
  year      = {2023},
  month     = jun,
  journal   = {Nature Medicine},
  volume    = {29},
  number    = {6},
  pages     = {1563--1577},
  publisher = {Nature Publishing Group},
  issn      = {1546-170X},
  doi       = {10.1038/s41591-023-02327-2},
  urldate   = {2024-07-15},
  abstract  = {Single-cell technologies have transformed our understanding of human tissues. Yet, studies typically capture only a limited number of donors and disagree on cell type definitions. Integrating many single-cell datasets can address these limitations of individual studies and capture the variability present in the population. Here we present the integrated Human Lung Cell Atlas (HLCA), combining 49 datasets of the human respiratory system into a single atlas spanning over 2.4\,million cells from 486 individuals. The HLCA presents a consensus cell type re-annotation with matching marker genes, including annotations of rare and previously undescribed cell types. Leveraging the number and diversity of individuals in the HLCA, we identify gene modules that are associated with demographic covariates such as age, sex and body mass index, as well as gene modules changing expression along the proximal-to-distal axis of the bronchial tree. Mapping new data to the HLCA enables rapid data annotation and interpretation. Using the HLCA as a reference for the study of disease, we identify shared cell states across multiple lung diseases, including SPP1+ profibrotic monocyte-derived macrophages in COVID-19, pulmonary fibrosis and lung carcinoma. Overall, the HLCA serves as an example for the development and use of large-scale, cross-dataset organ atlases within the Human Cell Atlas.},
  copyright = {2023 The Author(s)},
  langid    = {english},
  keywords  = {Cell biology,Computational models,Data integration,Mechanisms of disease,Transcriptomics},
  file      = {/Users/jkobject/Zotero/storage/YBE9LXDP/Sikkema et al. - 2023 - An integrated cell atlas of the lung in health and.pdf}
}

@article{silverMasteringGameGo2016,
  title        = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author       = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date         = {2016-01},
  journaltitle = {Nature},
  volume       = {529},
  number       = {7587},
  pages        = {484--489},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/f77tw6},
  url          = {http://www.nature.com/articles/nature16961},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{silverMasteringGameGo2017,
  title        = {Mastering the Game of {{Go}} without Human Knowledge},
  author       = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Graepel, Thore and Hassabis, Demis},
  date         = {2017-10-18},
  journaltitle = {Nature},
  volume       = {550},
  number       = {7676},
  pages        = {354--359},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/gcsmk9},
  url          = {http://www.nature.com/doifinder/10.1038/nature24270},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@article{simoncelliModelNeuronalResponses,
  title  = {A {{Model}} of {{Neuronal Responses}} in {{Visual Area MT}}},
  author = {SIMONCELLI, EERO P and HEEGERt, DAVID J},
  pages  = {19},
  langid = {english},
  file   = {/Users/jeremie/Dropbox/Journal Club/SIMONCELLI et HEEGERt - A Model of Neuronal Responses in Visual Area MT.pdf}
}

@misc{simonyanVeryDeepConvolutional2015,
  title         = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author        = {Simonyan, Karen and Zisserman, Andrew},
  year          = 2015,
  month         = apr,
  number        = {arXiv:1409.1556},
  eprint        = {1409.1556},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1409.1556},
  urldate       = {2025-12-07},
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {/Users/jkobject/Zotero/storage/68D5ZNKA/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/Users/jkobject/Zotero/storage/288FJ8AR/1409.html}
}

@article{smetAdvantagesLimitationsCurrent2010,
  title        = {Advantages and Limitations of Current Network Inference Methods},
  author       = {Smet, Riet De and Marchal, Kathleen},
  date         = {2010-10},
  journaltitle = {Nature Reviews Microbiology},
  volume       = {8},
  number       = {10},
  pages        = {717--729},
  issn         = {1740-1526, 1740-1534},
  doi          = {10/fhj88k},
  url          = {http://www.nature.com/articles/nrmicro2419},
  urldate      = {2018-04-11},
  abstract     = {Network inference, which is the reconstruction of biological networks from high-throughput data, can provide valuable information about the regulation of gene expression in cells. However, it is an underdetermined problem, as the number of interactions that can be inferred exceeds the number of independent measurements. Different state-of-the-art tools for network inference use specific assumptions and simplifications to deal with underdetermination, and these influence the inferences. The outcome of network inference therefore varies between tools and can be highly complementary. Here we categorize the available tools according to the strategies that they use to deal with the problem of underdetermination. Such categorization allows an insight into why a certain tool is more appropriate for the specific research question or data set at hand.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/De Smet et Marchal - 2010 - Advantages and limitations of current network infe.pdf}
}

@unpublished{smithOptimizationTechniquesRiemannian2014,
  title       = {Optimization {{Techniques}} on {{Riemannian Manifolds}}},
  author      = {Smith, Steven Thomas},
  date        = {2014-07-22},
  eprint      = {1407.5965},
  eprinttype  = {arXiv},
  eprintclass = {cs, math},
  url         = {http://arxiv.org/abs/1407.5965},
  urldate     = {2019-03-22},
  abstract    = {The techniques and analysis presented in this paper provide new methods to solve optimization problems posed on Riemannian manifolds. A new point of view is offered for the solution of constrained optimization problems. Some classical optimization techniques on Euclidean space are generalized to Riemannian manifolds. Several algorithms are presented and their convergence properties are analyzed employing the Riemannian structure of the manifold. Specifically, two apparently new algorithms, which can be thought of as Newton’s method and the conjugate gradient method on Riemannian manifolds, are presented and shown to possess, respectively, quadratic and superlinear convergence. Examples of each method on certain Riemannian manifolds are given with the results of numerical experiments. Rayleigh’s quotient defined on the sphere is one example. It is shown that Newton’s method applied to this function converges cubically, and that the Rayleigh quotient iteration is an efficient approximation of Newton’s method. The Riemannian version of the conjugate gradient method applied to this function gives a new algorithm for finding the eigenvectors corresponding to the extreme eigenvalues of a symmetric matrix. Another example arises from extremizing the function tr ΘTQΘN on the special orthogonal group. In a similar example, it is shown that Newton’s method applied to the sum of the squares of the off-diagonal entries of a symmetric matrix converges cubically.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computational Geometry,Computer Science - Numerical Analysis,Mathematics - Differential Geometry,Mathematics - Dynamical Systems,Mathematics - Optimization and Control},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/6GZATZC3/Smith - 2014 - Optimization Techniques on Riemannian Manifolds.pdf}
}

@article{socherBetterWordRepresentations,
  title      = {Better {{Word Representations}} with {{Recursive Neural Networks}} for {{Morphology}}},
  author     = {Socher, Minh-Thang Luong Richard and Manning, Christopher D},
  pages      = {10},
  abstract   = {Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{socherZeroShotLearningCrossModal,
  title    = {Zero-{{Shot Learning Through Cross-Modal Transfer}}},
  author   = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
  pages    = {10},
  abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high.},
  langid   = {english},
  keywords = {⛔ No DOI found}
}

@online{songAIDrivenDigitalOrganism2024,
  title       = {Toward {{AI-Driven Digital Organism}}: {{Multiscale Foundation Models}} for {{Predicting}}, {{Simulating}} and {{Programming Biology}} at {{All Levels}}},
  shorttitle  = {Toward {{AI-Driven Digital Organism}}},
  author      = {Song, Le and Segal, Eran and Xing, Eric},
  date        = {2024-12-09},
  eprint      = {2412.06993},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2412.06993},
  url         = {http://arxiv.org/abs/2412.06993},
  urldate     = {2025-05-16},
  abstract    = {We present an approach of using AI to model and simulate biology and life. Why is it important? Because at the core of medicine, pharmacy, public health, longevity, agriculture and food security, environmental protection, and clean energy, it is biology at work. Biology in the physical world is too complex to manipulate and always expensive and risky to tamper with. In this perspective, we layout an engineering viable approach to address this challenge by constructing an AI-Driven Digital Organism (AIDO), a system of integrated multiscale foundation models, in a modular, connectable, and holistic fashion to reflect biological scales, connectedness, and complexities. An AIDO opens up a safe, affordable and high-throughput alternative platform for predicting, simulating and programming biology at all levels from molecules to cells to individuals. We envision that an AIDO is poised to trigger a new wave of better-guided wet-lab experimentation and better-informed first-principle reasoning, which can eventually help us better decode and improve life.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file        = {/Users/jkobject/Zotero/storage/VB7Z7QG7/Song et al. - 2024 - Toward AI-Driven Digital Organism Multiscale Foun.pdf;/Users/jkobject/Zotero/storage/FMSBFQNM/2412.html}
}

@article{srinivasanCurrentProgressNetwork2007,
  title        = {Current Progress in Network Research: Toward Reference Networks for Key Model Organisms},
  shorttitle   = {Current Progress in Network Research},
  author       = {Srinivasan, B. S. and Shah, N. H. and Flannick, J. A. and Abeliuk, E. and Novak, A. F. and Batzoglou, S.},
  date         = {2007-06-20},
  journaltitle = {Briefings in Bioinformatics},
  volume       = {8},
  number       = {5},
  pages        = {318--332},
  issn         = {1467-5463, 1477-4054},
  doi          = {10/fr7htt},
  url          = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbm038},
  urldate      = {2018-04-11},
  abstract     = {The collection of multiple genome-scale datasets is now routine, and the frontier of research in systems biology has shifted accordingly. Rather than clustering a single dataset to produce a static map of functional modules, the focus today is on data integration, network alignment, interactive visualization and ontological markup. Because of the intrinsic noisiness of high-throughput measurements, statistical methods have been central to this effort. In this review, we briefly survey available datasets in functional genomics, review methods for data integration and network alignment, and describe recent work on using network models to guide experimental validation. We explain how the integration and validation steps spring from a Bayesian description of network uncertainty, and conclude by describing an important near-term milestone for systems biology: the construction of a set of rich reference networks for key model organisms.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Srinivasan et al. - 2007 - Current progress in network research toward refer.pdf}
}

@article{srinivasanPredictiveCodingFresh1982,
  title        = {Predictive {{Coding}}: {{A Fresh View}} of {{Inhibition}} in the {{Retina}}},
  shorttitle   = {Predictive {{Coding}}},
  author       = {Srinivasan, M. V. and Laughlin, S. B. and Dubs, A.},
  date         = {1982-11-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  volume       = {216},
  number       = {1205},
  pages        = {427--459},
  issn         = {0962-8452, 1471-2954},
  doi          = {10/ch2m76},
  url          = {http://rspb.royalsocietypublishing.org/cgi/doi/10.1098/rspb.1982.0085},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Srinivasan et al. - 1982 - Predictive Coding A Fresh View of Inhibition in t.pdf}
}

@article{srivastavaDropoutSimpleWay2014,
  title      = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author     = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year       = 2014,
  journal    = {Journal of Machine Learning Research},
  volume     = {15},
  number     = {56},
  pages      = {1929--1958},
  issn       = {1533-7928},
  urldate    = {2025-12-02},
  abstract   = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file       = {/Users/jkobject/Zotero/storage/QKPBY2DM/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf}
}

@article{stahlVisualizationAnalysisGene2016,
  title    = {Visualization and Analysis of Gene Expression in Tissue Sections by Spatial Transcriptomics},
  author   = {St{\aa}hl, Patrik L. and Salm{\'e}n, Fredrik and Vickovic, Sanja and Lundmark, Anna and Navarro, Jos{\'e} Fern{\'a}ndez and Magnusson, Jens and Giacomello, Stefania and Asp, Michaela and Westholm, Jakub O. and Huss, Mikael and Mollbrink, Annelie and Linnarsson, Sten and Codeluppi, Simone and Borg, {\AA}ke and Pont{\'e}n, Fredrik and Costea, Paul Igor and Sahl{\'e}n, Pelin and Mulder, Jan and Bergmann, Olaf and Lundeberg, Joakim and Fris{\'e}n, Jonas},
  year     = 2016,
  month    = jul,
  journal  = {Science (New York, N.Y.)},
  volume   = {353},
  number   = {6294},
  pages    = {78--82},
  issn     = {1095-9203},
  doi      = {10.1126/science.aaf2403},
  abstract = {Analysis of the pattern of proteins or messengerRNAs (mRNAs) in histological tissue sections is a cornerstone in biomedical research and diagnostics. This typically involves the visualization of a few proteins or expressed genes at a time. We have devised a strategy, which we call "spatial transcriptomics," that allows visualization and quantitative analysis of the transcriptome with spatial resolution in individual tissue sections. By positioning histological sections on arrayed reverse transcription primers with unique positional barcodes, we demonstrate high-quality RNA-sequencing data with maintained two-dimensional positional information from the mouse brain and human breast cancer. Spatial transcriptomics provides quantitative gene expression data and visualization of the distribution of mRNAs within tissue sections and enables novel types of bioinformatics analyses, valuable in research and diagnostics.},
  langid   = {english},
  pmid     = {27365449},
  keywords = {Animals,Brain,Breast Neoplasms,DNA Complementary,Female,Gene Expression Profiling,Humans,Mice,Organ Specificity,RNA Messenger,Sequence Analysis RNA,Transcriptome}
}

@article{stanleyEvolvingNeuralNetworks2002,
  title        = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author       = {Stanley, Kenneth O. and Miikkulainen, Risto},
  date         = {2002-06},
  journaltitle = {Evolutionary Computation},
  volume       = {10},
  number       = {2},
  pages        = {99--127},
  issn         = {1063-6560, 1530-9304},
  doi          = {10/c7whj6},
  url          = {http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
  urldate      = {2018-04-11},
  abstract     = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/ML/NN/NE/Stanley et Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topolo.pdf}
}

@article{stanleyHypercubeBasedIndirectEncoding,
  title      = {A {{Hypercube-Based Indirect Encoding}} for {{Evolving Large-Scale Neural Networks}}},
  author     = {Stanley, Kenneth O and D’Ambrosio, David and Gauci, Jason},
  pages      = {39},
  abstract   = {Research in neuroevolution, i.e. evolving artificial neural networks (ANNs) through evolutionary algorithms, is inspired by the evolution of biological brains. Because natural evolution discovered intelligent brains with billions of neurons and trillions of connections, perhaps neuroevolution can do the same. Yet while neuroevolution has produced successful results in a variety of domains, the scale of natural brains remains far beyond reach. This paper presents a method called Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective Compositional Pattern Producing Networks (connective CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. The advantage of this approach is that it can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/NE/Stanley et al. - A Hypercube-Based Indirect Encoding for Evolving L.pdf}
}

@article{starkRNASequencingTeenage2019,
  title      = {{{RNA}} Sequencing: The Teenage Years},
  shorttitle = {{{RNA}} Sequencing},
  author     = {Stark, Rory and Grzelak, Marta and Hadfield, James},
  year       = 2019,
  month      = nov,
  journal    = {Nature Reviews Genetics},
  volume     = {20},
  number     = {11},
  pages      = {631--656},
  publisher  = {Nature Publishing Group},
  issn       = {1471-0064},
  doi        = {10.1038/s41576-019-0150-2},
  urldate    = {2025-12-02},
  abstract   = {Over the past decade, RNA sequencing (RNA-seq) has become an indispensable tool for transcriptome-wide analysis of differential gene expression and differential splicing of mRNAs. However, as next-generation sequencing technologies have developed, so too has RNA-seq. Now, RNA-seq methods are available for studying many different aspects of RNA biology, including single-cell gene expression, translation (the translatome) and RNA structure (the structurome). Exciting new applications are being explored, such as spatial transcriptomics (spatialomics). Together with new long-read and direct RNA-seq technologies and better computational tools for data analysis, innovations in RNA-seq are contributing to a fuller understanding of RNA biology, from questions such as when and where transcription occurs to the folding and intermolecular interactions that govern RNA function.},
  copyright  = {2019 Springer Nature Limited},
  langid     = {english},
  keywords   = {Gene expression,Gene expression analysis,Gene expression profiling,Next-generation sequencing,RNA,RNA metabolism,RNA sequencing,Transcriptomics},
  file       = {/Users/jkobject/Zotero/storage/3X7IGF4R/Stark et al. - 2019 - RNA sequencing the teenage years.pdf}
}

@article{statelloGeneRegulationLong2021,
  title     = {Gene Regulation by Long Non-Coding {{RNAs}} and Its Biological Functions},
  author    = {Statello, Luisa and Guo, Chun-Jie and Chen, Ling-Ling and Huarte, Maite},
  year      = {2021},
  month     = feb,
  journal   = {Nature Reviews Molecular Cell Biology},
  volume    = {22},
  number    = {2},
  pages     = {96--118},
  publisher = {Nature Publishing Group},
  issn      = {1471-0080},
  doi       = {10.1038/s41580-020-00315-9},
  urldate   = {2024-07-10},
  abstract  = {Evidence accumulated over the past decade shows that long non-coding RNAs (lncRNAs) are widely expressed and have key roles in gene regulation. Recent studies have begun to unravel how the biogenesis of lncRNAs is distinct from that of mRNAs and is linked with their specific subcellular localizations and functions. Depending on their localization and their specific interactions with DNA, RNA and proteins, lncRNAs can modulate chromatin function, regulate the assembly and function of membraneless nuclear bodies, alter the stability and translation of cytoplasmic mRNAs and interfere with signalling pathways. Many of these functions ultimately affect gene expression in diverse biological and physiopathological contexts, such as in neuronal disorders, immune responses and cancer. Tissue-specific and condition-specific expression patterns suggest that lncRNAs are potential biomarkers and provide a rationale to target them clinically. In this Review, we discuss the mechanisms of lncRNA biogenesis, localization and functions in transcriptional, post-transcriptional and other modes of gene regulation, and their potential therapeutic applications.},
  copyright = {2020 Springer Nature Limited},
  langid    = {english},
  keywords  = {Long non-coding RNAs,Transcription},
  file      = {/Users/jkobject/Zotero/storage/6ST76TZ9/Statello et al. - 2021 - Gene regulation by long non-coding RNAs and its bi.pdf}
}

@article{stergachisConservationTransactingCircuitry2014,
  title     = {Conservation of Trans-Acting Circuitry during Mammalian Regulatory Evolution},
  author    = {Stergachis, Andrew B. and Neph, Shane and Sandstrom, Richard and Haugen, Eric and Reynolds, Alex P. and Zhang, Miaohua and Byron, Rachel and Canfield, Theresa and {Stelhing-Sun}, Sandra and Lee, Kristen and Thurman, Robert E. and Vong, Shinny and Bates, Daniel and Neri, Fidencio and Diegel, Morgan and Giste, Erika and Dunn, Douglas and Vierstra, Jeff and Hansen, R. Scott and Johnson, Audra K. and Sabo, Peter J. and Wilken, Matthew S. and Reh, Thomas A. and Treuting, Piper M. and Kaul, Rajinder and Groudine, Mark and Bender, M. A. and Borenstein, Elhanan and Stamatoyannopoulos, John A.},
  year      = {2014},
  month     = nov,
  journal   = {Nature},
  volume    = {515},
  number    = {7527},
  pages     = {365--370},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/nature13972},
  urldate   = {2024-04-18},
  abstract  = {The basic body plan and major physiological axes have been highly conserved during mammalian evolution, yet only a small fraction of the human genome sequence appears to be subject to evolutionary constraint. To quantify cis- versus trans-acting contributions to mammalian regulatory evolution, we performed genomic DNase I footprinting of the mouse genome across 25 cell and tissue types, collectively defining {$\sim$}8.6 million transcription factor (TF) occupancy sites at nucleotide resolution. Here we show that mouse TF footprints conjointly encode a regulatory lexicon that is {$\sim$}95\% similar with that derived from human TF footprints. However, only {$\sim$}20\% of mouse TF footprints have human orthologues. Despite substantial turnover of the cis-regulatory landscape, nearly half of all pairwise regulatory interactions connecting mouse TF genes have been maintained in orthologous human cell types through evolutionary innovation of TF recognition sequences. Furthermore, the higher-level organization of mouse TF-to-TF connections into cellular network architectures is nearly identical with human. Our results indicate that evolutionary selection on mammalian gene regulation is targeted chiefly at the level of trans-regulatory circuitry, enabling and potentiating cis-regulatory plasticity.},
  copyright = {2014 The Author(s)},
  langid    = {english},
  keywords  = {Evolutionary biology,Gene regulation},
  file      = {/Users/jkobject/Zotero/storage/PYDW8GG5/Stergachis et al. - 2014 - Conservation of trans-acting circuitry during mamm.pdf}
}

@article{stolteGenomescaleCRISPRCas9Screen2018,
  title        = {Genome-Scale {{CRISPR-Cas9}} Screen Identifies Druggable Dependencies in {{{\mkbibemph{TP53}}}} Wild-Type {{Ewing}} Sarcoma},
  author       = {Stolte, Björn and Iniguez, Amanda Balboni and Dharia, Neekesh V. and Robichaud, Amanda L. and Conway, Amy Saur and Morgan, Ann M. and Alexe, Gabriela and Schauer, Nathan J. and Liu, Xiaoxi and Bird, Gregory H. and Tsherniak, Aviad and Vazquez, Francisca and Buhrlage, Sara J. and Walensky, Loren D. and Stegmaier, Kimberly},
  date         = {2018-08-06},
  journaltitle = {The Journal of Experimental Medicine},
  volume       = {215},
  number       = {8},
  pages        = {2137--2155},
  issn         = {0022-1007, 1540-9538},
  doi          = {10/gd4fb2},
  url          = {http://www.jem.org/lookup/doi/10.1084/jem.20171066},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/NPKPGEJD/Stolte et al. - 2018 - Genome-scale CRISPR-Cas9 screen identifies druggab.pdf}
}

@article{submissionMachineTheoryMind,
  title      = {Machine {{Theory}} of {{Mind}}},
  author     = {Submission, Anonymous},
  pages      = {21},
  abstract   = {Theory of mind (ToM; Premack \& Woodruff, 1978) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network – a ToMnet – which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents’ behaviour, as well as the ability to bootstrap to richer predictions about agents’ characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the “SallyAnne” test (Wimmer \& Perner, 1983; BaronCohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system – which autonomously learns how to model other agents in its world – is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Artificial Intelligence},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/Submission - Machine Theory of Mind.pdf}
}

@article{subramanianGeneSetEnrichment2005,
  title        = {Gene Set Enrichment Analysis: {{A}} Knowledge-Based Approach for Interpreting Genome-Wide Expression Profiles},
  shorttitle   = {Gene Set Enrichment Analysis},
  author       = {Subramanian, A. and Tamayo, P. and Mootha, V. K. and Mukherjee, S. and Ebert, B. L. and Gillette, M. A. and Paulovich, A. and Pomeroy, S. L. and Golub, T. R. and Lander, E. S. and Mesirov, J. P.},
  date         = {2005-10-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume       = {102},
  number       = {43},
  pages        = {15545--15550},
  issn         = {0027-8424, 1091-6490},
  doi          = {10/d4qbh8},
  url          = {http://www.pnas.org/cgi/doi/10.1073/pnas.0506580102},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/X8RCT7LW/Subramanian et al. - 2005 - Gene set enrichment analysis A knowledge-based ap.pdf}
}

@article{subramanianGeneSetEnrichment2005a,
  title      = {Gene Set Enrichment Analysis: A Knowledge-Based Approach for Interpreting Genome-Wide Expression Profiles},
  shorttitle = {Gene Set Enrichment Analysis},
  author     = {Subramanian, Aravind and Tamayo, Pablo and Mootha, Vamsi K. and Mukherjee, Sayan and Ebert, Benjamin L. and Gillette, Michael A. and Paulovich, Amanda and Pomeroy, Scott L. and Golub, Todd R. and Lander, Eric S. and Mesirov, Jill P.},
  year       = {2005},
  month      = oct,
  journal    = {Proceedings of the National Academy of Sciences of the United States of America},
  volume     = {102},
  number     = {43},
  pages      = {15545--15550},
  issn       = {0027-8424},
  doi        = {10.1073/pnas.0506580102},
  abstract   = {Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.},
  langid     = {english},
  pmcid      = {PMC1239896},
  pmid       = {16199517},
  keywords   = {Cell Line Tumor,Female,Gene Expression Profiling,Genes p53,Genome,Humans,Leukemia Myeloid Acute,Lung Neoplasms,Male,Oligonucleotide Array Sequence Analysis,Precursor Cell Lymphoblastic Leukemia-Lymphoma},
  file       = {/Users/jkobject/Zotero/storage/N8G85GJS/Subramanian et al. - 2005 - Gene set enrichment analysis a knowledge-based ap.pdf}
}

@misc{suInferringGeneRegulatory2024,
  title         = {Inferring Gene Regulatory Networks by Hypergraph Variational Autoencoder},
  author        = {Su, Guangxin and Wang, Hanchen and Zhang, Ying and Coster, Adelle CF and Wilkins, Marc R. and Canete, Pablo F. and Yu, Di and Yang, Yang and Zhang, Wenjie},
  year          = {2024},
  month         = apr,
  primaryclass  = {New Results},
  pages         = {2024.04.01.586509},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.04.01.586509},
  urldate       = {2024-07-10},
  abstract      = {In constructing Gene Regulatory Networks (GRNs), it is crucial to consider cellular heterogeneity and differential gene regulatory modules. However, traditional methods have predominantly focused on cellular heterogeneity, approaching the subject from a relatively narrow scope. We present HyperG-VAE, a Bayesian deep generative model that utilizes a hypergraph to model single-cell RNA sequencing (scRNA-seq) data. HyperG-VAE employs a cell encoder with a Structural Equation Model to address cellular heterogeneity and build GRNs, alongside a gene encoder using hypergraph self-attention to identify gene modules. Encoders are synergistically optimized by a decoder, enabling HyperG-VAE to excel in GRN inference, single-cell clustering, and data visualization, evidenced by benchmarks. Additionally, HyperG-VAE effectively reveals gene regulation patterns and shows robustness in varied downstream analyses, demonstrated using B cell development data in bone marrow. The interplay of encoders by the overlapping genes between predicted GRNs and gene modules is further validated by gene set enrichment analysis, underscoring that the gene encoder boosts the GRN inference. HyperG-VAE proves efficient in scRNA-seq data analysis and GRN inference.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/F6LT7SNA/Su et al. - 2024 - Inferring gene regulatory networks by hypergraph v.pdf}
}

@article{suterMammalianGenesAre2011,
  title        = {Mammalian {{Genes Are Transcribed}} with {{Widely Different Bursting Kinetics}}},
  author       = {Suter, D. M. and Molina, N. and Gatfield, D. and Schneider, K. and Schibler, U. and Naef, F.},
  date         = {2011-04-22},
  journaltitle = {Science},
  volume       = {332},
  number       = {6028},
  pages        = {472--474},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/cmzs4g},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.1198817},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{szalataBenchmarkPredictionTranscriptomicb,
  title  = {A Benchmark for Prediction of Transcriptomic Responses to Chemical Perturbations across Cell Types},
  author = {Sza{\l}ata, Artur and Benz, Andrew and Cannoodt, Robrecht and Cortes, Mauricio and Fong, Jason and Kuppasani, Sunil and Lieberman, Richard and Liu, Tianyu and {Mas-Rosario}, Javier A and Meinl, Rico and Nourisa, Jalil and Tumiel, Jared and Tunjic, Tin M and Wang, Mengbo and Weber, Noah and Zhao, Hongyu and Anchang, Benedict and Theis, Fabian J and Luecken, Malte D and Burkhardt, Daniel B},
  langid = {english},
  file   = {/Users/jkobject/Zotero/storage/ML5SWLXC/Szałata et al. - A benchmark for prediction of transcriptomic respo.pdf}
}

@article{szklarczykSTRINGDatabase20232023,
  title      = {The {{STRING}} Database in 2023: Protein--Protein Association Networks and Functional Enrichment Analyses for Any Sequenced Genome of Interest},
  shorttitle = {The {{STRING}} Database in 2023},
  author     = {Szklarczyk, Damian and Kirsch, Rebecca and Koutrouli, Mikaela and Nastou, Katerina and Mehryary, Farrokh and Hachilif, Radja and Gable, Annika L and Fang, Tao and Doncheva, Nadezhda~T and Pyysalo, Sampo and Bork, Peer and Jensen, Lars~J and {von~Mering}, Christian},
  year       = {2023},
  month      = jan,
  journal    = {Nucleic Acids Research},
  volume     = {51},
  number     = {D1},
  pages      = {D638-D646},
  issn       = {0305-1048},
  doi        = {10.1093/nar/gkac1000},
  urldate    = {2024-04-19},
  abstract   = {Much of the complexity within cells arises from functional and regulatory interactions among proteins. The core of these interactions is increasingly known, but novel interactions continue to be discovered, and the information remains scattered across different database resources, experimental modalities and levels of mechanistic detail. The STRING database (https://string-db.org/) systematically collects and integrates protein--protein interactions---both physical interactions as well as functional associations. The data originate from a number of sources: automated text mining of the scientific literature, computational interaction predictions from co-expression, conserved genomic context, databases of interaction experiments and known complexes/pathways from curated sources. All of these interactions are critically assessed, scored, and subsequently automatically transferred to less well-studied organisms using hierarchical orthology information. The data can be accessed via the website, but also programmatically and via bulk downloads. The most recent developments in STRING (version 12.0) are: (i) it is now possible to create, browse and analyze a full interaction network for any novel genome of interest, by submitting its complement of encoded proteins, (ii) the co-expression channel now uses variational auto-encoders to predict interactions, and it covers two new sources, single-cell RNA-seq and experimental proteomics data and (iii) the confidence in each experimentally derived interaction is now estimated based on the detection method used, and communicated to the user in the web-interface. Furthermore, STRING continues to enhance its facilities for functional enrichment analysis, which are now fully available also for user-submitted genomes.},
  file       = {/Users/jkobject/Zotero/storage/IP7E47XW/Szklarczyk et al. - 2023 - The STRING database in 2023 protein–protein assoc.pdf;/Users/jkobject/Zotero/storage/IXU48Z5X/6825349.html}
}

@article{talukdarTranscriptionalCoactivatorsEmerging2023,
  title      = {Transcriptional Co-Activators: Emerging Roles in Signaling Pathways and Potential Therapeutic Targets for Diseases},
  shorttitle = {Transcriptional Co-Activators},
  author     = {Talukdar, Priyanka Dey and Chatterji, Urmi},
  year       = {2023},
  month      = nov,
  journal    = {Signal Transduction and Targeted Therapy},
  volume     = {8},
  number     = {1},
  pages      = {1--41},
  publisher  = {Nature Publishing Group},
  issn       = {2059-3635},
  doi        = {10.1038/s41392-023-01651-w},
  urldate    = {2024-07-10},
  abstract   = {Specific cell states in metazoans are established by the symphony of gene expression programs that necessitate intricate synergic interactions between transcription factors and the co-activators. Deregulation of these regulatory molecules is associated with cell state transitions, which in turn is accountable for diverse maladies, including developmental disorders, metabolic disorders, and most significantly, cancer. A decade back most transcription factors, the key enablers of disease development, were historically viewed as `undruggable'; however, in the intervening years, a wealth of literature validated that they can be targeted indirectly through transcriptional co-activators, their confederates in various physiological and molecular processes. These co-activators, along with transcription factors, have the ability to initiate and modulate transcription of diverse genes necessary for normal physiological functions, whereby, deregulation of such interactions may foster tissue-specific disease phenotype. Hence, it is essential to analyze how these co-activators modulate specific multilateral processes in coordination with other factors. The proposed review attempts to elaborate an in-depth account of the transcription co-activators, their involvement in transcription regulation, and context-specific contributions to pathophysiological conditions. This review also addresses an issue that has not been dealt with in a comprehensive manner and hopes to direct attention towards future research that will encompass patient-friendly therapeutic strategies, where drugs targeting co-activators will have enhanced benefits and reduced side effects. Additional insights into currently available therapeutic interventions and the associated constraints will eventually reveal multitudes of advanced therapeutic targets aiming for disease amelioration and good patient prognosis.},
  copyright  = {2023 The Author(s)},
  langid     = {english},
  keywords   = {Drug development},
  file       = {/Users/jkobject/Zotero/storage/LSN2REKC/Talukdar and Chatterji - 2023 - Transcriptional co-activators emerging roles in s.pdf}
}

@article{tangEvaluatingRepresentationalPower2024,
  title    = {Evaluating the Representational Power of Pre-Trained {{DNA}} Language Models for Regulatory Genomics},
  author   = {Tang, Ziqi and Somia, Nirali and Yu, Yiyang and Koo, Peter K},
  year     = {2024},
  month    = sep,
  journal  = {bioRxiv},
  pages    = {2024.02.29.582810},
  issn     = {2692-8205},
  doi      = {10.1101/2024.02.29.582810},
  urldate  = {2025-03-27},
  abstract = {The emergence of genomic language models (gLMs) offers an unsupervised approach to learning a wide diversity of cis-regulatory patterns in the non-coding genome without requiring labels of functional activity generated by wet-lab experiments. Previous evaluations have shown that pre-trained gLMs can be leveraged to improve predictive performance across a broad range of regulatory genomics tasks, albeit using relatively simple benchmark datasets and baseline models. Since the gLMs in these studies were tested upon fine-tuning their weights for each downstream task, determining whether gLM representations embody a foundational understanding of cis-regulatory biology remains an open question. Here we evaluate the representational power of pre-trained gLMs to predict and interpret cell-type-specific functional genomics data that span DNA and RNA regulation. Our findings suggest that probing the representations of pre-trained gLMs do not offer substantial advantages over conventional machine learning approaches that use one-hot encoded sequences. This work highlights a major gap with current gLMs, raising potential issues in conventional pre-training strategies for the non-coding genome.},
  pmcid    = {PMC10925287},
  pmid     = {38464101},
  file     = {/Users/jkobject/Zotero/storage/IDNNPITW/Tang et al. - 2024 - Evaluating the representational power of pre-train.pdf}
}

@misc{TargetingLIPAIndependent,
  title        = {Targeting {{LIPA}} Independent of Its Lipase Activity Is a Therapeutic Strategy in Solid Tumors via Induction of Endoplasmic Reticulum Stress {\textbar} {{Nature Cancer}}},
  urldate      = {2024-07-25},
  howpublished = {https://www.nature.com/articles/s43018-022-00389-8},
  file         = {/Users/jkobject/Zotero/storage/8E7FAIUY/s43018-022-00389-8.html}
}

@article{taroniMultiPLIERTransferLearning2019,
  title        = {{{MultiPLIER}}: A Transfer Learning Framework for Transcriptomics Reveals Systemic Features of Rare Disease},
  shorttitle   = {{{MultiPLIER}}},
  author       = {Taroni, Jaclyn N and Grayson, Peter C and Hu, Qiwen and Eddy, Sean and Kretzler, Matthias and Merkel, Peter A and Greene, Casey S},
  date         = {2019-01-15},
  journaltitle = {bioRxiv},
  doi          = {10/gfc9bb},
  url          = {http://biorxiv.org/lookup/doi/10.1101/395947},
  urldate      = {2019-03-22},
  abstract     = {Unsupervised machine learning methods provide a promising means to analyze and interpret large datasets. However, most datasets generated by individual researchers remain too small to fully benefit from these methods. In the case of rare diseases, there may be too few cases available, even when multiple studies are combined. We sought to determine whether or not machine learning models could be constructed from large public data compendia and then transferred to small datasets for subsequent analysis. We trained models using Pathway Level Information ExtractoR (PLIER) over datasets of different types and scales. Models constructed from large public datasets were i) more detailed than those constructed from individual datasets; ii) included features that aligned well to important biological factors; iii) transferrable to rare disease datasets where the models describe biological processes related to disease severity more effectively than models trained within those datasets.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/58XRPSVS/Taroni et al. - 2019 - MultiPLIER a transfer learning framework for tran.pdf}
}

@misc{tayLongRangeArena2020,
  title         = {Long {{Range Arena}}: {{A Benchmark}} for {{Efficient Transformers}}},
  shorttitle    = {Long {{Range Arena}}},
  author        = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  year          = {2020},
  month         = nov,
  number        = {arXiv:2011.04006},
  eprint        = {2011.04006},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2011.04006},
  urldate       = {2024-10-25},
  abstract      = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from \$1K\$ to \$16K\$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/8WDXNUV4/Tay et al. - 2020 - Long Range Arena A Benchmark for Efficient Transf.pdf;/Users/jkobject/Zotero/storage/DPYZ7854/2011.html}
}

@article{taylor-weinerScalingComputationalGenomics2019,
  title        = {Scaling Computational Genomics to Millions of Individuals with {{GPUs}}},
  author       = {Taylor-weiner, Amaro and Aguet, Francois and Haradhvala, Nicholas and Gosai, Sager and Anand, Shankara and Kim, Jaegil and Ardlie, Kristin and Van Allen, Eliezer and Getz, Gad},
  date         = {2019-02-08},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbg4},
  url          = {http://biorxiv.org/lookup/doi/10.1101/470138},
  urldate      = {2019-03-22},
  abstract     = {Current genomics methods were designed to handle tens to thousands of samples, but will soon need to scale to millions to keep up with the pace of data and hypothesis generation in biomedical science. Moreover, costs associated with processing these growing datasets will become prohibitive without improving the computational efficiency and scalability of methods. Here, we show that recently developed machine-learning libraries (TensorFlow and PyTorch) facilitate implementation of genomics methods for GPUs and significantly accelerate computations. To demonstrate this, we re-implemented methods for two commonly performed computational genomics tasks: QTL mapping and Bayesian non-negative matrix factorization. Our implementations ran {$>$} 200 times faster than current CPUbased versions, and these analyses are \textasciitilde 5-10 fold cheaper on GPUs due to the vastly shorter runtimes. We anticipate that the accessibility of these libraries, and the improvements in run-time will lead to a transition to GPU-based implementations for a wide range of computational genomics methods.},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/N6DE5728/Taylor-weiner et al. - 2019 - Scaling computational genomics to millions of indi.pdf}
}

@article{tegmarkConsciousnessStateMatter2015,
  title        = {Consciousness as a {{State}} of {{Matter}}},
  author       = {Tegmark, Max},
  date         = {2015-07},
  journaltitle = {Chaos, Solitons \& Fractals},
  volume       = {76},
  eprint       = {1401.1219},
  eprinttype   = {arXiv},
  pages        = {238--270},
  issn         = {09600779},
  doi          = {10/f7gnqb},
  url          = {http://arxiv.org/abs/1401.1219},
  urldate      = {2019-03-22},
  abstract     = {We examine the hypothesis that consciousness can be understood as a state of matter, "perceptronium", with distinctive information processing abilities. We explore five basic principles that may distinguish conscious matter from other physical systems such as solids, liquids and gases: the information, integration, independence, dynamics and utility principles. If such principles can identify conscious entities, then they can help solve the quantum factorization problem: why do conscious observers like us perceive the particular Hilbert space factorization corresponding to classical space (rather than Fourier space, say), and more generally, why do we perceive the world around us as a dynamic hierarchy of objects that are strongly integrated and relatively independent? Tensor factorization of matrices is found to play a central role, and our technical results include a theorem about Hamiltonian separability (defined using Hilbert-Schmidt superoperators) being maximized in the energy eigenbasis. Our approach generalizes Giulio Tononi's integrated information framework for neural-network-based consciousness to arbitrary quantum systems, and we find interesting links to error-correcting codes, condensed matter criticality, and the Quantum Darwinism program, as well as an interesting connection between the emergence of consciousness and the emergence of time.},
  langid       = {english},
  keywords     = {Condensed Matter - Disordered Systems and Neural Networks,High Energy Physics - Theory,Quantum Physics},
  annotation   = {00000}
}

@article{tejadaLapuertaNicheformerFoundationModel2025,
  title   = {Nicheformer: a foundation model for single-cell and spatial omics},
  author  = {Tejada-Lapuerta, Alejandro and others},
  journal = {Nature Methods},
  pages   = {1--14},
  year    = {2025},
  doi     = {10.1038/s41592-025-02814-z}
}

@article{teschendorffTensorialBlindSource2018,
  title        = {Tensorial Blind Source Separation for Improved Analysis of Multi-Omic Data},
  author       = {Teschendorff, Andrew E. and Jing, Han and Paul, Dirk S. and Virta, Joni and Nordhausen, Klaus},
  date         = {2018-12},
  journaltitle = {Genome Biology},
  volume       = {19},
  number       = {1},
  issn         = {1474-760X},
  doi          = {10/gdrss5},
  url          = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-018-1455-8},
  urldate      = {2019-03-22},
  abstract     = {There is an increased need for integrative analyses of multi-omic data. We present and benchmark a novel tensorial independent component analysis (tICA) algorithm against current state-of-the-art methods. We find that tICA outperforms competing methods in identifying biological sources of data variation at a reduced computational cost. On epigenetic data, tICA can identify methylation quantitative trait loci at high sensitivity. In the cancer context, tICA identifies gene modules whose expression variation across tumours is driven by copy-number or DNA methylation changes, but whose deregulation relative to normal tissue is independent of such alterations, a result we validate by direct analysis of individual data types.},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/VNC4NAAZ/Teschendorff et al. - 2018 - Tensorial blind source separation for improved ana.pdf}
}

@article{tessonUsingZebrafishModel,
  title      = {Using {{Zebrafish}} as a Model to {{Understand}} the {{Endothelial}} to {{Hematopoietic Transition}}},
  author     = {Tesson, Baptiste},
  pages      = {16},
  abstract   = {In vertebrate embryos, hematopoietic stem cells emerge (HSCs) from the floor of the dorsal aorta in a process known as the endothelial to hematopoietic transition (EHT). During EHT, endothelial genes are downregulated concomitant with the upregulation of hematopoietic specific genes. Runx1 is a transcription factor that has been shown to be required for EHT. Knockdown of Runx1 results in apoptosis of endothelial cells committed to the HSC fate. However, how Runx1 promotes cell survival during EHT remains poorly understood. In this study, we characterize the role of the Runx1 and of the Notch signaling during EHT. Here we show that inhibition of integrin linked kinase (Ilk) results in selective cell loss of HSCs from the dorsal aorta, suggesting that Ilk is required for HSC cell survival during EHT. In addition, we show that inhibition of the mTor pathway results in the inability of HSCs to leave the dorsal aorta in a process that is not directly regulated by Runx1. Finally, we show that the HSCs undergoing EHT are positive for Tp1::YFP, a reporter for the Notch Signaling pathway. However, inhibition of Notch signaling pathway results in normal EHT. Our results indicate that Runx1 regulates integrin signaling components that are required for HSC survival during EHT, and that Notch signaling is dispensable for EHT to occur.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/friends/Tesson - Using Zebrafish as a model to Understand the Endot.pdf}
}

@misc{testft,
  title         = {Med42 -- {{Evaluating Fine-Tuning Strategies}} for {{Medical LLMs}}: {{Full-Parameter}} vs. {{Parameter-Efficient Approaches}}},
  shorttitle    = {Med42 -- {{Evaluating Fine-Tuning Strategies}} for {{Medical LLMs}}},
  author        = {Christophe, Cl{\'e}ment and Kanithi, Praveen K. and Munjal, Prateek and Raha, Tathagata and Hayat, Nasir and Rajan, Ronnie and {Al-Mahrooqi}, Ahmed and Gupta, Avani and Salman, Muhammad Umar and Gosal, Gurpreet and Kanakiya, Bhargav and Chen, Charles and Vassilieva, Natalia and Amor, Boulbaba Ben and Pimentel, Marco AF and Khan, Shadab},
  year          = {2024},
  month         = apr,
  number        = {arXiv:2404.14779},
  eprint        = {2404.14779},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2404.14779},
  urldate       = {2025-02-25},
  abstract      = {This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72\% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/U6AZXFTF/Christophe et al. - 2024 - Med42 -- Evaluating Fine-Tuning Strategies for Med.pdf;/Users/jkobject/Zotero/storage/2QSSY2GI/2404.html}
}

@article{thakurRealtimeMeasurementProtein2018,
  title        = {Real-Time Measurement of Protein–Protein Interactions at Single-Molecule Resolution Using a Biological Nanopore},
  author       = {Thakur, Avinash Kumar and Movileanu, Liviu},
  date         = {2018-12-10},
  journaltitle = {Nature Biotechnology},
  volume       = {37},
  number       = {1},
  pages        = {96--101},
  issn         = {1087-0156, 1546-1696},
  doi          = {10/gfxbgx},
  url          = {http://www.nature.com/doifinder/10.1038/nbt.4316},
  urldate      = {2019-03-22},
  langid       = {english}
}

@article{thecancergenomeatlasresearchnetworkCancerGenomeAtlas2013,
  title        = {The {{Cancer Genome Atlas Pan-Cancer}} Analysis Project},
  author       = {{The Cancer Genome Atlas Research Network} and Weinstein, John N and Collisson, Eric A and Mills, Gordon B and Shaw, Kenna R Mills and Ozenberger, Brad A and Ellrott, Kyle and Shmulevich, Ilya and Sander, Chris and Stuart, Joshua M},
  date         = {2013-10},
  journaltitle = {Nature Genetics},
  volume       = {45},
  number       = {10},
  pages        = {1113--1120},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/f3nt5c},
  url          = {http://www.nature.com/articles/ng.2764},
  urldate      = {2019-03-22},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/HKB7VXPA/The Cancer Genome Atlas Research Network et al. - 2013 - The Cancer Genome Atlas Pan-Cancer analysis projec.pdf}
}

@article{thedream5consortiumWisdomCrowdsRobust2012,
  title        = {Wisdom of Crowds for Robust Gene Network Inference},
  author       = {{The DREAM5 Consortium} and Marbach, Daniel and Costello, James C and Küffner, Robert and Vega, Nicole M and Prill, Robert J and Camacho, Diogo M and Allison, Kyle R and Kellis, Manolis and Collins, James J and Stolovitzky, Gustavo},
  date         = {2012-08},
  journaltitle = {Nature Methods},
  volume       = {9},
  number       = {8},
  pages        = {796--804},
  issn         = {1548-7091, 1548-7105},
  doi          = {10/f36vjn},
  url          = {http://www.nature.com/articles/nmeth.2016},
  urldate      = {2018-04-11},
  abstract     = {Reconstructing gene regulatory networks from high-throughput data is a long-standing problem. Through the DREAM project (Dialogue on Reverse Engineering Assessment and Methods), we performed a comprehensive blind assessment of over thirty network inference methods on Escherichia coli, Staphylococcus aureus, Saccharomyces cerevisiae, and in silico microarray data. We characterize performance, data requirements, and inherent biases of different inference approaches offering guidelines for both algorithm application and development. We observe that no single inference method performs optimally across all datasets. In contrast, integration of predictions from multiple inference methods shows robust and high performance across diverse datasets. Thereby, we construct high-confidence networks for E. coli and S. aureus, each comprising \textasciitilde 1700 transcriptional interactions at an estimated precision of 50\%. We experimentally test 53 novel interactions in E. coli, of which 23 were supported (43\%). Our results establish community-based methods as a powerful and robust tool for the inference of transcriptional gene regulatory networks.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/The DREAM5 Consortium et al. - 2012 - Wisdom of crowds for robust gene network inference.pdf}
}

@article{theencodeprojectconsortiumIntegratedEncyclopediaDNA2012,
  title        = {An Integrated Encyclopedia of {{DNA}} Elements in the Human Genome},
  author       = {{The ENCODE Project Consortium}},
  date         = {2012-09},
  journaltitle = {Nature},
  volume       = {489},
  number       = {7414},
  pages        = {57--74},
  issn         = {0028-0836, 1476-4687},
  doi          = {10/bg9d},
  url          = {http://www.nature.com/articles/nature11247},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000}
}

@article{theodorisTransferLearningEnables2023,
  title     = {Transfer Learning Enables Predictions in Network Biology},
  author    = {Theodoris, Christina V. and Xiao, Ling and Chopra, Anant and Chaffin, Mark D. and Al Sayed, Zeina R. and Hill, Matthew C. and Mantineo, Helene and Brydon, Elizabeth M. and Zeng, Zexian and Liu, X. Shirley and Ellinor, Patrick T.},
  year      = {2023},
  month     = jun,
  journal   = {Nature},
  volume    = {618},
  number    = {7965},
  pages     = {616--624},
  publisher = {Nature Publishing Group},
  issn      = {1476-4687},
  doi       = {10.1038/s41586-023-06139-9},
  urldate   = {2024-04-18},
  abstract  = {Mapping gene networks requires large amounts of transcriptomic data to learn the connections between genes, which impedes discoveries in settings with limited data, including rare diseases and diseases affecting clinically inaccessible tissues. Recently, transfer learning has revolutionized fields such as natural language understanding1,2 and computer vision3 by leveraging deep learning models pretrained on large-scale general datasets that can then be fine-tuned towards a vast array of downstream tasks with limited task-specific data. Here, we developed a context-aware, attention-based deep learning model, Geneformer, pretrained on a large-scale corpus of about 30\,million single-cell transcriptomes to enable context-specific predictions in settings with limited data in network biology. During pretraining, Geneformer gained a fundamental understanding of network dynamics, encoding network hierarchy in the attention weights of the model in a completely self-supervised manner. Fine-tuning towards a diverse panel of downstream tasks relevant to chromatin and network dynamics using limited task-specific data demonstrated that Geneformer consistently boosted predictive accuracy. Applied to disease modelling with limited patient data, Geneformer identified candidate therapeutic targets for cardiomyopathy. Overall, Geneformer represents a pretrained deep learning model from which fine-tuning towards a broad range of downstream applications can be pursued to accelerate discovery of key network regulators and candidate therapeutic targets.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid    = {english},
  keywords  = {Cardiomyopathies,Computational models,Gene regulatory networks,Machine learning,Regulatory networks},
  file      = {/Users/jkobject/Zotero/storage/W69QLFKD/Theodoris et al. - 2023 - Transfer learning enables predictions in network b.pdf}
}

@article{thetabulasapiensconsortiumTabulaSapiensMultipleorgan2022,
  title      = {The {{Tabula Sapiens}}: {{A}} Multiple-Organ, Single-Cell Transcriptomic Atlas of Humans},
  shorttitle = {The {{Tabula Sapiens}}},
  author     = {{THE TABULA SAPIENS CONSORTIUM}},
  year       = {2022},
  month      = may,
  journal    = {Science},
  volume     = {376},
  number     = {6594},
  pages      = {eabl4896},
  publisher  = {American Association for the Advancement of Science},
  doi        = {10.1126/science.abl4896},
  urldate    = {2024-04-19},
  abstract   = {Molecular characterization of cell types using single-cell transcriptome sequencing is revolutionizing cell biology and enabling new insights into the physiology of human organs. We created a human reference atlas comprising nearly 500,000 cells from 24 different tissues and organs, many from the same donor. This atlas enabled molecular characterization of more than 400 cell types, their distribution across tissues, and tissue-specific variation in gene expression. Using multiple tissues from a single donor enabled identification of the clonal distribution of T cells between tissues, identification of the tissue-specific mutation rate in B cells, and analysis of the cell cycle state and proliferative potential of shared cell types across tissues. Cell type--specific RNA splicing was discovered and analyzed across tissues within an individual.},
  file       = {/Users/jkobject/Zotero/storage/CJ2PIQY3/THE TABULA SAPIENS CONSORTIUM - 2022 - The Tabula Sapiens A multiple-organ, single-cell .pdf}
}

@article{thompsonOpticalStimulationNeurons,
  title      = {Optical {{Stimulation}} of {{Neurons}}},
  author     = {Thompson, Alexander C and Stoddart, Paul R and Jansen, E Duco},
  pages      = {16},
  abstract   = {Our capacity to interface with the nervous system remains overwhelmingly reliant on electrical stimulation devices, such as electrode arrays and cuff electrodes that can stimulate both central and peripheral nervous systems. However, electrical stimulation has to deal with multiple challenges, including selectivity, spatial resolution, mechanical stability, implant-induced injury and the subsequent inflammatory response. Optical stimulation techniques may avoid some of these challenges by providing more selective stimulation, higher spatial resolution and reduced invasiveness of the device, while also avoiding the electrical artefacts that complicate recordings of electrically stimulated neuronal activity. This review explores the current status of optical stimulation techniques, including optogenetic methods, photoactive molecule approaches and infrared neural stimulation, together with emerging techniques such as hybrid optical-electrical stimulation, nanoparticle enhanced stimulation and optoelectric methods. Infrared neural stimulation is particularly emphasised, due to the potential for direct activation of neural tissue by infrared light, as opposed to techniques that rely on the introduction of exogenous light responsive materials. However, infrared neural stimulation remains imperfectly understood, and techniques for accurately delivering light are still under development. While the various techniques reviewed here confirm the overall feasibility of optical stimulation, a number of challenges remain to be overcome before they can deliver their full potential.},
  langid     = {english},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/computational neuro/Thompson et al. - Optical Stimulation of Neurons.pdf}
}

@inproceedings{tilletTritonIntermediateLanguage2019,
  title      = {Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
  shorttitle = {Triton},
  booktitle  = {Proceedings of the 3rd {{ACM SIGPLAN International Workshop}} on {{Machine Learning}} and {{Programming Languages}}},
  author     = {Tillet, Philippe and Kung, H. T. and Cox, David},
  year       = {2019},
  month      = jun,
  series     = {{{MAPL}} 2019},
  pages      = {10--19},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  doi        = {10.1145/3315508.3329973},
  urldate    = {2025-02-25},
  abstract   = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts -- usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
  isbn       = {978-1-4503-6719-6}
}

@article{tishbyInformationBottleneckMethod,
  title      = {The {{Information Bottleneck Method}}},
  author     = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  pages      = {11},
  abstract   = {We define the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of finding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a ‘bottleneck’ formed by a limited set of codewords X˜ . This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x˜) emerges from the joint statistics of X and Y . The approach yields an exact set of self-consistent equations for the coding rules X → X˜ and X˜ → Y . Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Tishby et al. - The Information Bottleneck Method.pdf}
}

@article{tishbyInformationBottleneckMethoda,
  title    = {The {{Information Bottleneck Method}}},
  author   = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  abstract = {We define the relevant information in a signal x {$\in$} X as being the information that this signal provides about another signal y {$\in$} Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of finding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a `bottleneck' formed by a limited set of codewords X{\texttildelow} . This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x{\texttildelow}) emerges from the joint statistics of X and Y . The approach yields an exact set of self-consistent equations for the coding rules X {$\rightarrow$} X{\texttildelow} and X{\texttildelow} {$\rightarrow$} Y . Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut--Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  langid   = {english},
  file     = {/Users/jkobject/Zotero/storage/P58NPKNI/Tishby et al. - The Information Bottleneck Method.pdf}
}

@unpublished{tobinDomainRandomizationTransferring2017,
  title       = {Domain {{Randomization}} for {{Transferring Deep Neural Networks}} from {{Simulation}} to the {{Real World}}},
  author      = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  date        = {2017-03-20},
  eprint      = {1703.06907},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1703.06907},
  urldate     = {2019-03-22},
  abstract    = {Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Robotics},
  file        = {/Users/jkobject/Zotero/storage/CH7EU537/Tobin et al. - 2017 - Domain Randomization for Transferring Deep Neural .pdf}
}

@inproceedings{tobinDomainRandomizationTransferring2017a,
  title     = {Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},
  author    = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  date      = {2017-09},
  pages     = {23--30},
  publisher = {IEEE},
  doi       = {10.1109/IROS.2017.8202133},
  url       = {http://ieeexplore.ieee.org/document/8202133/},
  urldate   = {2018-04-11},
  abstract  = {Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  isbn      = {978-1-5386-2682-5},
  langid    = {english},
  file      = {/Users/jeremie/Documents/science/ML/Tobin et al. - 2017 - Domain randomization for transferring deep neural .pdf}
}

@article{tolstikhinWassersteinAutoEncoders,
  title      = {Wasserstein {{Auto-Encoders}}},
  author     = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Scholkopf, Bernhard},
  pages      = {18},
  abstract   = {We propose the Wasserstein Auto-Encoder (WAE)—a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE) [1]. This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE) [2]. Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/autoEncoder/Tolstikhin et al. - Wasserstein Auto-Encoders.pdf}
}

@article{topolHighperformanceMedicineConvergence2019,
  title        = {High-Performance Medicine: The Convergence of Human and Artificial Intelligence},
  shorttitle   = {High-Performance Medicine},
  author       = {Topol, Eric J.},
  date         = {2019-01},
  journaltitle = {Nature Medicine},
  volume       = {25},
  number       = {1},
  pages        = {44--56},
  issn         = {1078-8956, 1546-170X},
  doi          = {10/gfsvzn},
  url          = {http://www.nature.com/articles/s41591-018-0300-7},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{tranBenchmarkBatcheffectCorrection2020,
  title    = {A Benchmark of Batch-Effect Correction Methods for Single-Cell {{RNA}} Sequencing Data},
  author   = {Tran, Hoa Thi Nhu and Ang, Kok Siong and Chevrier, Marion and Zhang, Xiaomeng and Lee, Nicole Yee Shin and Goh, Michelle and Chen, Jinmiao},
  year     = {2020},
  month    = jan,
  journal  = {Genome Biology},
  volume   = {21},
  number   = {1},
  pages    = {12},
  issn     = {1474-760X},
  doi      = {10.1186/s13059-019-1850-9},
  urldate  = {2024-04-19},
  abstract = {Large-scale single-cell transcriptomic datasets generated using different technologies contain batch-specific systematic variations that present a challenge to batch-effect removal and data integration. With continued growth expected in scRNA-seq data, achieving effective batch integration with available computational resources is crucial. Here, we perform an in-depth benchmark study on available batch correction methods to determine the most suitable method for batch-effect removal.},
  keywords = {Batch correction,Batch effect,Differential gene expression,Integration,Single-cell RNA-seq},
  file     = {/Users/jkobject/Zotero/storage/KYK45PHR/Tran et al. - 2020 - A benchmark of batch-effect correction methods for.pdf;/Users/jkobject/Zotero/storage/A3UY257F/s13059-019-1850-9.html}
}

@article{tsherniakDefiningCancerDependency2017,
  title        = {Defining a {{Cancer Dependency Map}}},
  author       = {Tsherniak, Aviad and Vazquez, Francisca and Montgomery, Phil G. and Weir, Barbara A. and Kryukov, Gregory and Cowley, Glenn S. and Gill, Stanley and Harrington, William F. and Pantel, Sasha and Krill-Burger, John M. and Meyers, Robin M. and Ali, Levi and Goodale, Amy and Lee, Yenarae and Jiang, Guozhi and Hsiao, Jessica and Gerath, William F.J. and Howell, Sara and Merkel, Erin and Ghandi, Mahmoud and Garraway, Levi A. and Root, David E. and Golub, Todd R. and Boehm, Jesse S. and Hahn, William C.},
  date         = {2017-07},
  journaltitle = {Cell},
  volume       = {170},
  number       = {3},
  pages        = {564-576.e16},
  issn         = {00928674},
  doi          = {10.1016/j.cell.2017.06.010},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867417306517},
  urldate      = {2019-03-22},
  abstract     = {Most human epithelial tumors harbor numerous alterations, making it difficult to predict which genes are required for tumor survival. To systematically identify cancer dependencies, we analyzed 501 genome-scale loss-of-function screens performed in diverse human cancer cell lines. We developed DEMETER, an analytical framework that segregates on- from off-target effects of RNAi. 769 genes were differentially required in subsets of these cell lines at a threshold of six SDs from the mean. We found predictive models for 426 dependencies (55\%) by nonlinear regression modeling considering 66,646 molecular features. Many dependencies fall into a limited number of classes, and unexpectedly, in 82\% of models, the top biomarkers were expression based. We demonstrated the basis behind one such predictive model linking hypermethylation of the UBB ubiquitin gene to a dependency on UBC. Together, these observations provide a foundation for a cancer dependency map that facilitates the prioritization of therapeutic targets.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/N67F9S4Z/Tsherniak et al. - 2017 - Defining a Cancer Dependency Map.pdf}
}

@article{tureiOmniPathGuidelinesGateway2016,
  title      = {{{OmniPath}}: Guidelines and Gateway for Literature-Curated Signaling Pathway Resources},
  shorttitle = {{{OmniPath}}},
  author     = {T{\"u}rei, D{\'e}nes and Korcsm{\'a}ros, Tam{\'a}s and {Saez-Rodriguez}, Julio},
  year       = {2016},
  month      = dec,
  journal    = {Nature Methods},
  volume     = {13},
  number     = {12},
  pages      = {966--967},
  publisher  = {Nature Publishing Group},
  issn       = {1548-7105},
  doi        = {10.1038/nmeth.4077},
  urldate    = {2024-07-23},
  copyright  = {2016 Springer Nature America, Inc.},
  langid     = {english},
  keywords   = {Cellular signalling networks,Software},
  file       = {/Users/jkobject/Zotero/storage/A54UU76Y/Türei et al. - 2016 - OmniPath guidelines and gateway for literature-cu.pdf}
}

@article{turkarslanComprehensiveMapGenomewide2015,
  title        = {A Comprehensive Map of Genome-Wide Gene Regulation in {{Mycobacterium}} Tuberculosis},
  author       = {Turkarslan, Serdar and Peterson, Eliza J R and Rustad, Tige R and Minch, Kyle J and Reiss, David J and Morrison, Robert and Ma, Shuyi and Price, Nathan D and Sherman, David R and Baliga, Nitin S},
  date         = {2015-03-31},
  journaltitle = {Scientific Data},
  volume       = {2},
  pages        = {150010},
  issn         = {2052-4463},
  doi          = {10/f7dpbm},
  url          = {http://www.nature.com/articles/sdata201510},
  urldate      = {2018-04-11},
  langid       = {english}
}

@article{tyckoIdentificationMitigationPervasive2019,
  title        = {Identification and Mitigation of Pervasive Off-Target Activity in {{CRISPR-Cas9}} Screens for Essential Non-Coding Elements: {{Supplementary Information}}},
  shorttitle   = {Identification and Mitigation of Pervasive Off-Target Activity in {{CRISPR-Cas9}} Screens for Essential Non-Coding Elements},
  author       = {Tycko, Josh and Wainberg, Michael and Marinov, Georgi K and Ursu, Oana and Hess, Gaelen T and Ego, Braeden K and Aradhana and Li, Amy and Truong, Alisa and Trevino, Alexandro E and Spees, Kaitlyn and Yao, David and Kaplow, Irene M and Greenside, Peyton G and Morgens, David W and Phanstiel, Douglas H and Snyder, Michael P and Bintu, Lacramioara and Greenleaf, William J and Kundaje, Anshul and Bassik, Michael C},
  date         = {2019-01-18},
  journaltitle = {bioRxiv},
  doi          = {10/gftj39},
  url          = {http://biorxiv.org/lookup/doi/10.1101/520569},
  urldate      = {2019-03-22},
  abstract     = {Pooled CRISPR-Cas9 screens have recently emerged as a powerful method for functionally characterizing regulatory elements in the non-coding genome, but off-target effects in these experiments have not been systematically evaluated. Here, we conducted a genome-scale screen for essential CTCF loop anchors in the K562 leukemia cell line. Surprisingly, the primary drivers of signal in this screen were single guide RNAs (sgRNAs) with low specificity scores. After removing these guides, we found that there were no CTCF loop anchors critical for cell growth. We also observed this effect in an independent screen fine-mapping the core motifs in enhancers of the GATA1 gene. We then conducted screens in parallel with CRISPRi and CRISPRa, which do not induce DNA damage, and found that an unexpected and distinct set of off-targets also caused strong confounding growth effects with these epigenome-editing platforms. Promisingly, strict filtering of CRISPRi libraries using GuideScan specificity scores removed these confounded sgRNAs and allowed for the identification of essential enhancers, which we validated extensively. Together, our results show off-target activity can severely limit identification of essential functional motifs by active Cas9, while strictly filtered CRISPRi screens can be reliably used for assaying larger regulatory elements.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/8A5PIUFC/Tycko et al. - 2019 - Identification and mitigation of pervasive off-tar.pdf}
}

@article{uriaRNADERealvaluedNeural,
  title      = {{{RNADE}}: {{The}} Real-Valued Neural Autoregressive Density-Estimator},
  author     = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  pages      = {9},
  abstract   = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of onedimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradientbased optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Uria et al. - RNADE The real-valued neural autoregressive densi.pdf}
}

@article{vangroningenNeuroblastomaComposedTwo2017,
  title        = {Neuroblastoma Is Composed of Two Super-Enhancer-Associated Differentiation States},
  author       = {family=Groningen, given=Tim, prefix=van, useprefix=true and Koster, Jan and Valentijn, Linda J and Zwijnenburg, Danny A and Akogul, Nurdan and Hasselt, Nancy E and Broekmans, Marloes and Haneveld, Franciska and Nowakowska, Natalia E and Bras, Johannes and family=Noesel, given=Carel J M, prefix=van, useprefix=true and Jongejan, Aldo and family=Kampen, given=Antoine H, prefix=van, useprefix=true and Koster, Linda and Baas, Frank and family=Dijk-Kerkhoven, given=Lianne, prefix=van, useprefix=true and Huizer-Smit, Margriet and Lecca, Maria C and Chan, Alvin and Lakeman, Arjan and Molenaar, Piet and Volckmann, Richard and Westerhout, Ellen M and Hamdi, Mohamed and family=Sluis, given=Peter G, prefix=van, useprefix=true and Ebus, Marli E and Molenaar, Jan J and Tytgat, Godelieve A and Westerman, Bart A and family=Nes, given=Johan, prefix=van, useprefix=true and Versteeg, Rogier},
  date         = {2017-08},
  journaltitle = {Nature Genetics},
  volume       = {49},
  number       = {8},
  pages        = {1261--1266},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gbjz5r},
  url          = {http://www.nature.com/articles/ng.3899},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{vanzylCellAtlasHuman2022,
  title      = {Cell Atlas of the Human Ocular Anterior Segment: {{Tissue-specific}} and Shared Cell Types},
  shorttitle = {Cell Atlas of the Human Ocular Anterior Segment},
  author     = {{van Zyl}, Tav{\'e} and Yan, Wenjun and McAdams, Alexi M. and Monavarfeshani, Aboozar and Hageman, Gregory S. and Sanes, Joshua R.},
  year       = {2022},
  month      = jul,
  journal    = {Proceedings of the National Academy of Sciences},
  volume     = {119},
  number     = {29},
  pages      = {e2200914119},
  publisher  = {Proceedings of the National Academy of Sciences},
  doi        = {10.1073/pnas.2200914119},
  urldate    = {2024-07-19},
  abstract   = {The anterior segment of the eye consists of the cornea, iris, ciliary body, crystalline lens, and aqueous humor outflow pathways. Together, these tissues are essential for the proper functioning of the eye. Disorders of vision have been ascribed to defects in all of them; some disorders, including glaucoma and cataract, are among the most prevalent causes of blindness in the world. To characterize the cell types that compose these tissues, we generated an anterior segment cell atlas of the human eye using high-throughput single-nucleus RNA sequencing (snRNAseq). We profiled 195,248 nuclei from nondiseased anterior segment tissues of six human donors, identifying {$>$}60 cell types. Many of these cell types were discrete, whereas others, especially in the lens and cornea, formed continua corresponding to known developmental transitions that persist in adulthood. Having profiled each tissue separately, we performed an integrated analysis of the entire anterior segment, revealing that some cell types are unique to a single structure, whereas others are shared across tissues. The integrated cell atlas was then used to investigate cell type--specific expression patterns of more than 900 human ocular disease genes identified through either Mendelian inheritance patterns or genome-wide association studies.},
  file       = {/Users/jkobject/Zotero/storage/YDD9DRH4/van Zyl et al. - 2022 - Cell atlas of the human ocular anterior segment T.pdf}
}


@article{varshneyStructuralPropertiesCaenorhabditis2011,
  title        = {Structural {{Properties}} of the {{Caenorhabditis}} Elegans {{Neuronal Network}}},
  author       = {Varshney, Lav R. and Chen, Beth L. and Paniagua, Eric and Hall, David H. and Chklovskii, Dmitri B.},
  editor       = {Sporns, Olaf},
  date         = {2011-02-03},
  journaltitle = {PLoS Computational Biology},
  volume       = {7},
  number       = {2},
  pages        = {e1001066},
  issn         = {1553-7358},
  doi          = {10/dfh2mc},
  url          = {http://dx.plos.org/10.1371/journal.pcbi.1001066},
  urldate      = {2018-04-11},
  abstract     = {Despite recent interest in reconstructing neuronal networks, complete wiring diagrams on the level of individual synapses remain scarce and the insights into function they can provide remain unclear. Even for Caenorhabditis elegans, whose neuronal network is relatively small and stereotypical from animal to animal, published wiring diagrams are neither accurate nor complete and self-consistent. Using materials from White et al. and new electron micrographs we assemble whole, selfconsistent gap junction and chemical synapse networks of hermaphrodite C. elegans. We propose a method to visualize the wiring diagram, which reflects network signal flow. We calculate statistical and topological properties of the network, such as degree distributions, synaptic multiplicities, and small-world properties, that help in understanding network signal propagation. We identify neurons that may play central roles in information processing, and network motifs that could serve as functional modules of the network. We explore propagation of neuronal activity in response to sensory or artificial stimulation using linear systems theory and find several activity patterns that could serve as substrates of previously described behaviors. Finally, we analyze the interaction between the gap junction and the chemical synapse networks. Since several statistical properties of the C. elegans network, such as multiplicity and motif distributions are similar to those found in mammalian neocortex, they likely point to general principles of neuronal networks. The wiring diagram reported here can help in understanding the mechanistic basis of behavior by generating predictions about future experiments involving genetic perturbations, laser ablations, or monitoring propagation of neuronal activity in response to stimulation.},
  langid       = {english},
  annotation   = {00000}
}

@misc{vasilevskyMondoUnifyingDiseases2022,
  title         = {Mondo: {{Unifying}} Diseases for the World, by the World},
  shorttitle    = {Mondo},
  author        = {Vasilevsky, Nicole A. and Matentzoglu, Nicolas A. and Toro, Sabrina and Flack, Joseph E. and Hegde, Harshad and Unni, Deepak R. and Alyea, Gioconda F. and Amberger, Joanna S. and Babb, Larry and Balhoff, James P. and Bingaman, Taylor I. and Burns, Gully A. and Buske, Orion J. and Callahan, Tiffany J. and Carmody, Leigh C. and Cordo, Paula Carrio and Chan, Lauren E. and Chang, George S. and Christiaens, Sean L. and Daugherty, Louise C. and Dumontier, Michel and Failla, Laura E. and Flowers, May J. and Garrett, H. Alpha and Goldstein, Jennifer L. and Gration, Dylan and Groza, Tudor and Hanauer, Marc and Harris, Nomi L. and Hilton, Jason A. and Himmelstein, Daniel S. and Hoyt, Charles Tapley and Kane, Megan S. and K{\"o}hler, Sebastian and Lagorce, David and Lai, Abbe and Larralde, Martin and Lock, Antonia and Santiago, Irene L{\'o}pez and Maglott, Donna R. and Malheiro, Adriana J. and Meldal, Birgit H. M. and {Munoz-Torres}, Monica C. and Nelson, Tristan H. and Nicholas, Frank W. and Ochoa, David and Olson, Daniel P. and Oprea, Tudor I. and {Osumi-Sutherland}, David and Parkinson, Helen and Pendlington, Zo{\"e} May and Rath, Ana and Rehm, Heidi L. and Remennik, Lyubov and Riggs, Erin R. and Roncaglia, Paola and Ross, Justyne E. and Shadbolt, Marion F. and Shefchek, Kent A. and Similuk, Morgan N. and Sioutos, Nicholas and Smedley, Damian and Sparks, Rachel and Stefancsik, Ray and Stephan, Ralf and Storm, Andrea L. and Stupp, Doron and Stupp, Gregory S. and Sundaramurthi, Jagadish Chandrabose and Tammen, Imke and Tay, Darin and Thaxton, Courtney L. and Valasek, Eloise and {Valls-Margarit}, Jordi and Wagner, Alex H. and Welter, Danielle and Whetzel, Patricia L. and Whiteman, Lori L. and Wood, Valerie and Xu, Colleen H. and Zankl, Andreas and Zhang, Xingmin Aaron and Chute, Christopher G. and Robinson, Peter N. and Mungall, Christopher J. and Hamosh, Ada and Haendel, Melissa A.},
  year          = {2022},
  month         = may,
  pages         = {2022.04.13.22273750},
  publisher     = {medRxiv},
  doi           = {10.1101/2022.04.13.22273750},
  urldate       = {2025-02-25},
  abstract      = {There are thousands of distinct disease entities and concepts, each of which are known by different and sometimes contradictory names. The Monarch Initiative aims to integrate genotype, phenotype, and disease knowledge from a large variety of sources in support of improved diagnostics and mechanism discovery through various algorithms and tools. However, the lack of a unified system for managing disease entities poses a major challenge for both machines and humans to predict causes and treatments for disease. The multitude of disease resources have not been well coordinated nor computationally integrated. Furthermore, the classification of phenotypes and their association with diseases is another source of disagreement across sources. The Human Phenotype Ontology has helped to standardize phenotypic features across knowledge sources, but there was no equivalent computationally-harmonized disease ontology. To address these problems, a community of disease resources worked together to create the Mondo Disease Ontology as an open, community-driven ontology that integrates key medical and biomedical terminologies and is iteratively and regularly updated via manual curation and through synchronization with external sources using a Bayesian algorithm. Mondo supports disease data integration to improve diagnosis, treatment, and translational research. It records the sources of all data and is continually updated, making it suitable for research and clinical applications that require up-to-date disease knowledge. Evidence before this study Many disease terminologies currently exist, but there is not a definitive standard for encoding diseases while addressing requirements for information exchange. Existing sources of disease definitions include the National Cancer Institute Thesaurus (NCIt), the Online Mendelian Inheritance in Man (OMIM), Orphanet, SNOMED CT, Disease Ontology (DO), ICD-10, MedGen, and numerous others. Each of these is designed for a particular purpose, and as such has different strengths. However, these standards only partially overlap and often conflict in the classification or mapping approach, making it difficult to align them with each other and/or with other knowledge sources. This need to integrate information has resulted in a proliferation of mappings between disease entries in different resources; these mappings lack completeness, accuracy, and precision, and are often inconsistent between resources.Added value of this study In order to computationally leverage the available knowledge sources for diagnostics and to reveal underlying mechanisms of diseases, we need to understand which terms are meaningfully equivalent across different resources. This will allow integration of associated information, such as treatments, genetics, phenotypes, etc. We therefore created the Mondo Disease Ontology to provide a logic-based structure for unifying multiple disease resources.Implications of all the available evidence Mondo can be leveraged by researchers and clinicians for disease annotations and data integration to aid in clinical diagnosis, treatment and advancement of human health care. Mondo is a freely available, open terminology that contains over 20,000 disease classes. Mondo is iteratively developed with contributions from the intended community and is under continuous revision, with future plans to further revise the top-level classes. Recently, efforts to classify rare diseases have centered on retrieving terms from various sources to provide a unified resource. Mondo can be explored using any of a variety of ontology browsers such as the Ontology Lookup Service (OLS) (ebi.ac.uk/ols/ontologies/mondo), and the ontology files and current releases are available on GitHub (github.com/monarch-initiative/mondo).},
  archiveprefix = {medRxiv},
  copyright     = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/524ZTVAC/Vasilevsky et al. - 2022 - Mondo Unifying diseases for the world, by the wor.pdf}
}

@article{vaskeInferencePatientspecificPathway2010,
  title        = {Inference of Patient-Specific Pathway Activities from Multi-Dimensional Cancer Genomics Data Using {{PARADIGM}}},
  author       = {Vaske, Charles J. and Benz, Stephen C. and Sanborn, J. Zachary and Earl, Dent and Szeto, Christopher and Zhu, Jingchun and Haussler, David and Stuart, Joshua M.},
  date         = {2010-06-15},
  journaltitle = {Bioinformatics},
  volume       = {26},
  number       = {12},
  pages        = {i237-i245},
  issn         = {1460-2059, 1367-4803},
  doi          = {10/bcvgjf},
  url          = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btq182},
  urldate      = {2019-03-22},
  abstract     = {Motivation: High-throughput data is providing a comprehensive view of the molecular changes in cancer tissues. New technologies allow for the simultaneous genome-wide assay of the state of genome copy number variation, gene expression, DNA methylation and epigenetics of tumor samples and cancer cell lines. Analyses of current data sets find that genetic alterations between patients can differ but often involve common pathways. It is therefore critical to identify relevant pathways involved in cancer progression and detect how they are altered in different patients.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/ZURHEU4P/Vaske et al. - 2010 - Inference of patient-specific pathway activities f.pdf}
}

@misc{vaswaniAttentionAllYou2023,
  title         = {Attention {{Is All You Need}}},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year          = {2023},
  month         = aug,
  number        = {arXiv:1706.03762},
  eprint        = {1706.03762},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1706.03762},
  urldate       = {2024-07-10},
  abstract      = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/ZN2NS5ZX/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/jkobject/Zotero/storage/BTYSZA4L/1706.html}
}

@article{vogelsteinFastNonnegativeDeconvolution2010,
  title        = {Fast {{Nonnegative Deconvolution}} for {{Spike Train Inference From Population Calcium Imaging}}},
  author       = {Vogelstein, Joshua T. and Packer, Adam M. and Machado, Timothy A. and Sippy, Tanya and Babadi, Baktash and Yuste, Rafael and Paninski, Liam},
  date         = {2010-12},
  journaltitle = {Journal of Neurophysiology},
  volume       = {104},
  number       = {6},
  pages        = {3691--3704},
  issn         = {0022-3077, 1522-1598},
  doi          = {10/fpddqn},
  url          = {http://www.physiology.org/doi/10.1152/jn.01073.2009},
  urldate      = {2018-04-11},
  abstract     = {Fluorescent calcium indicators are becoming increasingly popular as a means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from a raw fluorescence movie is a nontrivial problem. This work presents a fast non-negative deconvolution filter to infer the approximately most likely spike train of each neuron, given the fluorescence observations. This algorithm outperforms optimal linear deconvolution (Wiener filtering) on both simulated and biological data. The performance gains come from restricting the inferred spike trains to be positive (using an interior-point method), unlike the Wiener filter. The algorithm runs in linear time, like the Wiener filter, and is fast enough that even when imaging over 100 neurons simultaneously, inference can be performed on the set of all observed traces faster than real-time. Performing optimal spatial filtering on the images further refines the inferred spike train estimates. Importantly, all the parameters required to perform the inference can be estimated using only the fluorescence data, obviating the need to perform joint electrophysiological and imaging calibration experiments.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/imaging/Vogelstein et al. - 2010 - Fast Nonnegative Deconvolution for Spike Train Inf.pdf}
}

@misc{wagnerKnearestNeighborSmoothing2018,
  title         = {K-Nearest Neighbor Smoothing for High-Throughput Single-Cell {{RNA-Seq}} Data},
  author        = {Wagner, Florian and Yan, Yun and Yanai, Itai},
  year          = {2018},
  month         = apr,
  primaryclass  = {New Results},
  pages         = {217737},
  publisher     = {bioRxiv},
  doi           = {10.1101/217737},
  urldate       = {2024-07-15},
  abstract      = {High-throughput single-cell RNA-Seq (scRNA-Seq) is a powerful approach for studying heterogeneous tissues and dynamic cellular processes. However, compared to bulk RNA-Seq, single-cell expression profiles are extremely noisy, as they only capture a fraction of the transcripts present in the cell. Here, we propose the k-nearest neighbor smoothing (kNN-smoothing) algorithm, designed to reduce noise by aggregating information from similar cells (neighbors) in a computationally efficient and statistically tractable manner. The algorithm is based on the observation that across protocols, the technical noise exhibited by UMI-filtered scRNA-Seq data closely follows Poisson statistics. Smoothing is performed by first identifying the nearest neighbors of each cell in a step-wise fashion, based on partially smoothed and variance-stabilized expression profiles, and then aggregating their transcript counts. We show that kNN-smoothing greatly improves the detection of clusters of cells and co-expressed genes, and clearly outperforms other smoothing methods on simulated data. To accurately perform smoothing for datasets containing highly similar cell populations, we propose the kNN-smoothing 2 algorithm, in which neighbors are determined after projecting the partially smoothed data onto the first few principal components. We show that unlike its predecessor, kNN-smoothing 2 can accurately distinguish between cells from different T cell subsets, and enables their identification in peripheral blood using unsupervised methods. Our work facilitates the analysis of scRNA-Seq data across a broad range of applications, including the identification of cell populations in heterogeneous tissues and the characterization of dynamic processes such as cellular differentiation. Reference implementations of our algorithms can be found at https://github.com/yanailab/knn-smoothing.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/8VZGUTN9/Wagner et al. - 2018 - K-nearest neighbor smoothing for high-throughput s.pdf}
}

@article{wainbergDeepLearningBiomedicine2018,
  title        = {Deep Learning in Biomedicine},
  author       = {Wainberg, Michael and Merico, Daniele and Delong, Andrew and Frey, Brendan J},
  date         = {2018-09-06},
  journaltitle = {Nature Biotechnology},
  volume       = {36},
  number       = {9},
  pages        = {829--838},
  issn         = {1087-0156, 1546-1696},
  doi          = {10/gd4ws2},
  url          = {http://www.nature.com/doifinder/10.1038/nbt.4233},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{wallaceEntropyInformationGene2019,
  title        = {On Entropy and Information in Gene Interaction Networks},
  author       = {Wallace, Z S and Rosenthal, S B and Fisch, K M and Ideker, T and Sasik, R},
  editor       = {Wren, Jonathan},
  date         = {2019-03-01},
  journaltitle = {Bioinformatics},
  volume       = {35},
  number       = {5},
  pages        = {815--822},
  issn         = {1367-4803, 1460-2059},
  doi          = {10.1093/bioinformatics/bty691},
  url          = {https://academic.oup.com/bioinformatics/article/35/5/815/5068595},
  urldate      = {2019-03-22},
  abstract     = {Motivation: Modern biological experiments often produce candidate lists of genes presumably related to the studied phenotype. One can ask if the gene list as a whole makes sense in the context of existing knowledge: Are the genes in the list reasonably related to each other or do they look like a random assembly? There are also situations when one wants to know if two or more gene sets are closely related. Gene enrichment tests based on counting the number of genes two sets have in common are adequate if we presume that two genes are related only when they are in fact identical. If by related we mean well connected in the interaction network space, we need a new measure of relatedness for gene sets.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/8R4S26Q4/Wallace et al. - 2019 - On entropy and information in gene interaction net.pdf}
}

@unpublished{wallachAtomNetDeepConvolutional2015,
  title       = {{{AtomNet}}: {{A Deep Convolutional Neural Network}} for {{Bioactivity Prediction}} in {{Structure-based Drug Discovery}}},
  shorttitle  = {{{AtomNet}}},
  author      = {Wallach, Izhar and Dzamba, Michael and Heifets, Abraham},
  date        = {2015-10-09},
  eprint      = {1510.02855},
  eprinttype  = {arXiv},
  eprintclass = {cs, q-bio, stat},
  url         = {http://arxiv.org/abs/1510.02855},
  urldate     = {2019-03-22},
  abstract    = {Deep convolutional neural networks comprise a subclass of deep neural networks (DNN) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. Convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models. Although DNNs have been used in drug discovery for QSAR and ligand-based bioactivity predictions, none of these models have benefited from this powerful convolutional architecture. This paper introduces AtomNet, the first structure-based, deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications. We demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. In further contrast to existing DNN techniques, we show that AtomNet’s application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators. Finally, we show that AtomNet outperforms previous docking approaches on a diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9 on 57.8\% of the targets in the DUDE benchmark.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/PLXBNIEP/Wallach et al. - 2015 - AtomNet A Deep Convolutional Neural Network for B.pdf}
}

@article{wangComparativeAnalysisHuman2024,
  title     = {Comparative Analysis of Human and Mouse Transcriptomes during Skin Wound Healing},
  author    = {Wang, Maochun and Zhang, Jiao and Qiao, Chongxu and Yan, Shunchao and Wu, Guoping},
  year      = 2024,
  month     = oct,
  journal   = {Frontiers in Cell and Developmental Biology},
  volume    = {12},
  publisher = {Frontiers},
  issn      = {2296-634X},
  doi       = {10.3389/fcell.2024.1486493},
  urldate   = {2025-12-04},
  abstract  = {Skin wound healing is a complex process which involves multiple molecular events and the underlying mechanism is not fully understood. We presented a comparative transcriptomic analysis of skin wound healing in humans and mice to identify shared molecular mechanisms across species. We analyzed transcriptomes from three distinct stages of the healing process and constructed protein-protein interaction networks to elucidate commonalities in the healing process. A substantial number of differentially expressed genes (DEGs) were identified in human transcriptomes, particularly upregulated genes before and after wound injury, and enriched in processes related to extracellular matrix organization and leukocyte migration. Similarly, the mouse transcriptome revealed thousands of DEGs, with shared biological processes and enriched KEGG pathways, highlighting a conserved molecular signature in skin wound healing. A total of 21 common DEGs were found across human comparisons, and 591 in mouse comparisons, with four genes (KRT2, MARCKSL1, MMP1, and TNC) consistently differentially expressed in both species, suggesting critical roles in mammalian skin wound healing. The expression trends of these genes were consistent, indicating their potential as therapeutic targets. The molecular network analysis identified five subnetworks associated with collagen synthesis, immunity, cell-cell adhesion, and extracellular matrix, with hub genes such as COL4A1, TLR7, TJP3, MMP13, and HIF1A exhibited significant expression changes before and after wound injury in humans and mice. In conclusion, our study provided a detailed molecular network for understanding the healing process in humans and mice, revealing conserved mechanisms that could help the development of targeted therapies across species.},
  langid    = {english},
  keywords  = {comparative analysis,human transcriptomes,molecular network,mouse transcriptomes,skin wound healing},
  file      = {/Users/jkobject/Zotero/storage/JL2F5K48/Wang et al. - 2024 - Comparative analysis of human and mouse transcriptomes during skin wound healing.pdf}
}

@article{wangDataDenoisingTransfer2019,
  title     = {Data Denoising with Transfer Learning in Single-Cell Transcriptomics},
  author    = {Wang, Jingshu and Agarwal, Divyansh and Huang, Mo and Hu, Gang and Zhou, Zilu and Ye, Chengzhong and Zhang, Nancy R.},
  year      = {2019},
  month     = sep,
  journal   = {Nature Methods},
  volume    = {16},
  number    = {9},
  pages     = {875--878},
  publisher = {Nature Publishing Group},
  issn      = {1548-7105},
  doi       = {10.1038/s41592-019-0537-1},
  urldate   = {2024-04-19},
  abstract  = {Single-cell RNA sequencing (scRNA-seq) data are noisy and sparse. Here, we show that transfer learning across datasets remarkably improves data quality. By coupling a deep autoencoder with a Bayesian model, SAVER-X extracts transferable gene-gene relationships across data from different labs, varying conditions and divergent species, to denoise new target datasets.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid    = {english},
  keywords  = {Machine learning,Statistical methods},
  file      = {/Users/jkobject/Zotero/storage/DDD37SRK/Wang et al. - 2019 - Data denoising with transfer learning in single-ce.pdf}
}

@article{wangDecipheringDriverRegulators2023,
  title     = {Deciphering Driver Regulators of Cell Fate Decisions from Single-Cell Transcriptomics Data with {{CEFCON}}},
  author    = {Wang, Peizhuo and Wen, Xiao and Li, Han and Lang, Peng and Li, Shuya and Lei, Yipin and Shu, Hantao and Gao, Lin and Zhao, Dan and Zeng, Jianyang},
  year      = {2023},
  month     = dec,
  journal   = {Nature Communications},
  volume    = {14},
  number    = {1},
  pages     = {8459},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-023-44103-3},
  urldate   = {2024-11-13},
  abstract  = {Single-cell technologies enable the dynamic analyses of cell fate mapping. However, capturing the gene regulatory relationships and identifying the driver factors that control cell fate decisions are still challenging. We present CEFCON, a network-based framework that first uses a graph neural network with attention mechanism to infer a cell-lineage-specific gene regulatory network (GRN) from single-cell RNA-sequencing data, and then models cell fate dynamics through network control theory to identify driver regulators and the associated gene modules, revealing their critical biological processes related to cell states. Extensive benchmarking tests consistently demonstrated the superiority of CEFCON in GRN construction, driver regulator identification, and gene module identification over baseline methods. When applied to the mouse hematopoietic stem cell differentiation data, CEFCON successfully identified driver regulators for three developmental lineages, which offered useful insights into their differentiation from a network control perspective. Overall, CEFCON provides a valuable tool for studying the underlying mechanisms of cell fate decisions from single-cell RNA-seq data.},
  copyright = {2023 The Author(s)},
  langid    = {english},
  keywords  = {Biotechnology,Computational biology and bioinformatics,Control theory,Differentiation,Regulatory networks},
  file      = {/Users/jkobject/Zotero/storage/BDAXC5BM/Wang et al. - 2023 - Deciphering driver regulators of cell fate decisio.pdf}
}

@article{wangDictysDynamicGene2023,
  title      = {Dictys: Dynamic Gene Regulatory Network Dissects Developmental Continuum with Single-Cell Multiomics},
  shorttitle = {Dictys},
  author     = {Wang, Lingfei and Trasanidis, Nikolaos and Wu, Ting and Dong, Guanlan and Hu, Michael and Bauer, Daniel E. and Pinello, Luca},
  year       = {2023},
  month      = sep,
  journal    = {Nature Methods},
  volume     = {20},
  number     = {9},
  pages      = {1368--1378},
  publisher  = {Nature Publishing Group},
  issn       = {1548-7105},
  doi        = {10.1038/s41592-023-01971-3},
  urldate    = {2024-04-18},
  abstract   = {Gene regulatory networks (GRNs) are key determinants of cell function and identity and are dynamically rewired during development and disease. Despite decades of advancement, challenges remain in GRN inference, including dynamic rewiring, causal inference, feedback loop modeling and context specificity. To address these challenges, we develop Dictys, a dynamic GRN inference and analysis method that leverages multiomic single-cell assays of chromatin accessibility and gene expression, context-specific transcription factor footprinting, stochastic process network and efficient probabilistic modeling of single-cell RNA-sequencing read counts. Dictys improves GRN reconstruction accuracy and reproducibility and enables the inference and comparative analysis of context-specific and dynamic GRNs across developmental contexts. Dictys' network analyses recover unique insights in human blood and mouse skin development with cell-type-specific and dynamic GRNs. Its dynamic network visualizations enable time-resolved discovery and investigation of developmental driver transcription factors and their regulated targets. Dictys is available as a free, open-source and user-friendly Python package.},
  copyright  = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid     = {english},
  keywords   = {Epigenomics,Gene regulatory networks,Genomic analysis,Statistical methods,Transcriptomics},
  file       = {/Users/jkobject/Zotero/storage/F8ZR292Q/Wang et al. - 2023 - Dictys dynamic gene regulatory network dissects d.pdf}
}

@misc{wangHierarchicalInterpretationOutofDistribution2024,
  title         = {Hierarchical {{Interpretation}} of {{Out-of-Distribution Cells Using Bottlenecked Transformer}}},
  author        = {Wang, Qifei and Zhu, He and Hu, Yiwen and Chen, Yanjie and Wang, Yuwei and Zhang, Xuegong and Zou, James and Kellis, Manolis and Li, Yue and Liu, Dianbo and Jiang, Lan},
  year          = {2024},
  month         = dec,
  primaryclass  = {New Results},
  pages         = {2024.12.17.628533},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.12.17.628533},
  urldate       = {2025-05-16},
  abstract      = {Identifying the genetic and molecular drivers of phenotypic heterogeneity among individuals is vital for understanding human health and for diagnosing, monitoring, and treating diseases. To this end, international consortia such as the Human Cell Atlas and the Tabula Sapiens are creating comprehensive cellular references. Due to the massive volume of data generated, machine learning methods, especially transformer architectures, have been widely employed in related studies. However, applying machine learning to cellular data presents several challenges. One such challenge is making the methods interpretable with respect to both the input cellular information and its context. Another less explored challenge is the accurate representation of cells outside existing references, referred to as out-of-distribution (OOD) cells. The out-of-distribution could be attributed to various physiological conditions, such as comparing diseased cells, particularly tumor cells, with healthy reference data, or significant technical variations, such as using transfer learning from single-cell reference to spatial query data. Inspired by the global workspace theory in cognitive neuroscience, we introduce CellMemory, a bottlenecked Transformer with improved generalization capabilities designed for the hierarchical interpretation of OOD cells unseen during reference building. Even without pre-training, it exceeds the performance of large language models pre-trained with tens of millions of cells. In particular, when deciphering spatially resolved single-cell transcriptomics data, CellMemory demonstrates the ability to interpret data at the granule level accurately. Finally, we harness CellMemory's robust representational capabilities to elucidate malignant cells and their founder cells in different patients, providing reliable characterizations of the cellular changes caused by the disease.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/NXJVEP9Z/Wang et al. - 2024 - Hierarchical Interpretation of Out-of-Distribution.pdf}
}

@article{wangIdentificationCharacterizationEssential2015,
  title        = {Identification and Characterization of Essential Genes in the Human Genome},
  author       = {Wang, T. and Birsoy, K. and Hughes, N. W. and Krupczak, K. M. and Post, Y. and Wei, J. J. and Lander, E. S. and Sabatini, D. M.},
  date         = {2015-11-27},
  journaltitle = {Science},
  volume       = {350},
  number       = {6264},
  pages        = {1096--1101},
  issn         = {0036-8075, 1095-9203},
  doi          = {10.1126/science.aac7041},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac7041},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/P2P4GZRF/Wang et al. - 2015 - Identification and characterization of essential g.pdf}
}

@article{wangInductiveInferenceGene2020,
  title    = {Inductive Inference of Gene Regulatory Network Using Supervised and Semi-Supervised Graph Neural Networks},
  author   = {Wang, Juexin and Ma, Anjun and Ma, Qin and Xu, Dong and Joshi, Trupti},
  year     = {2020},
  month    = nov,
  journal  = {Computational and Structural Biotechnology Journal},
  volume   = {18},
  pages    = {3335--3343},
  issn     = {2001-0370},
  doi      = {10.1016/j.csbj.2020.10.022},
  urldate  = {2024-07-10},
  abstract = {{$\bullet$}               A novel graph classification formulation in inferring gene regulatory relationships.                                         {$\bullet$}               Graph neural network is powerful to ensemble powers from heuristic skeletons.                                         {$\bullet$}               Our results show GRGRNN outperforms previous methods inductively on benchmarks.                                         {$\bullet$}               GRGNN can be interpreted following the biological network motif hypothesis.                                 , Discovering gene regulatory relationships and reconstructing gene regulatory networks (GRN) based on gene expression data is a classical, long-standing computational challenge in bioinformatics. Computationally inferring a possible regulatory relationship between two genes can be formulated as a link prediction problem between two nodes in a graph. Graph neural network (GNN) provides an opportunity to construct GRN by integrating topological neighbor propagation through the whole gene network. We propose an end-to-end gene regulatory graph neural network (GRGNN) approach to reconstruct GRNs from scratch utilizing the gene expression data, in both a supervised and a semi-supervised framework. To get better inductive generalization capability, GRN inference is formulated as a graph classification problem, to distinguish whether a subgraph centered at two nodes contains the link between the two nodes. A linked pair between a transcription factor (TF) and a target gene, and their neighbors are labeled as a positive subgraph, while an unlinked TF and target gene pair and their neighbors are labeled as a negative subgraph. A GNN model is constructed with node features from both explicit gene expression and graph embedding. We demonstrate a noisy starting graph structure built from partial information, such as Pearson's correlation coefficient and mutual information can help guide the GRN inference through an appropriate ensemble technique. Furthermore, a semi-supervised scheme is implemented to increase the quality of the classifier. When compared with established methods, GRGNN achieved state-of-the-art performance on the DREAM5 GRN inference benchmarks. GRGNN is publicly available at https://github.com/juexinwang/GRGNN.},
  pmcid    = {PMC7677691},
  pmid     = {33294129},
  file     = {/Users/jkobject/Zotero/storage/BUF4CAIV/Wang et al. - 2020 - Inductive inference of gene regulatory network usi.pdf}
}

@unpublished{wangLearningReinforcementLearn2016,
  title       = {Learning to Reinforcement Learn},
  author      = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  date        = {2016-11-17},
  eprint      = {1611.05763},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1611.05763},
  urldate     = {2019-03-22},
  abstract    = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/X5YRXGE5/Wang et al. - 2016 - Learning to reinforcement learn.pdf}
}

@article{wangMultipurposeRNALanguage2024,
  title     = {Multi-Purpose {{RNA}} Language Modelling with Motif-Aware Pretraining and Type-Guided Fine-Tuning},
  author    = {Wang, Ning and Bian, Jiang and Li, Yuchen and Li, Xuhong and Mumtaz, Shahid and Kong, Linghe and Xiong, Haoyi},
  year      = {2024},
  month     = may,
  journal   = {Nature Machine Intelligence},
  volume    = {6},
  number    = {5},
  pages     = {548--557},
  publisher = {Nature Publishing Group},
  issn      = {2522-5839},
  doi       = {10.1038/s42256-024-00836-4},
  urldate   = {2025-02-25},
  abstract  = {Pretrained language models have shown promise in analysing nucleotide sequences, yet a versatile model excelling across diverse tasks with a single pretrained weight set remains elusive. Here we introduce RNAErnie, an RNA-focused pretrained model built upon the transformer architecture, employing two simple yet effective strategies. First, RNAErnie enhances pretraining by incorporating RNA motifs as biological priors and introducing motif-level random masking in addition to masked language modelling at base/subsequence levels. It also tokenizes RNA types (for example, miRNA, lnRNA) as stop words, appending them to sequences during pretraining. Second, subject to out-of-distribution tasks with RNA sequences not seen during the pretraining phase, RNAErnie proposes a type-guided fine-tuning strategy that first predicts possible RNA types using an RNA sequence and then appends the predicted type to the tail of sequence to refine feature embedding in a post hoc way. Our extensive evaluation across seven datasets and five tasks demonstrates the superiority of RNAErnie in both supervised and unsupervised learning. It surpasses baselines with up to 1.8\% higher accuracy in classification, 2.2\% greater accuracy in interaction prediction and 3.3\% improved F1 score in structure prediction, showcasing its robustness and adaptability with a unified pretrained foundation.},
  copyright = {2024 The Author(s)},
  langid    = {english},
  keywords  = {Computational models,Computational science},
  file      = {/Users/jkobject/Zotero/storage/AI6EPC84/Wang et al. - 2024 - Multi-purpose RNA language modelling with motif-aw.pdf}
}

@article{wangSinglecellMultiomeHuman2022b,
  title    = {Single-Cell Multiome of the Human Retina and Deep Learning Nominate Causal Variants in Complex Eye Diseases},
  author   = {Wang, Sean K. and Nair, Surag and Li, Rui and Kraft, Katerina and Pampari, Anusri and Patel, Aman and Kang, Joyce B. and Luong, Christy and Kundaje, Anshul and Chang, Howard Y.},
  year     = {2022},
  month    = aug,
  journal  = {Cell Genomics},
  volume   = {2},
  number   = {8},
  pages    = {100164},
  issn     = {2666-979X},
  doi      = {10.1016/j.xgen.2022.100164},
  urldate  = {2025-01-06},
  abstract = {Genome-wide association studies (GWASs) of eye disorders have identified hundreds of genetic variants associated with ocular disease. However, the vast majority of these variants are noncoding, making it challenging to interpret their function. Here we present a joint single-cell atlas of gene expression and chromatin accessibility of the adult human retina with more than 50,000 cells, which we used to analyze single-nucleotide polymorphisms (SNPs) implicated by GWASs of age-related macular degeneration, glaucoma, diabetic retinopathy, myopia, and type 2 macular telangiectasia. We integrate this atlas with a HiChIP enhancer connectome, expression quantitative trait loci (eQTL) data, and base-resolution deep learning models to predict noncoding SNPs with causal roles in eye disease, assess SNP impact on transcription factor binding, and define their known and novel target genes. Our efforts nominate pathogenic SNP-target gene interactions for multiple vision disorders and provide a potentially powerful resource for interpreting noncoding variation in the eye.},
  keywords = {HiChIP,multiome,retina,single-cell ATAC-seq,single-cell RNA-seq,single-nucleotide polymorphism},
  file     = {/Users/jkobject/Zotero/storage/9IAXHPBR/Wang et al. - 2022 - Single-cell multiome of the human retina and deep .pdf;/Users/jkobject/Zotero/storage/J5AWV2UD/S2666979X22001069.html}
}


@article{wangVicusExploitingLocal2017,
  title        = {Vicus: {{Exploiting}} Local Structures to Improve Network-Based Analysis of Biological Data},
  shorttitle   = {Vicus},
  author       = {Wang, Bo and Huang, Lin and Zhu, Yuke and Kundaje, Anshul and Batzoglou, Serafim and Goldenberg, Anna},
  editor       = {Ideker, Trey},
  date         = {2017-10-12},
  journaltitle = {PLOS Computational Biology},
  volume       = {13},
  number       = {10},
  pages        = {e1005621},
  issn         = {1553-7358},
  doi          = {10/gb368p},
  url          = {https://dx.plos.org/10.1371/journal.pcbi.1005621},
  urldate      = {2019-03-22},
  abstract     = {Biological networks entail important topological features and patterns critical to understanding interactions within complicated biological systems. Despite a great progress in understanding their structure, much more can be done to improve our inference and network analysis. Spectral methods play a key role in many network-based applications. Fundamental to spectral methods is the Laplacian, a matrix that captures the global structure of the network. Unfortunately, the Laplacian does not take into account intricacies of the network’s local structure and is sensitive to noise in the network. These two properties are fundamental to biological networks and cannot be ignored. We propose an alternative matrix Vicus. The Vicus matrix captures the local neighborhood structure of the network and thus is more effective at modeling biological interactions. We demonstrate the advantages of Vicus in the context of spectral methods by extensive empirical benchmarking on tasks such as single cell dimensionality reduction, protein module discovery and ranking genes for cancer subtyping. Our experiments show that using Vicus, spectral methods result in more accurate and robust performance in all of these tasks.},
  langid       = {english},
  annotation   = {00000}
}

@unpublished{weberImaginationAugmentedAgentsDeep2017,
  title       = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author      = {Weber, Théophane and Racanière, Sébastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdomènech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  date        = {2017-07-19},
  eprint      = {1707.06203},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1707.06203},
  urldate     = {2019-03-22},
  abstract    = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/2CA5HQR7/Weber et al. - 2017 - Imagination-Augmented Agents for Deep Reinforcemen.pdf}
}

% Additional references for scPRINT-2 paper

@misc{weiEmergentAbilitiesLarge2022,
  title         = {Emergent {{Abilities}} of {{Large Language Models}}},
  author        = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year          = 2022,
  month         = oct,
  number        = {arXiv:2206.07682},
  eprint        = {2206.07682},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2206.07682},
  urldate       = {2025-12-02},
  abstract      = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/jkobject/Zotero/storage/CMWC7AM5/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf;/Users/jkobject/Zotero/storage/YX8WKF7X/2206.html}
}

@article{wellingBayesianLearningStochastic,
  title      = {Bayesian {{Learning}} via {{Stochastic Gradient Langevin Dynamics}}},
  author     = {Welling, Max and Teh, Yee Whye},
  pages      = {8},
  abstract   = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a “sampling threshold” and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/Welling et Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf}
}

@misc{wencksternAIpoweredVirtualTissues2025,
  title         = {{{AI-powered}} Virtual Tissues from Spatial Proteomics for Clinical Diagnostics and Biomedical Discovery},
  author        = {Wenckstern, Johann and Jain, Eeshaan and Vasilev, Kiril and Pariset, Matteo and Wicki, Andreas and Gut, Gabriele and Bunne, Charlotte},
  year          = {2025},
  month         = jan,
  number        = {arXiv:2501.06039},
  eprint        = {2501.06039},
  primaryclass  = {q-bio},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2501.06039},
  urldate       = {2025-03-27},
  abstract      = {Spatial proteomics technologies have transformed our understanding of complex tissue architectures by enabling simultaneous analysis of multiple molecular markers and their spatial organization. The high dimensionality of these data, varying marker combinations across experiments and heterogeneous study designs pose unique challenges for computational analysis. Here, we present Virtual Tissues (VirTues), a foundation model framework for biological tissues that operates across the molecular, cellular and tissue scale. VirTues introduces innovations in transformer architecture design, including a novel tokenization scheme that captures both spatial and marker dimensions, and attention mechanisms that scale to high-dimensional multiplex data while maintaining interpretability. Trained on diverse cancer and non-cancer tissue datasets, VirTues demonstrates strong generalization capabilities without task-specific fine-tuning, enabling cross-study analysis and novel marker integration. As a generalist model, VirTues outperforms existing approaches across clinical diagnostics, biological discovery and patient case retrieval tasks, while providing insights into tissue function and disease mechanisms.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file          = {/Users/jkobject/Zotero/storage/H6KZKT68/Wenckstern et al. - 2025 - AI-powered virtual tissues from spatial proteomics.pdf;/Users/jkobject/Zotero/storage/JQ3A98FZ/2501.html}
}

@article{WhateverNextPredictive2013,
  title        = {Whatever next? {{Predictive}} Brains, Situated Agents, and the Future of Cognitive Science},
  shorttitle   = {Whatever Next?},
  date         = {2013-06},
  journaltitle = {Behavioral and Brain Sciences},
  volume       = {36},
  number       = {03},
  pages        = {181--204},
  issn         = {0140-525X, 1469-1825},
  doi          = {10/f4xkv5},
  url          = {http://www.journals.cambridge.org/abstract_S0140525X12000477},
  urldate      = {2019-03-22},
  abstract     = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this “hierarchical prediction machine” approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
  langid       = {english}
}

@article{whiteleyAttentionBayesianFramework2012,
  title        = {Attention in a {{Bayesian Framework}}},
  author       = {Whiteley, Louise and Sahani, Maneesh},
  date         = {2012},
  journaltitle = {Frontiers in Human Neuroscience},
  volume       = {6},
  issn         = {1662-5161},
  doi          = {10/gfxbg8},
  url          = {http://journal.frontiersin.org/article/10.3389/fnhum.2012.00100/abstract},
  urldate      = {2019-03-22},
  abstract     = {The behavioral phenomena of sensory attention are thought to reflect the allocation of a limited processing resource, but there is little consensus on the nature of the resource or why it should be limited. Here we argue that a fundamental bottleneck emerges naturally within Bayesian models of perception, and use this observation to frame a new computational account of the need for, and action of, attention – unifying diverse attentional phenomena in a way that goes beyond previous inferential, probabilistic and Bayesian models. Attentional effects are most evident in cluttered environments, and include both selective phenomena, where attention is invoked by cues that point to particular stimuli, and integrative phenomena, where attention is invoked dynamically by endogenous processing. However, most previous Bayesian accounts of attention have focused on describing relatively simple experimental settings, where cues shape expectations about a small number of upcoming stimuli and thus convey “prior” information about clearly defined objects. While operationally consistent with the experiments it seeks to describe, this view of attention as prior seems to miss many essential elements of both its selective and integrative roles, and thus cannot be easily extended to complex environments. We suggest that the resource bottleneck stems from the computational intractability of exact perceptual inference in complex settings, and that attention reflects an evolved mechanism for approximate inference which can be shaped to refine the local accuracy of perception. We show that this approach extends the simple picture of attention as prior, so as to provide a unified and computationally driven account of both selective and integrative attentional phenomena.},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/5RB24T6W/Whiteley and Sahani - 2012 - Attention in a Bayesian Framework.pdf}
}

@article{whyteMasterTranscriptionFactors2013,
  title        = {Master {{Transcription Factors}} and {{Mediator Establish Super-Enhancers}} at {{Key Cell Identity Genes}}},
  author       = {Whyte, Warren~A. and Orlando, David~A. and Hnisz, Denes and Abraham, Brian~J. and Lin, Charles~Y. and Kagey, Michael~H. and Rahl, Peter~B. and Lee, Tong~Ihn and Young, Richard~A.},
  date         = {2013-04},
  journaltitle = {Cell},
  volume       = {153},
  number       = {2},
  pages        = {307--319},
  issn         = {00928674},
  doi          = {10/f4shgq},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0092867413003929},
  urldate      = {2019-03-22},
  abstract     = {Master transcription factors Oct4, Sox2, and Nanog bind enhancer elements and recruit Mediator to activate much of the gene expression program of pluripotent embryonic stem cells (ESCs). We report here that the ESC master transcription factors form unusual enhancer domains at most genes that control the pluripotent state. These domains, which we call super-enhancers, consist of clusters of enhancers that are densely occupied by the master regulators and Mediator. Super-enhancers differ from typical enhancers in size, transcription factor density and content, ability to activate transcription, and sensitivity to perturbation. Reduced levels of Oct4 or Mediator cause preferential loss of expression of super-enhancer-associated genes relative to other genes, suggesting how changes in gene expression programs might be accomplished during development. In other more differentiated cells, superenhancers containing cell-type-specific master transcription factors are also found at genes that define cell identity. Super-enhancers thus play key roles in the control of mammalian cell identity.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/SWGCGMSA/Whyte et al. - 2013 - Master Transcription Factors and Mediator Establis.pdf}
}

@article{wickDeepbinnerDemultiplexingBarcoded2018,
  title        = {Deepbinner: {{Demultiplexing}} Barcoded {{Oxford Nanopore}} Reads with Deep Convolutional Neural Networks},
  shorttitle   = {Deepbinner},
  author       = {Wick, Ryan R. and Judd, Louise M. and Holt, Kathryn E.},
  editor       = {Pertea, Mihaela},
  date         = {2018-11-20},
  journaltitle = {PLOS Computational Biology},
  volume       = {14},
  number       = {11},
  pages        = {e1006583},
  issn         = {1553-7358},
  doi          = {10/gfnq34},
  url          = {http://dx.plos.org/10.1371/journal.pcbi.1006583},
  urldate      = {2019-03-22},
  abstract     = {Multiplexing, the simultaneous sequencing of multiple barcoded DNA samples on a single flow cell, has made Oxford Nanopore sequencing cost-effective for small genomes. However, it depends on the ability to sort the resulting sequencing reads by barcode, and current demultiplexing tools fail to classify many reads. Here we present Deepbinner, a tool for Oxford Nanopore demultiplexing that uses a deep neural network to classify reads based on the raw electrical read signal. This ‘signal-space’ approach allows for greater accuracy than existing ‘base-space’ tools (Albacore and Porechop) for which signals must first be converted to DNA base calls, itself a complex problem that can introduce noise into the barcode sequence. To assess Deepbinner and existing tools, we performed multiplex sequencing on 12 amplicons chosen for their distinguishability. This allowed us to establish a ground truth classification for each read based on internal sequence alone. Deepbinner had the lowest rate of unclassified reads (7.8\%) and the highest demultiplexing precision (98.5\% of classified reads were correctly assigned). It can be used alone (to maximise the number of classified reads) or in conjunction with other demultiplexers (to maximise precision and minimise false positive classifications). We also found cross-sample chimeric reads (0.3\%) and evidence of barcode switching (0.3\%) in our dataset, which likely arise during library preparation and may be detrimental for quantitative studies that use multiplexing. Deepbinner is open source (GPLv3) and available at https://github.com/rrwick/Deepbinner.},
  langid       = {english},
  annotation   = {00000}
}

@online{WikiPathways2024Next,
  title   = {{{WikiPathways}} 2024: Next Generation Pathway Database | {{Nucleic Acids Research}} | {{Oxford Academic}}},
  url     = {https://academic.oup.com/nar/article/52/D1/D679/7369835},
  urldate = {2024-07-26},
  file    = {/Users/jkobject/Zotero/storage/AP7EMW7F/7369835.html}
}

@article{williamsonEquivalenceInformationTheoreticLikelihoodBased2015,
  title        = {The {{Equivalence}} of {{Information-Theoretic}} and {{Likelihood-Based Methods}} for {{Neural Dimensionality Reduction}}},
  author       = {Williamson, Ross S. and Sahani, Maneesh and Pillow, Jonathan W.},
  editor       = {Bethge, Matthias},
  date         = {2015-04-01},
  journaltitle = {PLOS Computational Biology},
  volume       = {11},
  number       = {4},
  pages        = {e1004141},
  issn         = {1553-7358},
  doi          = {10/gfxbg9},
  url          = {http://dx.plos.org/10.1371/journal.pcbi.1004141},
  urldate      = {2019-03-22},
  abstract     = {Stimulus dimensionality-reduction methods in neuroscience seek to identify a lowdimensional space of stimulus features that affect a neuron’s probability of spiking. One popular method, known as maximally informative dimensions (MID), uses an information-theoretic quantity known as “single-spike information” to identify this space. Here we examine MID from a model-based perspective. We show that MID is a maximum-likelihood estimator for the parameters of a linear-nonlinear-Poisson (LNP) model, and that the empirical single-spike information corresponds to the normalized log-likelihood under a Poisson model. This equivalence implies that MID does not necessarily find maximally informative stimulus dimensions when spiking is not well described as Poisson. We provide several examples to illustrate this shortcoming, and derive a lower bound on the information lost when spiking is Bernoulli in discrete time bins. To overcome this limitation, we introduce model-based dimensionality reduction methods for neurons with non-Poisson firing statistics, and show that they can be framed equivalently in likelihood-based or information-theoretic terms. Finally, we show how to overcome practical limitations on the number of stimulus dimensions that MID can estimate by constraining the form of the non-parametric nonlinearity in an LNP model. We illustrate these methods with simulations and data from primate visual cortex.},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/U9R5PEZB/Williamson et al. - 2015 - The Equivalence of Information-Theoretic and Likel.pdf}
}

@article{wiskottSlowFeatureAnalysis2002,
  title        = {Slow {{Feature Analysis}}: {{Unsupervised Learning}} of {{Invariances}}},
  shorttitle   = {Slow {{Feature Analysis}}},
  author       = {Wiskott, Laurenz and Sejnowski, Terrence J.},
  date         = {2002-04},
  journaltitle = {Neural Computation},
  volume       = {14},
  number       = {4},
  pages        = {715--770},
  issn         = {0899-7667, 1530-888X},
  doi          = {10/d36t8h},
  url          = {http://www.mitpressjournals.org/doi/10.1162/089976602317318938},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Dropbox/Journal Club/Wiskott et Sejnowski - 2002 - Slow Feature Analysis Unsupervised Learning of In.pdf}
}

@article{wolfSCANPYLargescaleSinglecell2018,
  title        = {{{SCANPY}}: Large-Scale Single-Cell Gene Expression Data Analysis},
  shorttitle   = {{{SCANPY}}},
  author       = {Wolf, F. Alexander and Angerer, Philipp and Theis, Fabian J.},
  date         = {2018-02-06},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  volume       = {19},
  number       = {1},
  pages        = {15},
  issn         = {1474-760X},
  doi          = {10.1186/s13059-017-1382-0},
  url          = {https://doi.org/10.1186/s13059-017-1382-0},
  urldate      = {2024-07-26},
  abstract     = {Scanpy is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells (https://github.com/theislab/Scanpy). Along with Scanpy, we present AnnData, a generic class for handling annotated data matrices (https://github.com/theislab/anndata).},
  keywords     = {Bioinformatics,Clustering,Differential expression testing,Graph analysis,Machine learning,Pseudotemporal ordering,Scalability,Single-cell transcriptomics,Trajectory inference,Visualization},
  file         = {/Users/jkobject/Zotero/storage/EC35F5I4/Wolf et al. - 2018 - SCANPY large-scale single-cell gene expression da.pdf;/Users/jkobject/Zotero/storage/W9IESANT/s13059-017-1382-0.html}
}

@misc{xiaNatureLMDecipheringLanguage2025,
  title         = {{{NatureLM}}: {{Deciphering}} the {{Language}} of {{Nature}} for {{Scientific Discovery}}},
  shorttitle    = {{{NatureLM}}},
  author        = {Xia, Yingce and Jin, Peiran and Xie, Shufang and He, Liang and Cao, Chuan and Luo, Renqian and Liu, Guoqing and Wang, Yue and Liu, Zequn and Chen, Yuan-Jyue and Guo, Zekun and Bai, Yeqi and Deng, Pan and Min, Yaosen and Lu, Ziheng and Hao, Hongxia and Yang, Han and Li, Jielan and Liu, Chang and Zhang, Jia and Zhu, Jianwei and Wu, Kehan and Zhang, Wei and Gao, Kaiyuan and Pei, Qizhi and Wang, Qian and Liu, Xixian and Li, Yanting and Zhu, Houtian and Lu, Yeqing and Ma, Mingqian and Wang, Zun and Xie, Tian and Maziarz, Krzysztof and Segler, Marwin and Yang, Zhao and Chen, Zilong and Shi, Yu and Zheng, Shuxin and Wu, Lijun and Hu, Chen and Dai, Peggy and Liu, Tie-Yan and Liu, Haiguang and Qin, Tao},
  year          = {2025},
  month         = feb,
  number        = {arXiv:2502.07527},
  eprint        = {2502.07527},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2502.07527},
  urldate       = {2025-02-25},
  abstract      = {Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/S76I8JMZ/Xia et al. - 2025 - NatureLM Deciphering the Language of Nature for S.pdf;/Users/jkobject/Zotero/storage/LHC28JAD/2502.html}
}

@article{xiongHumanSplicingCode2015,
  title        = {The Human Splicing Code Reveals New Insights into the Genetic Determinants of Disease},
  author       = {Xiong, H. Y. and Alipanahi, B. and Lee, L. J. and Bretschneider, H. and Merico, D. and Yuen, R. K. C. and Hua, Y. and Gueroussov, S. and Najafabadi, H. S. and Hughes, T. R. and Morris, Q. and Barash, Y. and Krainer, A. R. and Jojic, N. and Scherer, S. W. and Blencowe, B. J. and Frey, B. J.},
  date         = {2015-01-09},
  journaltitle = {Science},
  volume       = {347},
  number       = {6218},
  pages        = {1254806--1254806},
  issn         = {0036-8075, 1095-9203},
  doi          = {10/f6wzj2},
  url          = {http://www.sciencemag.org/cgi/doi/10.1126/science.1254806},
  urldate      = {2018-04-11},
  abstract     = {Introduction—Advancing whole-genome precision medicine requires understanding how gene expression is altered by genetic variants, especially those that are outside of protein-coding regions. We developed a computational technique that scores how strongly genetic variants alter RNA splicing, a critical step in gene expression whose disruption contributes to many diseases, including cancers and neurological disorders. A genome-wide analysis reveals tens of thousands of variants that alter splicing and are enriched with a wide range of known diseases. Our results provide insight into the genetic basis of spinal muscular atrophy, hereditary nonpolyposis colorectal cancer and autism spectrum disorder.},
  langid       = {english}
}

@article{xiongREDMLNovelEffective2017,
  title        = {{{RED-ML}}: A Novel, Effective {{RNA}} Editing Detection Method Based on Machine Learning},
  shorttitle   = {{{RED-ML}}},
  author       = {Xiong, Heng and Liu, Dongbing and Li, Qiye and Lei, Mengyue and Xu, Liqin and Wu, Liang and Wang, Zongji and Ren, Shancheng and Li, Wangsheng and Xia, Min and Lu, Lihua and Lu, Haorong and Hou, Yong and Zhu, Shida and Liu, Xin and Sun, Yinghao and Wang, Jian and Yang, Huanming and Wu, Kui and Xu, Xun and Lee, Leo J.},
  date         = {2017-05},
  journaltitle = {GigaScience},
  volume       = {6},
  number       = {5},
  pages        = {1--8},
  issn         = {2047-217X},
  doi          = {10/f9vxct},
  url          = {https://academic.oup.com/gigascience/article-lookup/doi/10.1093/gigascience/gix012},
  urldate      = {2018-04-11},
  langid       = {english},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Xiong et al. - 2017 - RED-ML a novel, effective RNA editing detection m.pdf}
}

@inproceedings{xiongScCLIPMultimodalSinglecell2023,
  title     = {scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training},
  author    = {Xiong, Lei and Chen, Tianyi and Kellis, Manolis},
  year      = {2023},
  booktitle = {NeurIPS 2023 Workshop on Machine Learning for Health}
}

@article{xtrimo,
  title      = {{{xTrimoGene}}: {{An Efficient}} and {{Scalable Representation Learner}} for {{Single-Cell RNA-Seq Data}}},
  shorttitle = {{{xTrimoGene}}},
  author     = {Gong, Jing and Hao, Minsheng and Cheng, Xingyi and Zeng, Xin and Liu, Chiming and Ma, Jianzhu and Zhang, Xuegong and Wang, Taifeng and Song, Le},
  year       = {2023},
  month      = dec,
  journal    = {Advances in Neural Information Processing Systems},
  volume     = {36},
  pages      = {69391--69403},
  urldate    = {2024-11-13},
  langid     = {english},
  file       = {/Users/jkobject/Zotero/storage/JX47SJR7/Gong et al. - 2023 - xTrimoGene An Efficient and Scalable Representati.pdf}
}

@unpublished{xueCellDetectionMicroscopy2017,
  title       = {Cell {{Detection}} in {{Microscopy Images}} with {{Deep Convolutional Neural Network}} and {{Compressed Sensing}}},
  author      = {Xue, Yao and Ray, Nilanjan},
  date        = {2017-08-10},
  eprint      = {1708.03307},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/1708.03307},
  urldate     = {2019-03-22},
  abstract    = {The ability to automatically detect certain types of cells or cellular subunits in microscopy images is of significant interest to a wide range of biomedical research and clinical practices. Cell detection methods have evolved from employing hand-crafted features to deep learning-based techniques. The essential idea of these methods is that their cell classifiers or detectors are trained in the pixel space, where the locations of target cells are labeled. In this paper, we seek a different route and propose a convolutional neural network (CNN)-based cell detection method that uses encoding of the output pixel space. For the cell detection problem, the output space is the sparsely labeled pixel locations indicating cell centers. We employ random projections to encode the output space to a compressed vector of fixed dimension. Then, CNN regresses this compressed vector from the input pixels. Furthermore, it is possible to stably recover sparse cell locations on the output pixel space from the predicted compressed vector using L1-norm optimization. In the past, output space encoding using compressed sensing (CS) has been used in conjunction with linear and non-linear predictors. To the best of our knowledge, this is the first successful use of CNN with CSbased output space encoding. We made substantial experiments on several benchmark datasets, where the proposed CNN + CS framework (referred to as CNNCS) achieved the highest or at least top-3 performance in terms of F1-score, compared with other state-of-the-art methods.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/ZNCLD86F/Xue and Ray - 2017 - Cell Detection in Microscopy Images with Deep Conv.pdf}
}

@misc{xuParameterEfficientFineTuning2023,
  title  = {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment},
  author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
  year   = {2023},
  doi    = {10.48550/arXiv.2312.12148}
}

@article{xuProbabilisticHarmonizationAnnotation2021,
  title    = {Probabilistic Harmonization and Annotation of Single-cell Transcriptomics Data with Deep Generative Models},
  author   = {Xu, Chenling and Lopez, Romain and Mehlman, Edouard and Regier, Jeffrey and Jordan, Michael I. and Yosef, Nir},
  year     = 2021,
  month    = jan,
  journal  = {Molecular Systems Biology},
  volume   = {17},
  number   = {1},
  pages    = {MSB20209620},
  issn     = {1744-4292},
  doi      = {10.15252/msb.20209620},
  urldate  = {2025-12-03},
  abstract = {As the number of single-cell transcriptomics datasets grows, the natural next step is to integrate the accumulating data to achieve a common ontology of cell types and states. However, it is not straightforward to compare gene expression levels across datasets and to automatically assign cell type labels in a new dataset based on existing annotations. In this manuscript, we demonstrate that our previously developed method, scVI, provides an effective and fully probabilistic approach for joint representation and analysis of scRNA-seq data, while accounting for uncertainty caused by biological and measurement noise. We also introduce single-cell ANnotation using Variational Inference (scANVI), a semi-supervised variant of scVI designed to leverage existing cell state annotations. We demonstrate that scVI and scANVI compare favorably to state-of-the-art methods for data integration and cell state annotation in terms of accuracy, scalability, and adaptability to challenging settings. In contrast to existing methods, scVI and scANVI integrate multiple datasets with a single generative model that can be directly used for downstream tasks, such as differential expression. Both methods are easily accessible through scvi-tools.},
  langid   = {english},
  keywords = {annotation,differential expression,harmonization,scRNA-seq,variational inference},
  file     = {/Users/jkobject/Zotero/storage/HFH53KM8/Xu et al. - 2021 - Probabilistic harmonization and annotation of single‐cell transcriptomics data with deep generative.pdf}
}

@article{yaminsUsingGoaldrivenDeep2016,
  title        = {Using Goal-Driven Deep Learning Models to Understand Sensory Cortex},
  author       = {Yamins, Daniel L K and DiCarlo, James J},
  date         = {2016-03},
  journaltitle = {Nature Neuroscience},
  volume       = {19},
  number       = {3},
  pages        = {356--365},
  issn         = {1097-6256, 1546-1726},
  doi          = {10/gcsgrw},
  url          = {http://www.nature.com/articles/nn.4244},
  urldate      = {2018-04-11},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/Yamins et DiCarlo - 2016 - Using goal-driven deep learning models to understa.pdf}
}

@article{yangAllelespecificBindingRNAbinding2018,
  title        = {Allele-Specific Binding of {{RNA-binding}} Proteins Reveals Functional Genetic Variants in the {{RNA}}},
  author       = {Yang, Ei-Wen and Bahn, Jae Hoon and Hsiao, Esther Yun-Hua and Tan, Boon Xin and Sun, Yiwei and Fu, Ting and Zhou, Bo and Van Nostrand, Eric L. and Pratt, Gabriel A. and Freese, Peter and Wei, Xintao and Quinones-Valdez, Giovanni and Urban, Alexander E. and Graveley, Brenton R. and Burge, Christopher B. and Yeo, Gene W. and Xiao, Xinshu},
  date         = {2018-08-20},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbfr},
  url          = {http://biorxiv.org/lookup/doi/10.1101/396275},
  urldate      = {2019-03-22},
  abstract     = {Allele-specific protein-RNA binding is an essential aspect that may reveal functional genetic variants influencing RNA processing and gene expression phenotypes. Recently, genome-wide detection of in vivo binding sites of RNA binding proteins (RBPs) is greatly facilitated by the enhanced UV crosslinking and immunoprecipitation (eCLIP) protocol. Hundreds of eCLIP-Seq data sets were generated from HepG2 and K562 cells during the ENCODE3 phase. These data afford a valuable opportunity to examine allele-specific binding (ASB) of RBPs. To this end, we developed a new computational algorithm, called BEAPR (Binding Estimation of Allele-specific Protein-RNA interaction). In identifying statistically significant ASB sites, BEAPR takes into account UV cross-linking induced sequence propensity and technical variations between replicated experiments. Using simulated data and actual eCLIP-Seq data, we show that BEAPR largely outperforms often-used methods Chi-Squared test and Fisher’s Exact test. Importantly, BEAPR overcomes the inherent over-dispersion problem of the other methods. Complemented by experimental validations, we demonstrate that ASB events are significantly associated with genetic regulation of splicing and mRNA abundance, supporting the usage of this method to pinpoint functional genetic variants in post-transcriptional gene regulation. Many variants with ASB patterns of RBPs were found as genetic variants with cancer or other disease relevance. About 38\% of ASB variants were in linkage disequilibrium with single nucleotide polymorphisms from genome-wide association studies. Overall, our results suggest that BEAPR is an effective method to reveal ASB patterns in eCLIP and can inform functional interpretation of diseaserelated genetic variants.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/29Y7HZA4/Yang et al. - 2018 - Allele-specific binding of RNA-binding proteins re.pdf}
}

@article{yangDatabasesWebTools2015,
  title        = {Databases and {{Web Tools}} for {{Cancer Genomics Study}}},
  author       = {Yang, Yadong and Dong, Xunong and Xie, Bingbing and Ding, Nan and Chen, Juan and Li, Yongjun and Zhang, Qian and Qu, Hongzhu and Fang, Xiangdong},
  date         = {2015-02},
  journaltitle = {Genomics, Proteomics \& Bioinformatics},
  volume       = {13},
  number       = {1},
  pages        = {46--50},
  issn         = {16720229},
  doi          = {10/gfxbf5},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S1672022915000066},
  urldate      = {2019-03-22},
  abstract     = {Publicly-accessible resources have promoted the advance of scientific discovery. The era of genomics and big data has brought the need for collaboration and data sharing in order to make effective use of this new knowledge. Here, we describe the web resources for cancer genomics research and rate them on the basis of the diversity of cancer types, sample size, omics data comprehensiveness, and user experience. The resources reviewed include data repository and analysis tools; and we hope such introduction will promote the awareness and facilitate the usage of these resources in the cancer research community.},
  langid       = {english},
  annotation   = {00000}
}

@misc{yangGeneCompassDecipheringUniversal2023,
  title         = {{{GeneCompass}}: {{Deciphering Universal Gene Regulatory Mechanisms}} with {{Knowledge-Informed Cross-Species Foundation Model}}},
  shorttitle    = {{{GeneCompass}}},
  author        = {Yang, Xiaodong and Liu, Guole and Feng, Guihai and Bu, Dechao and Wang, Pengfei and Jiang, Jie and Chen, Shubai and Yang, Qinmeng and Zhang, Yiyang and Man, Zhenpeng and Liang, Zhongming and Wang, Zichen and Li, Yaning and Li, Zheng and Liu, Yana and Tian, Yao and Li, Ao and Dong, Jingxi and Hu, Zhilong and Fang, Chen and Miao, Hefan and Cui, Lina and Deng, Zixu and Jiang, Haiping and Cui, Wentao and Zhang, Jiahao and Yang, Zhaohui and Li, Handong and He, Xingjian and Zhong, Liqun and Zhou, Jiaheng and Wang, Zijian and Long, Qingqing and Xu, Ping and Consortium, The X.-Compass and Wang, Hongmei and Meng, Zhen and Wang, Xuezhi and Wang, Yangang and Wang, Yong and Zhang, Shihua and Guo, Jingtao and Zhao, Yi and Zhou, Yuanchun and Li, Fei and Liu, Jing and Chen, Yiqiang and Yang, Ge and Li, Xin},
  year          = {2023},
  month         = sep,
  primaryclass  = {New Results},
  pages         = {2023.09.26.559542},
  publisher     = {bioRxiv},
  doi           = {10.1101/2023.09.26.559542},
  urldate       = {2024-04-19},
  abstract      = {Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, the traditional research paradigm primarily focuses on individual model organisms, resulting in limited collection and integration of complex features on various cell types across species. Recent breakthroughs in single-cell sequencing and advancements in deep learning techniques present an unprecedented opportunity to tackle this challenge. In this study, we developed GeneCompass, the first knowledge-informed, cross-species foundation model pre-trained on an extensive dataset of over 120 million single-cell transcriptomes from human and mouse. During pre-training, GeneCompass effectively integrates four types of biological prior knowledge to enhance the understanding of gene regulatory mechanisms in a self-supervised manner. Fine-tuning towards multiple downstream tasks, GeneCompass outperforms competing state-of-the-art models in multiple tasks on single species and unlocks new realms of cross-species biological investigation. Overall, GeneCompass marks a milestone in advancing knowledge of universal gene regulatory mechanisms and accelerating the discovery of key cell fate regulators and candidate targets for drug development.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/EHRR9VK9/Yang et al. - 2023 - GeneCompass Deciphering Universal Gene Regulatory.pdf}
}

@article{yangScBERTLargescalePretrained2022a,
  title     = {{{scBERT}} as a Large-Scale Pretrained Deep Language Model for Cell Type Annotation of Single-Cell {{RNA-seq}} Data},
  author    = {Yang, Fan and Wang, Wenchuan and Wang, Fang and Fang, Yuan and Tang, Duyu and Huang, Junzhou and Lu, Hui and Yao, Jianhua},
  year      = 2022,
  month     = oct,
  journal   = {Nature Machine Intelligence},
  volume    = {4},
  number    = {10},
  pages     = {852--866},
  publisher = {Nature Publishing Group},
  issn      = {2522-5839},
  doi       = {10.1038/s42256-022-00534-z},
  urldate   = {2025-12-02},
  abstract  = {Annotating cell types on the basis of single-cell RNA-seq data is a prerequisite for research on disease progress and tumour microenvironments. Here we show that existing annotation methods typically suffer from a lack of curated marker gene lists, improper handling of batch effects and difficulty in leveraging the latent gene--gene interaction information, impairing their generalization and robustness. We developed a pretrained deep neural network-based model, single-cell bidirectional encoder representations from transformers (scBERT), to overcome the challenges. Following BERT's approach to pretraining and fine-tuning, scBERT attains a general understanding of gene--gene interactions by being pretrained on huge amounts of unlabelled scRNA-seq data; it is then transferred to the cell type annotation task of unseen and user-specific scRNA-seq data for supervised fine-tuning. Extensive and rigorous benchmark studies validated the superior performance of scBERT on cell type annotation, novel cell type discovery, robustness to batch effects and model interpretability.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid    = {english},
  keywords  = {Bioinformatics,Classification and taxonomy,Gene expression},
  file      = {/Users/jkobject/Zotero/storage/W43SMWF6/Yang et al. - 2022 - scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-s.pdf}
}

@article{yanSinglecellRNASeqProfiling2013,
  title    = {Single-Cell {{RNA-Seq}} Profiling of Human Preimplantation Embryos and Embryonic Stem Cells},
  author   = {Yan, Liying and Yang, Mingyu and Guo, Hongshan and Yang, Lu and Wu, Jun and Li, Rong and Liu, Ping and Lian, Ying and Zheng, Xiaoying and Yan, Jie and Huang, Jin and Li, Ming and Wu, Xinglong and Wen, Lu and Lao, Kaiqin and Li, Ruiqiang and Qiao, Jie and Tang, Fuchou},
  year     = {2013},
  month    = sep,
  journal  = {Nature structural \& molecular biology},
  volume   = {20},
  number   = {9},
  pages    = {1131--1139},
  issn     = {1545-9985},
  doi      = {10.1038/nsmb.2660},
  urldate  = {2024-07-19},
  abstract = {Measuring gene expression in individual cells is crucial for understanding the gene regulatory network controlling human embryonic development. Here we apply single-cell RNA sequencing (RNA-Seq) analysis to 124 individual cells from human preimplantation embryos and human embryonic stem cells (hESCs) at different passages. The number of maternally expressed genes detected in our data set is 22,687, including 8,701 long noncoding RNAs (lncRNAs), which represents a significant increase from 9,735 maternal genes detected previously by cDNA microarray. We discovered 2,733 novel lncRNAs, many of which are expressed in specific developmental stages. To address the long-standing question whether gene expression signatures of human epiblast (EPI) and in vitro hESCs are the same, we found that EPI cells and primary hESC outgrowth have dramatically different transcriptomes, with 1,498 genes showing differential expression between them. This work provides a comprehensive framework of the transcriptome landscapes of human early embryos and hESCs.},
  langid   = {english},
  pmid     = {23934149}
}

@article{yelinPersonalClinicalHistory2018,
  title        = {Personal Clinical History Predicts Antibiotic Resistance in Urinary Tract Infections},
  author       = {Yelin, Idan and Snitser, Olga and Novich, Gal and Katz, Rachel and Tal, Ofir and Parizade, Miriam and Chodick, Gabriel and Koren, Gideon and Shalev, Varda and Kishony, Roy},
  date         = {2018-08-09},
  journaltitle = {bioRxiv},
  doi          = {10.1101/384842},
  url          = {http://biorxiv.org/lookup/doi/10.1101/384842},
  urldate      = {2019-03-22},
  abstract     = {The prevalence of antibiotic resistance in urinary tract infections (UTIs) often renders the prescribed antimicrobial treatment ineffective, highlighting the need for personalized prediction of resistance at time of care. Here, crossing a 10-year longitudinal dataset of over 700,000 community-acquired UTIs with over 6,000,000 personally-linked records of antibiotic purchases, we show that the resistance profile of infections can be predicted based on patient-specific demographics and clinical history. Age, gender, and retirement home residence had strong, yet differential and even non-monotonic, associations with resistance to different antibiotics. Resistance profiles were also associated with the patient's records of past urine samples and antibiotic usage, with these associations persisting for months and even longer than a year. Drug usage selected specifically for its own cognate resistance, which led indirectly, through genetic linkage, also to resistance to other, even mechanistically unrelated, drugs. Applying machine learning models, these association patterns allowed good personalized predictions of resistance, which could inform and better optimize empirical prescription of antibiotics.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/KEGAGBT6/Yelin et al. - 2018 - Personal clinical history predicts antibiotic resi.pdf}
}

@article{yinEngineeringStemCell2016,
  title        = {Engineering {{Stem Cell Organoids}}},
  author       = {Yin, Xiaolei and Mead, Benjamin~E. and Safaee, Helia and Langer, Robert and Karp, Jeffrey~M. and Levy, Oren},
  date         = {2016-01},
  journaltitle = {Cell Stem Cell},
  volume       = {18},
  number       = {1},
  pages        = {25--38},
  issn         = {19345909},
  doi          = {10/gfxbdv},
  url          = {http://linkinghub.elsevier.com/retrieve/pii/S1934590915005500},
  urldate      = {2018-04-11},
  langid       = {english}
}

@misc{youngblutScBaseCount2025,
  title        = {scBaseCount: an AI agent-curated, uniformly processed, and autonomously updated single cell data repository},
  author       = {Youngblut, Nicholas D. and others},
  year         = {2025},
  howpublished = {bioRxiv},
  doi          = {10.1101/2025.02.27.640494},
  url          = {https://doi.org/10.1101/2025.02.27.640494},
  note         = {Preprint}
}

@article{yuanExactControllabilityComplex2013,
  title        = {Exact Controllability of Complex Networks},
  author       = {Yuan, Zhengzhong and Zhao, Chen and Di, Zengru and Wang, Wen-Xu and Lai, Ying-Cheng},
  date         = {2013-12},
  journaltitle = {Nature Communications},
  volume       = {4},
  number       = {1},
  issn         = {2041-1723},
  doi          = {10/gbdxsv},
  url          = {http://www.nature.com/articles/ncomms3447},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@inproceedings{yuFieldProgrammableDigital2023,
  title     = {Field Programmable Digital Microfluidics Chip for High-Throughput Droplet Array Manipulation},
  booktitle = {2023 {{International Electron Devices Meeting}} ({{IEDM}})},
  author    = {Yu, Jun and Jiang, Shengzhe and Wang, Dongping and Chang, Chunyu and Jia, Zhiqiang and Du, Maohua and Shi, Subao and Li, Jiahao and Dong, Wenfei and Ma, Hanbin and Nathan, Arokia},
  year      = 2023,
  month     = dec,
  pages     = {1--4},
  issn      = {2156-017X},
  doi       = {10.1109/IEDM45741.2023.10413813},
  urldate   = {2025-12-13},
  abstract  = {We present a field programmable active-matrix digital microfluidics chip for biological applications, where we demonstrate high-throughput droplet manipulation, as well as single-cell generation. The fabricated microfluidics chip comprises 640\texttimes 280 pixels that can be independently addressed to execute parallel concurrent droplet generation and single-cell operation. Here, the size of a single pixel is 100\texttimes 100 {$\mu$}m2. The system employs 9T2C GOA circuits to generate scanning signals and novel 3T1C pixel circuits to supply the driving voltage to each pixel. High-resolution digital droplet generation capability is presented, with a droplet splitting success rate of more than 80\% and movement stability of more than 20 hours. The smallest stable single-droplet volume achievable is 1.2 nL, constituting two orders of magnitude smaller than that reported at IEDM previously. In addition, single-cell sorting and recognition of biological (hybridoma) cells were performed with high accuracy owing to a system level optimization of pixel circuits and driving scheme. This work paves the way for new large-scale applications of digital microfluidics for in vitro diagnosis and single-cell-based screening process.},
  keywords  = {Biology,Cells (biology),Circuit stability,Microfluidics,Stability analysis,Throughput,Voltage}
}

@article{yuHighthroughputIdentificationGenotypespecific2016,
  title        = {High-Throughput Identification of Genotype-Specific Cancer Vulnerabilities in Mixtures of Barcoded Tumor Cell Lines},
  author       = {Yu, Channing and Mannan, Aristotle M and Yvone, Griselda Metta and Ross, Kenneth N and Zhang, Yan-Ling and Marton, Melissa A and Taylor, Bradley R and Crenshaw, Andrew and Gould, Joshua Z and Tamayo, Pablo and Weir, Barbara A and Tsherniak, Aviad and Wong, Bang and Garraway, Levi A and Shamji, Alykhan F and Palmer, Michelle A and Foley, Michael A and Winckler, Wendy and Schreiber, Stuart L and Kung, Andrew L and Golub, Todd R},
  date         = {2016-04},
  journaltitle = {Nature Biotechnology},
  volume       = {34},
  number       = {4},
  pages        = {419--423},
  issn         = {1087-0156, 1546-1696},
  doi          = {10/f8g6n2},
  url          = {http://www.nature.com/articles/nbt.3460},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@misc{zaheerDeepSets2018,
  title         = {Deep {{Sets}}},
  author        = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  year          = 2018,
  month         = apr,
  number        = {arXiv:1703.06114},
  eprint        = {1703.06114},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.1703.06114},
  urldate       = {2025-12-03},
  abstract      = {We study the problem of designing models for machine learning tasks defined on \textbackslash emph\textbraceleft sets\textbraceright. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \textbackslash cite\textbraceleft poczos13aistats\textbraceright, to anomaly detection in piezometer data of embankment dams \textbackslash cite\textbraceleft Jung15Exploration\textbraceright, to cosmology \textbackslash cite\textbraceleft Ntampaka16Dynamical,Ravanbakhsh16ICML1\textbraceright. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/9B44KLU2/Zaheer et al. - 2018 - Deep Sets.pdf;/Users/jkobject/Zotero/storage/RCDKRBCQ/1703.html}
}

@article{zaslavskiyGlobalAlignmentProteinprotein2009,
  title        = {Global Alignment of Protein-Protein Interaction Networks by Graph Matching Methods},
  author       = {Zaslavskiy, M. and Bach, F. and Vert, J.-P.},
  date         = {2009-06-15},
  journaltitle = {Bioinformatics},
  volume       = {25},
  number       = {12},
  pages        = {i259-1267},
  issn         = {1367-4803, 1460-2059},
  doi          = {10/bdrvdg},
  url          = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btp196},
  urldate      = {2018-04-11},
  abstract     = {Motivation: Aligning protein–protein interaction (PPI) networks of different species has drawn a considerable interest recently. This problem is important to investigate evolutionary conserved pathways or protein complexes across species, and to help in the identification of functional orthologs through the detection of conserved interactions. It is, however, a difficult combinatorial problem, for which only heuristic methods have been proposed so far. Results: We reformulate the PPI alignment as a graph matching problem, and investigate how state-of-the-art graph matching algorithms can be used for that purpose. We differentiate between two alignment problems, depending on whether strict constraints on protein matches are given, based on sequence similarity, or whether the goal is instead to find an optimal compromise between sequence similarity and interaction conservation in the alignment. We propose new methods for both cases, and assess their performance on the alignment of the yeast and fly PPI networks. The new methods consistently outperform state-of-the-art algorithms, retrieving in particular 78\% more conserved interactions than IsoRank for a given level of sequence similarity.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/bio info/genomics/Zaslavskiy et al. - 2009 - Global alignment of protein-protein interaction ne.pdf}
}

@article{zengCellFMLargescaleFoundation2025,
  title   = {CellFM: a large-scale foundation model pre-trained on transcriptomics of 100 million human cells},
  author  = {Zeng, Yuansong and others},
  journal = {Nature Communications},
  volume  = {16},
  pages   = {4679},
  year    = {2025},
  doi     = {10.1038/s41467-025-59749-2}
}

@misc{zhangComputingHumanInteractome2024,
  title         = {Computing the {{Human Interactome}}},
  author        = {Zhang, Jing and Humphreys, Ian R. and Pei, Jimin and Kim, Jinuk and Choi, Chulwon and Yuan, Rongqing and Durham, Jesse and Liu, Siqi and Choi, Hee-Jung and Baek, Minkyung and Baker, David and Cong, Qian},
  year          = 2024,
  month         = oct,
  primaryclass  = {New Results},
  pages         = {2024.10.01.615885},
  publisher     = {bioRxiv},
  doi           = {10.1101/2024.10.01.615885},
  urldate       = {2025-11-30},
  abstract      = {Protein-protein interactions (PPI) are essential for biological function. Recent advances in coevolutionary analysis and Deep Learning (DL) based protein structure prediction have enabled comprehensive PPI identification in bacterial and yeast proteomes, but these approaches have limited success to date for the more complex human proteome. Here, we overcome this challenge by 1) enhancing the coevolutionary signals with 7-fold deeper multiple sequence alignments harvested from 30 petabytes of unassembled genomic data, and 2) developing a new DL network trained on augmented datasets of domain-domain interactions from 200 million predicted protein structures. These advancements allow us to systematically screen through 200 million human protein pairs and predict 18,316 PPIs with an expected precision of 90\%, among which 5,578 are novel predictions. 3D models of these predicted PPIs nearly triple the number of human PPIs with accurate structural information, providing numerous insights into protein function and mechanisms of human diseases.},
  archiveprefix = {bioRxiv},
  chapter       = {New Results},
  copyright     = {\copyright{} 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid        = {english},
  file          = {/Users/jkobject/Zotero/storage/AWHTBTWG/Zhang et al. - 2024 - Computing the Human Interactome.pdf}
}

@article{zhangInferenceCellTypespecific2023,
  title     = {Inference of Cell Type-Specific Gene Regulatory Networks on Cell Lineages from Single Cell Omic Datasets},
  author    = {Zhang, Shilu and Pyne, Saptarshi and Pietrzak, Stefan and Halberg, Spencer and McCalla, Sunnie Grace and Siahpirani, Alireza Fotuhi and Sridharan, Rupa and Roy, Sushmita},
  year      = {2023},
  month     = may,
  journal   = {Nature Communications},
  volume    = {14},
  number    = {1},
  pages     = {3064},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-023-38637-9},
  urldate   = {2024-07-10},
  abstract  = {Cell type-specific gene expression patterns are outputs of transcriptional gene regulatory networks (GRNs) that connect transcription factors and signaling proteins to target genes. Single-cell technologies such as single cell RNA-sequencing (scRNA-seq) and single cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq), can examine cell-type specific gene regulation at unprecedented detail. However, current approaches to infer cell type-specific GRNs are limited in their ability to integrate scRNA-seq and scATAC-seq measurements and to model network dynamics on a cell lineage. To address this challenge, we have developed single-cell Multi-Task Network Inference (scMTNI), a multi-task learning framework to infer the GRN for each cell type on a lineage from scRNA-seq and scATAC-seq data. Using simulated and real datasets, we show that scMTNI is a broadly applicable framework for linear and branching lineages that accurately infers GRN dynamics and identifies key regulators of fate transitions for diverse processes such as cellular reprogramming and differentiation.},
  copyright = {2023 The Author(s)},
  langid    = {english},
  keywords  = {Bioinformatics,Data integration,Gene regulatory networks,Machine learning,Software},
  file      = {/Users/jkobject/Zotero/storage/9IJTBS56/Zhang et al. - 2023 - Inference of cell type-specific gene regulatory ne.pdf}
}

@article{zhangPredictionHighresolutionHiC2018,
  title        = {Prediction of High-Resolution {{Hi-C}} Interaction Matrices},
  author       = {Zhang, Shilu and Chasman, Deborah and Knaack, Sara and Roy, Sushmita},
  date         = {2018-09-01},
  journaltitle = {bioRxiv},
  doi          = {10/gfxbgj},
  url          = {http://biorxiv.org/lookup/doi/10.1101/406322},
  urldate      = {2019-03-22},
  abstract     = {The three-dimensional organization of the genome plays an important role in gene regulation by enabling distal sequence elements to control the expression level of genes hundreds of kilobases away. Hi-C is a powerful genome-wide technique to measure the contact count of pairs of genomic loci needed to study three-dimensional organization. Due to experimental costs high resolution Hi-C datasets are available only for a handful of cell lines. Computational prediction of Hi-C contact counts can offer a scalable and inexpensive approach to examine three-dimensional genome organization across many cellular contexts. Here we present HiC-Reg, a novel approach to predict contact counts from one-dimensional regulatory signals such as epigenetic marks and regulatory protein binding. HiC-Reg exploits the signal from the region spanning two interacting regions and from across multiple cell lines to generalize to new contexts. Using existing feature importance measures and a new matrix factorization based approach, we found CTCF and chromatin marks, especially repressive and elongation marks, as important for predictive performance. Predicted counts from HiC-Reg identify topologically associated domains as well as significant interactions that are enriched for CTCF bi-directional motifs and agree well with interactions identified from complementary long-range interaction assays. Taken together, HiC-Reg provides a powerful framework to generate high-resolution profiles of contact counts that can be used to study individual locus level interactions as well as higher-order organizational units of the genome.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/JE3YICU2/Zhang et al. - 2018 - prediction of high-resolution Hi-C interaction mat.pdf}
}

@article{zhangSinglecellAnalysisReveals2022,
  title     = {A Single-Cell Analysis Reveals Tumor Heterogeneity and Immune Environment of Acral Melanoma},
  author    = {Zhang, Chao and Shen, Hongru and Yang, Tielong and Li, Ting and Liu, Xinyue and Wang, Jin and Liao, Zhichao and Wei, Junqiang and Lu, Jia and Liu, Haotian and Xiang, Lijie and Yang, Yichen and Yang, Meng and Wang, Duan and Li, Yang and Xing, Ruwei and Teng, Sheng and Zhao, Jun and Yang, Yun and Zhao, Gang and Chen, Kexin and Li, Xiangchun and Yang, Jilong},
  year      = 2022,
  month     = nov,
  journal   = {Nature Communications},
  volume    = {13},
  number    = {1},
  pages     = {7250},
  publisher = {Nature Publishing Group},
  issn      = {2041-1723},
  doi       = {10.1038/s41467-022-34877-3},
  urldate   = {2025-12-03},
  abstract  = {Acral melanoma is a dismal subtype of melanoma occurring in glabrous acral skin, and has a higher incidence in East Asians. We perform single-cell RNA sequencing for 63,394 cells obtained from 5 acral and 3 cutaneous melanoma samples to investigate tumor heterogeneity and immune environment. We define 5 orthogonal functional cell clusters that are involved in TGF-beta signaling, Type I interferon, Wnt signaling, Cell cycle, and Cholesterol efflux signaling. Signatures of enriched TGF-beta, Type I interferon, and cholesterol efflux signaling are significantly associated with good prognosis of melanoma. Compared with cutaneous melanoma, acral melanoma samples have significantly severe immunosuppressive state including depletion of cytotoxic CD8+ T cells, enrichment of Treg cells, and exhausted CD8+ T cells. PD1 and TIM-3 have higher expression in the exhaustive CD8+ T cells of acral melanoma. Key findings are verified in two independent validation sets. This study contributes to our better understanding of acral melanoma.},
  copyright = {2022 The Author(s)},
  langid    = {english},
  keywords  = {Machine learning,Melanoma,Prognostic markers,Transcriptomics,Tumour heterogeneity},
  file      = {/Users/jkobject/Zotero/storage/E9Z9DTF9/Zhang et al. - 2022 - A single-cell analysis reveals tumor heterogeneity and immune environment of acral melanoma.pdf}
}

@misc{zhangSingleCellDataAnalysis2019,
  title  = {Single-Cell Data Analysis Using MMD Variational Autoencoder for a More Informative Latent Representation},
  author = {Zhang, Cheng},
  year   = {2019},
  eprint = {613414},
  doi    = {10.1101/613414}
}

@misc{zhangTahoe100M2025,
  title        = {Tahoe-100M: A Giga-Scale Single-Cell Perturbation Atlas for Context-Dependent Gene Function and Cellular Modeling},
  author       = {Zhang, J. and others},
  year         = {2025},
  howpublished = {bioRxiv},
  doi          = {10.1101/2025.02.20.639398},
  url          = {https://doi.org/10.1101/2025.02.20.639398},
  note         = {Preprint}
}

@article{zhangUnderstandingDeepLearning,
  title      = {Understanding Deep Learning Requires Rethinking Generalization},
  author     = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  pages      = {15},
  abstract   = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Machine Learning},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/Zhang et al. - Understanding deep learning requires rethinking ge.pdf}
}

@inproceedings{zhaoLangCellLanguagecellPretraining2024,
  title     = {LangCell: language-cell pre-training for cell identity understanding},
  author    = {Zhao, Suyuan and Zhang, Jiahuan and Wu, Yizhen and Luo, Yushuai and Nie, Zaiqing},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  volume    = {235},
  pages     = {61159--61185},
  year      = {2024},
  publisher = {JMLR.org}
}

@article{zhaoPolymorphismTumorSuppressor2018,
  title        = {A Polymorphism in the Tumor Suppressor P53 Affects Aging and Longevity in Mouse Models},
  author       = {Zhao, Yuhan and Wu, Lihua and Yue, Xuetian and Zhang, Cen and Wang, Jianming and Li, Jun and Sun, Xiaohui and Zhu, Yiming and Feng, Zhaohui and Hu, Wenwei},
  date         = {2018-03-20},
  journaltitle = {eLife},
  volume       = {7},
  issn         = {2050-084X},
  doi          = {10.7554/eLife.34701},
  url          = {https://elifesciences.org/articles/34701},
  urldate      = {2019-03-22},
  langid       = {english},
  file         = {/Users/jkobject/Zotero/storage/BG7UFB4E/Zhao et al. - 2018 - A polymorphism in the tumor suppressor p53 affects.pdf}
}

@article{zhongBenchmarkingCrossspeciesSinglecell2025,
  title   = {Benchmarking cross-species single-cell RNA-seq data integration methods: towards a cell type tree of life},
  author  = {Zhong, Haoyu and others},
  journal = {Nucleic Acids Research},
  volume  = {53},
  pages   = {gkae1316},
  year    = {2025},
  doi     = {10.1093/nar/gkae1316}
}

@article{zhouDeepLearningSequencebased2018,
  title        = {Deep Learning Sequence-Based Ab Initio Prediction of Variant Effects on Expression and Disease Risk},
  author       = {Zhou, Jian and Theesfeld, Chandra L. and Yao, Kevin and Chen, Kathleen M. and Wong, Aaron K. and Troyanskaya, Olga G.},
  date         = {2018-08},
  journaltitle = {Nature Genetics},
  volume       = {50},
  number       = {8},
  pages        = {1171--1179},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gdvmqw},
  url          = {http://www.nature.com/articles/s41588-018-0160-6},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{zhouDeepSupervisedConvolutional,
  title      = {Deep {{Supervised}} and {{Convolutional Generative Stochastic Network}} for {{Protein Secondary Structure Prediction}}},
  author     = {Zhou, Jian and Troyanskaya, Olga G},
  pages      = {9},
  abstract   = {Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio \& Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of aminoacids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30\% sequence identity. Our model achieves 66.4\% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9\% (Wang et al., 2011) for this challenging secondary structure prediction problem.},
  langid     = {english},
  keywords   = {⛔ No DOI found,Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  annotation = {00000},
  file       = {/Users/jeremie/Documents/science/ML/NN/autoEncoder/Zhou et Troyanskaya - Deep Supervised and Convolutional Generative Stoch.pdf}
}

@misc{zhouDNABERTSPioneeringSpecies2024,
  title         = {{{DNABERT-S}}: {{Pioneering Species Differentiation}} with {{Species-Aware DNA Embeddings}}},
  shorttitle    = {{{DNABERT-S}}},
  author        = {Zhou, Zhihan and Wu, Weimin and Ho, Harrison and Wang, Jiayi and Shi, Lizhen and Davuluri, Ramana V. and Wang, Zhong and Liu, Han},
  year          = {2024},
  month         = oct,
  number        = {arXiv:2402.08777},
  eprint        = {2402.08777},
  primaryclass  = {q-bio},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2402.08777},
  urldate       = {2025-02-06},
  abstract      = {We introduce DNABERT-S, a tailored genome model that develops species-aware embeddings to naturally cluster and segregate DNA sequences of different species in the embedding space. Differentiating species from genomic sequences (i.e., DNA and RNA) is vital yet challenging, since many real-world species remain uncharacterized, lacking known genomes for reference. Embedding-based methods are therefore used to differentiate species in an unsupervised manner. DNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enhance it with the proposed Curriculum Contrastive Learning (C\${\textasciicircum}2\$LR) strategy. Empirical results on 23 diverse datasets show DNABERT-S's effectiveness, especially in realistic label-scarce scenarios. For example, it identifies twice more species from a mixture of unlabeled genomic sequences, doubles the Adjusted Rand Index (ARI) in species clustering, and outperforms the top baseline's performance in 10-shot species classification with just a 2-shot training. Model, codes, and data are publicly available at {\textbackslash}url\{https://github.com/MAGICS-LAB/DNABERT\_S\}.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computational Engineering Finance and Science,Quantitative Biology - Genomics},
  file          = {/Users/jkobject/Zotero/storage/SH2375PI/Zhou et al. - 2024 - DNABERT-S Pioneering Species Differentiation with.pdf;/Users/jkobject/Zotero/storage/3BB9KPS7/2402.html}
}

@article{zhouEfficientAccurateExtraction2018,
  title        = {Efficient and Accurate Extraction of in Vivo Calcium Signals from Microendoscopic Video Data},
  author       = {Zhou, Pengcheng and Resendez, Shanna L and Rodriguez-Romaguera, Jose and Jimenez, Jessica C and Neufeld, Shay Q and Giovannucci, Andrea and Friedrich, Johannes and Pnevmatikakis, Eftychios A and Stuber, Garret D and Hen, Rene and Kheirbek, Mazen A and Sabatini, Bernardo L and Kass, Robert E and Paninski, Liam},
  date         = {2018-02-22},
  journaltitle = {eLife},
  volume       = {7},
  issn         = {2050-084X},
  doi          = {10/gfxbdp},
  url          = {https://elifesciences.org/articles/28728},
  urldate      = {2018-04-11},
  abstract     = {In vivo calcium imaging through microendoscopic lenses enables imaging of previously inaccessible neuronal populations deep within the brains of freely moving animals. However, it is computationally challenging to extract single-neuronal activity from microendoscopic data, because of the very large background fluctuations and high spatial overlaps intrinsic to this recording modality. Here, we describe a new matrix factorization approach to accurately separate the background and then demix and denoise the neuronal signals of interest. We compared the proposed method against widely-used independent components analysis and constrained nonnegative matrix factorization approaches. On both simulated and experimental data, our method substantially improved the quality of extracted cellular signals and detected more well-isolated neural signals, especially in noisy data regimes. These advances can in turn significantly enhance the statistical power of downstream analyses, and ultimately improve scientific conclusions derived from microendoscopic data.},
  langid       = {english},
  keywords     = {⛔ No DOI found,Quantitative Biology - Neurons and Cognition,Quantitative Biology - Quantitative Methods,Statistics - Applications},
  annotation   = {00000},
  file         = {/Users/jeremie/Documents/science/computational neuro/imaging/Zhou et al. - 2018 - Efficient and accurate extraction of in vivo calci.pdf}
}

@article{zhouPredictingEffectsNoncoding2015,
  title        = {Predicting Effects of Noncoding Variants with Deep Learning–Based Sequence Model},
  author       = {Zhou, Jian and Troyanskaya, Olga G},
  date         = {2015-10},
  journaltitle = {Nature Methods},
  volume       = {12},
  number       = {10},
  pages        = {931--934},
  issn         = {1548-7091, 1548-7105},
  doi          = {10/gcgk8g},
  url          = {http://www.nature.com/articles/nmeth.3547},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{zhuSemiSupervisedLearningUsing,
  title      = {Semi-{{Supervised Learning Using Gaussian Fields}} and {{Harmonic Functions}}},
  author     = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
  pages      = {8},
  abstract   = {An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm’s ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.},
  langid     = {english},
  keywords   = {⛔ No DOI found},
  annotation = {00000}
}

@article{zhuTAP1PotentialImmunerelated2023,
  title    = {{{TAP1}}, a Potential Immune-Related Prognosis Biomarker with Functional Significance in Uveal Melanoma},
  author   = {Zhu, Ru and Chen, Yu-Ting and Wang, Bo-Wen and You, Ya-Yan and Wang, Xing-Hua and Xie, Hua-Tao and Jiang, Fa-Gang and Zhang, Ming-Chang},
  year     = {2023},
  month    = feb,
  journal  = {BMC Cancer},
  volume   = {23},
  number   = {1},
  pages    = {146},
  issn     = {1471-2407},
  doi      = {10.1186/s12885-023-10527-9},
  urldate  = {2024-07-25},
  abstract = {TAP1 is an immunomodulation-related protein that plays different roles in various malignancies. This study investigated the transcriptional expression profile of TAP1 in uveal melanoma (UVM), revealed its potential biological interaction network, and determined its prognostic value.},
  keywords = {Immunomodulation,Metastasis,Prognostic biomarkers,TAP1,Uveal melanoma},
  file     = {/Users/jkobject/Zotero/storage/IRACA3LL/Zhu et al. - 2023 - TAP1, a potential immune-related prognosis biomark.pdf;/Users/jkobject/Zotero/storage/6NHWQ3R5/s12885-023-10527-9.html}
}

@article{zollerStructureSilentTranscription2015,
  title        = {Structure of Silent Transcription Intervals and Noise Characteristics of Mammalian Genes},
  author       = {Zoller, B. and Nicolas, D. and Molina, N. and Naef, F.},
  date         = {2015-07-27},
  journaltitle = {Molecular Systems Biology},
  volume       = {11},
  number       = {7},
  pages        = {823--823},
  issn         = {1744-4292},
  doi          = {10/f3czf4},
  url          = {http://msb.embopress.org/cgi/doi/10.15252/msb.20156257},
  urldate      = {2019-03-22},
  abstract     = {Mammalian transcription occurs stochastically in short bursts interspersed by silent intervals showing a refractory period. However, the underlying processes and consequences on fluctuations in gene products are poorly understood. Here, we use single allele time-lapse recordings in mouse cells to identify minimal models of promoter cycles, which inform on the number and durations of rate-limiting steps responsible for refractory periods. The structure of promoter cycles is gene specific and independent of genomic location. Typically, five rate-limiting steps underlie the silent periods of endogenous promoters, while minimal synthetic promoters exhibit only one. Strikingly, endogenous or synthetic promoters with TATA boxes show simplified two-state promoter cycles. Since transcriptional bursting constrains intrinsic noise depending on the number of promoter steps, this explains why TATA box genes display increased intrinsic noise genome-wide in mammals, as revealed by single-cell RNA-seq. These findings have implications for basic transcription biology and shed light on interpreting single-cell RNA-counting experiments.},
  langid       = {english},
  annotation   = {00000},
  file         = {/Users/jkobject/Zotero/storage/NIZ87VB3/Zoller et al. - 2015 - Structure of silent transcription intervals and no.pdf}
}


@unpublished{zophLearningTransferableArchitectures2017,
  title       = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  author      = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  date        = {2017-07-21},
  eprint      = {1707.07012},
  eprinttype  = {arXiv},
  eprintclass = {cs, stat},
  url         = {http://arxiv.org/abs/1707.07012},
  urldate     = {2019-03-22},
  abstract    = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we attempt to automate this engineering process by learning the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. Our key contribution is the design of a new search space which enables transferability. In our experiments, we search for the best convolutional layer (or “cell”) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters. Although the cell is not searched for directly on ImageNet, an architecture constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS – a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of our models exceed those of the state-of-theart human-designed models. For instance, a smaller network constructed from the best cell also achieves 74\% top1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. On CIFAR10, an architecture constructed from the best cell achieves 2.4\% error rate, which is also state-of-the-art. Finally, the image features learned from image classification can also be transferred to other computer vision problems. On the task of object detection, the learned features used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
  langid      = {english},
  keywords    = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation  = {00000},
  file        = {/Users/jkobject/Zotero/storage/N6SGBHPV/Zoph et al. - 2017 - Learning Transferable Architectures for Scalable I.pdf}
}


@article{zouPrimerDeepLearning2019,
  title        = {A Primer on Deep Learning in Genomics},
  author       = {Zou, James and Huss, Mikael and Abid, Abubakar and Mohammadi, Pejman and Torkamani, Ali and Telenti, Amalio},
  date         = {2019-01},
  journaltitle = {Nature Genetics},
  volume       = {51},
  number       = {1},
  pages        = {12--18},
  issn         = {1061-4036, 1546-1718},
  doi          = {10/gfkhmt},
  url          = {http://www.nature.com/articles/s41588-018-0295-5},
  urldate      = {2019-03-22},
  langid       = {english},
  annotation   = {00000}
}

@article{zugastiCARTCellTherapy2025,
  title      = {{{CAR-T}} Cell Therapy for Cancer: Current Challenges and Future Directions},
  shorttitle = {{{CAR-T}} Cell Therapy for Cancer},
  author     = {Zugasti, In{\'e}s and {Espinosa-Aroca}, Lady and Fidyt, Klaudyna and {Mulens-Arias}, Vladimir and {Diaz-Beya}, Marina and Juan, Manel and {Urbano-Ispizua}, {\'A}lvaro and Esteve, Jordi and {Velasco-Hernandez}, Talia and Men{\'e}ndez, Pablo},
  year       = 2025,
  month      = jul,
  journal    = {Signal Transduction and Targeted Therapy},
  volume     = {10},
  number     = {1},
  pages      = {210},
  publisher  = {Nature Publishing Group},
  issn       = {2059-3635},
  doi        = {10.1038/s41392-025-02269-w},
  urldate    = {2025-12-02},
  abstract   = {Chimeric antigen receptor T (CAR-T) cell therapies have transformed the treatment of relapsed/refractory (R/R) B-cell malignancies and multiple myeloma by redirecting activated T cells to CD19- or BCMA-expressing tumor cells. However, this approach has yet to be approved for acute myeloid leukemia (AML), the most common acute leukemia in adults and the elderly. Simultaneously, CAR-T cell therapies continue to face significant challenges in the treatment of solid tumors. The primary challenge in developing CAR-T cell therapies for AML is the absence of an ideal target antigen that is both effective and safe, as AML cells share most surface antigens with healthy hematopoietic stem and progenitor cells (HSPCs). Simultaneously targeting antigen expression on both AML cells and HSPCs may result in life-threatening on-target/off-tumor toxicities such as prolonged myeloablation. In addition, the immunosuppressive nature of the AML tumor microenvironment has a detrimental effect on the immune response. This review begins with a comprehensive overview of CAR-T cell therapy for cancer, covering the structure of CAR-T cells and the history of their clinical application. It then explores the current landscape of CAR-T cell therapy in both hematologic malignancies and solid tumors. Finally, the review delves into the specific challenges of applying CAR-T cell therapy to AML, highlights ongoing global clinical trials, and outlines potential future directions for developing effective CAR-T cell-based treatments for relapsed/refractory AML.},
  copyright  = {2025 The Author(s)},
  langid     = {english},
  keywords   = {Haematological cancer,Immunotherapy},
  file       = {/Users/jkobject/Zotero/storage/L42HM9SE/Zugasti et al. - 2025 - CAR-T cell therapy for cancer current challenges and future directions.pdf}
}

@misc{zuhriSoftpickNoAttention2025,
  title         = {Softpick: {{No Attention Sink}}, {{No Massive Activations}} with {{Rectified Softmax}}},
  shorttitle    = {Softpick},
  author        = {Zuhri, Zayd M. K. and Fuadi, Erland Hilman and Aji, Alham Fikri},
  year          = 2025,
  month         = may,
  number        = {arXiv:2504.20966},
  eprint        = {2504.20966},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2504.20966},
  urldate       = {2025-12-04},
  abstract      = {We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M and 1.8B parameter models demonstrate that softpick achieves 0\textbackslash\% sink rate consistently. The softpick transformers produce hidden states with significantly lower kurtosis and creates sparse attention maps. Quantized models using softpick outperform softmax on standard benchmarks, with a particularly pronounced advantage at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {/Users/jkobject/Zotero/storage/WM3W46PZ/Zuhri et al. - 2025 - Softpick No Attention Sink, No Massive Activations with Rectified Softmax.pdf;/Users/jkobject/Zotero/storage/DZP4JTDF/2504.html}
}

