\titleformat
{\chapter} % command
[display] % shape
{\bfseries\Huge} % format
{ } % label
{2ex} % sep
{
    %\vspace{1ex}
} % before-code
[ \vspace{0ex}
] % after-code

\raggedbottom % Allow flexible page heights to reduce underfull vbox warnings

\chapter[Xpressor: Towards foundation models that learn across biological scales]{Xpressor: Towards foundation models that learn across biological scales}
\label{article2}

\section{Summary}

We have reached a point where many bio foundation models exist across 4 different scales, from molecules to molecular chains, cells, and tissues. However, while related in many ways, these models do not yet bridge these scales. We present a framework and architecture called Xpressor that enables cross-scale learning by (1) using a novel cross-attention mechanism to compress high-dimensional gene representations into lower-dimensional cell-state vectors, and (2) implementing a multi-scale fine-tuning approach that allows cell models to leverage and adapt protein-level representations. Using a \gls{cFM} as an example, we demonstrate that our architecture improves model performance across multiple tasks, including cell-type prediction (+12\%) and embedding quality (+8\%). Together, these advances represent first steps toward models that can understand and bridge different scales of biological organization.

\section{Introduction}

Biology processes information across different scales, from individual molecules to entire tissues. Recent advances in artificial intelligence have led to the development of foundation models that excel at representing biological data at specific scales, such as protein structures \citep{esm2} or cell states \citep{scprint, cuiScGPTBuildingFoundation2024}. However, these models typically operate in isolation, unable to leverage the rich interconnections between different biological scales. Having models that can learn across biological scales will be crucial to capture the complexity of the biological phenomena. 

The main premise of our work is that by using information gained from a lower scale (e.g., molecules), we might improve the input representations of an higher scale phenomena (e.g., cells) \citep{bunneHowBuildVirtual2024, songAIDrivenDigitalOrganism2024}. Reciprocally, using relationships learned at the higher scale, we might improve the lower-scale models too. Finally, we would want to use joint representations of molecules, \gls{DNA}, proteins, cells, and tissues, which are all the elements of the organisms we want to study.

While it is likely infeasible to learn across all scales at once, we might be able to use foundation models that have been trained at specific scales, which we call uniscale models, using only fine-tuning and some architectural changes (see Figure~\ref{fig:scales}). We first review the existing uniscale foundation models in depth for each of the four main biological modalities \citep{siFoundationModelsMolecular2024}.

\subsection{Foundation models across scales}
\label{sec:foundation_models}

\textbf{\glspl{mFM}} try to model with atomistic precision the complex quantum physics-based rules that govern molecules and their interactions \citep{abramsonAccurateStructurePrediction2024}. They generate embeddings of molecules by encoding their chemical representation, often using \gls{SMILES} notation. These embeddings should contain information to predict molecular measurements such as binding to a target, potency, solubility, and more \citep{mendez-lucioMolEFoundationModel2024, rossLargescaleChemicalLanguage2022}. The models are often built with invariances concerning the symmetries of molecules (relative positions and angles) \citep{batznerE3equivariantGraphNeural2022}. These models can also be paired with ones that learn to predict the structure and dynamics of these molecules. Training data in this context is mostly limited by compute since molecular dynamics simulations can be generated at will \citep{kozinskyScalingLeadingAccuracy2023}. The first use cases of such models are in material generation and drug discovery. 

However, computing binding affinities and force-fields similar to the most precise  molecular dynamics methods remains a frontier \citep{benali2025pushingaccuracylimitfoundation, rhodes2025orbv3atomisticsimulationscale}.

\textbf{\glspl{nFM}} are a category of models designed to analyze sequences of nucleotides or amino acids, which are encoded in triplets of nucleotides, primarily using data derived from sequencing across various life forms. Although new architectures have been introduced to handle large context sizes \citep{nguyen2023hyenadnalongrangegenomicsequence}, most models generally rely on traditional transformer models with small context sizes and are trained with masking. These models are based on the transformer architecture and language model techniques (\gls{LLM}) \citep{vaswaniAttentionAllYou2023} to produce representations of the lengthy and repetitive molecular structures found in \gls{DNA} and \gls{RNA}, sometimes termed dnaLM and rnaLM \citep{dalla-torreNucleotideTransformerBuilding2024, wangMultipurposeRNALanguage2024, fradkinOrthrusEvolutionaryFunctional2024, brixiGenomeModelingDesign2025}. 

While protein language models like \gls{ESM}2 \citep{esm2} have shown real-world usage in helping generate 3D models of proteins, dnaLM mainly focused on the task of understanding regulatory mechanisms, such as binding interactions and chemical modifications on \gls{DNA}. It has been shown however, that representations learned by dnaLM can also contain information about the secondary structures of proteins and even protein-protein interactions \citep{brixiGenomeModelingDesign2025, cornmanOMGDatasetOpen2024a}.

For these reasons, we fold protein language models into the \gls{nFM} category, proposing that their distinctions will blur in the future.

Numerous challenges still exist in accurately predicting the diverse conformations of \gls{RNA}, \gls{DNA}, and proteins, as well as in modeling their intricate interactions \citep{abramsonAccurateStructurePrediction2024}. Indeed, it is still hard to measure complexes with the same accuracy as individual proteins.
A goal would be to generate \glspl{nFM} that learn across the very related lexicons, which are \gls{DNA}, \gls{RNA}, and proteins, by introducing architectures and training modalities that go beyond what exists today \citep{xiaNatureLMDecipheringLanguage2025}. Indeed, there we could use the framework of "learning across scales" by using the representations of molecules, learned and compressed by \glspl{mFM}, as the very tokens of \glspl{nFM}, allowing them to talk about ribonucleotides, deoxyribonucleotides, amino acids, and their potential modifications.

Currently, the main applications of \glspl{nFM} have been in drug, and target discovery, as well as many other fields of biology.

\textbf{\glspl{cFM}} are a class of models trained on a matrix of abundances of the different chemical elements (proteins, \gls{RNA}s) present in cells. \citep{bunneHowBuildVirtual2024, scprint, theodorisTransferLearningEnables2023, cuiScGPTBuildingFoundation2024, haoLargescaleFoundationModel2024, rosenUniversalCellEmbeddings2023}. Their architecture is often based on bidirectional encoder-based transformers trained on single-cell \gls{RNA-seq}uencing data. While diverse training strategies have been presented, the model's architectures have, for now, remained fairly classical. The goal of these \glspl{cFM} is to generate an accurate model of the cell that would allow predictions of cell evolution and response to perturbations \citep{kedzierskaAssessingLimitsZeroshot2023}.

However, immense challenges remain. Current promises have not stood up to experimental validations \citep{bendidiBenchmarkingTranscriptomicsFoundation2024,boiarskyDeepDiveSingleCell2023}. While many reasons can be formulated, issues exist around data quality, diversity, and coverage. Indeed, single-cell data is very noisy, only measures a tiny fraction of the molecular composition of cells, and has been mostly produced on human and model animals \citep{programCZCELLxGENEDiscover2023}.
While data will remain an important challenge, an area of improvement would be to, again, distill the rules of molecular interactions from sequence learned at the sequence level onto \glspl{cFM}. This allows them to better learn the complex regulatory mechanisms of the cell.

\textbf{\glspl{tFM}} strive to understand the interactions between cells that form tissues, mostly in higher-order organisms. Often based on imaging techniques, they consider the 2D structural relationship of cells or group of cells in a tissue slice. The stained microscopy slides allow the prediction of tissue type, organs, and even some protein expression levels. These models are often versions of the famous vision transformer architecture and framework (Dino V2), applied to medical images \citep{oquabDINOv2LearningRobust2024, chief}. They thus learn on image patches where each pixel has some channels of information (often from 2 to 30 different chemical elements are represented within these channels) \citep{brayCellPaintingHighcontent2016, wencksternAIpoweredVirtualTissues2025}. The number of channels can go up to tens of thousands in spatial transcriptomics image modalities, where each channel represent a transcripts location at a subcellular level(e.g., xenium) or at a cell-group-level (e.g., visium).

Overall, even more challenges arise in tissue foundation models. Most of the data exists behind institutional barriers, the resolution of high channel count modalities is really poor, while the channel amount of high-resolution modalities is really small, making it hard to predict even the cell state. Slices are often of tiny subparts of tissues. Most of the available data is in 2D slides, and 3D modalities are still burgeoning \citep{alonExpansionSequencingSpatially2021}. We lack good measurements of what cells are communicating, but we know that they do, from sequences to molecules and even entire organelles \citep{hertleHorizontalGenomeTransfer2021}. \glspl{tFM}' vocabulary can be seen as made of cells. Their tokens are cell representations and could be be the rich representations learned by \glspl{cFM}. The goal of a \gls{tFM} is then to predict the presence of cells given other cells in spatial context.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.5\linewidth] {figures/xpressor/scales}
\end{center}
\caption[Representation of the different foundation models]{we show how the representation of different foundation models could feed the upper scales and their learning could inform the lower scales' representations.}
\label{fig:scales}
\end{figure}

\subsection{Architectural modifications: compressed representations}

For biological representations, previous methods have leveraged many different methods from matrix factorization, nearest neighbors, and neural networks \citep{gunawanIntroductionRepresentationLearning2023, bengioRepresentationLearningReview2014a}. Popular approaches are \gls{VAE} such as \gls{scVI} and scArches \citep{scvi, scarches}. In the domain of protein embedding, the HourGlass embedding method \citep{cheap} introduced \gls{FSQ} \citep{mentzerFiniteScalarQuantization2023} as a framework to encode both amino acid sequences and 3D structural information from a \gls{pLM} into a quantized latent space. Meanwhile, \gls{DNA} sequence model embeddings have been mostly restricted to metagenomics, with the exception of DNA-BERT-S \citep{zhouDNABERTSPioneeringSpecies2024}.

Finally, it has been shown not only in biology but also in the \gls{NLP} community that for transformer models, embeddings based on {average,max,sum}-pooling of last-layer tokens are very restrictive and do not perform well \citep{schockaertEmbeddingsEpistemicStates2023, leeNVEmbedImprovedTechniques2025, ilseAttentionbasedDeepMultiple2018}. Indeed, current \gls{SOTA} methods use more complex approaches such as cross-attention mechanism and additional pre-training or fine-tuning tasks. 

In the following, we will show that we can use a similar cross-attention mechanisms to compress the output embeddings of a foundation model into a set of lower-dimensional vectors.

\subsection{Training modifications: fine-tuning}

An extensive literature exists on fine-tuning. The simplest and most powerful approach remains to continue training on a small set of epochs and with a lower learning rate \citep{testft}. Common tools include low-rank approximations of the \gls{MLP} and \gls{QKV} matrices using \gls{LoRA}, QLORA, and COLA \citep{lora, qlora, cola, tangEvaluatingRepresentationalPower2024}, which allow cheap fine-tuning of large foundation models. Other common approaches also mostly revolve around reducing the memory footprint of fine-tuning by only back-propagating the loss across a specific subset of parameters, from updating only specific layers of the model, only the \gls{MLP}s, the \gls{QKV} matrices, or only the biases of the \gls{MLP}s \citep{petersTuneNotTune2019, chronopoulouEmbarrassinglySimpleApproach2019}.
Finally, adapter layers have also been used for their versatility. They often consist of an additional \gls{MLP} on top of the large model's output representations \citep{efficientft}.

In the following, we will show that the adapter layer is a sensible approach to perform multi-scale fine-tuning.

\subsection{Contributions}

Following up on these recent advances, we propose:

\begin{itemize}
    \item A cross-attention "compressor" block whose goal is to compress a foundation model's output embeddings into a small set of low-dimensional vectors, called the \emph{Xpressor} (Cross-Attention Compressor transformer). This is learnt using an auto-encoding approach with a reconstruction loss. The Xpressor is modality agnostic and can be used by \glspl{mFM}, \glspl{nFM}, \glspl{cFM}, \glspl{tFM}, or even other non-biological domains, and can work in addition to other training tasks like masking or denoising (see Figure~\ref{fig:first}A).
    \item A multi-scale fine-tuning approach using adapter layers. This allows the fine-tuning of models from one level using the upper-scale model's task (see Figure~\ref{fig:first}B).
\end{itemize}

\section{Xpressor}

\subsection{Background}

\gls{scPRINT} \citep{scprint} is a foundation model trained on more than 50 million unique single-cell \gls{RNA-seq} profiles, representing around 100B tokens. It learns with a multi-task pre-training loss, allowing \gls{SOTA} zero-shot abilities in denoising and label prediction. \gls{scPRINT} builds on previous foundation models, like \gls{scGPT} \citep{cuiScGPTBuildingFoundation2024} and scFoundation \citep{haoLargescaleFoundationModel2024}. It improves upon them on multiple benchmarks and is also easier to use and faster to train than many other similar models. Additionally, it comes with a gymnasium of benchmarks presented in \citet{scprint}. For these reasons, we chose to use it as our \gls{cFM} and the starting point for our work.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.9\linewidth] {figures/xpressor/first}
\end{center}
\caption[Overview of the Xpressor architecture and multi-scale fine-tuning approach applied to a cell foundation model]{A. The Xpressor architecture, composed of M layers, shows how gene-level representations are compressed into cell-state vectors through cross-attention over the output embeddings of a transformer, composed of N layers. These compressed representations are then decompressed back using the same initial transformer model with cross-attention given the initial gene-level tokens. B. Example of the multi-scale fine-tuning setup illustrating how the adapter layer enables joint training of gene-level representations that are then used by a \gls{cFM}. C. Detailed structure of the transformer and Xpressor blocks showing the cross-attention and self-attention sub-blocks. Blue blocks are our contributions. Shaded blocks indicate inputs and outputs.}
\label{fig:first}
\end{figure}

\gls{ESM}2 \citep{esm2} is a protein language model that learns embeddings of amino acid sequences. It has been shown to be able to learn the evolutionary constraints of proteins and to be able to predict contact maps. Models like ESMfold \citep{esmfold} have been created to predict a protein's 3D structure directly from its output embeddings. It is also simple to use. For these reasons, we chose to use it as our \gls{nFM}.

\subsection{Approach}

Our first contribution is the compression of output embeddings of foundation models using a transformer block and a bottleneck-learning training modality (see Supplementary Material~\ref{sec:tishbi}): we call it the Xpressor (see Figure~\ref{fig:first}A). Compression / decompression is a key mechanism to transfer representations across scales (see Supplementary Material~\ref{sec:foundation_models}), we thus models that can compress and decompress their input into a lower-dimensional space. To do so, we introduce an additional set of transformer blocks called "Xpressor blocks". In the context of \gls{scPRINT}, these blocks represent cell features.

%% input 
As inputs \gls{scPRINT} continues to use a set of summed up gene expression and gene ID tokens. The first ones are generated using an \gls{MLP} on each expression values of genes in a cell $j$, the other ones are generated from \gls{ESM}2's output embeddings of each gene sequenced aggregated with mean-pooling. The newly proposed Xpressor block uses as input a set of learned latent tokens $\mT$. It then performs cross-attention between the last layer of the gene embeddings and the latent tokens (see Figure~\ref{fig:first}A). The goal is for the Xpressor blocks to be of smaller dimensions and context size than the main blocks, such that we end up with $\mC_j$ a set of $n$ tokens of dimension $d_t$ generated from the encoded gene expression and ID matrices $\mE_j$ and $\mG$. Where $\mG$ and $\mE_j$ are sets of $m$ tokens of size $d_c$ representing the IDs of the genes and their corresponding expression in cell $j$, respectively, where $d_c < d_t$ and $n << m$:

%% architecture
\begin{align}
\mO_j &= \mathrm{Transformer}(\mE_j, \mG) \label{eq:transformer} \\
\mC_j &= \mathrm{Xpressor}(\mO_j, \mT) \label{eq:xpressor}
\end{align}

for a cell $j$, with the \emph{Xpressor} being initialized with a learned set of input cell tokens, and $\mC_j$ being the cell tokens associated with the input $\mE_j$.

The \emph{Transformer} and \emph{Xpressor} are both transformer with N and M layers, respectively. Indeed, we have designed both blocks to contain a cross-attention architecture (see Figure~\ref{fig:first}C) such that we can also do: $\hat{\mO}_j = \mathrm{Transformer}(\mC_j, \mG)$, with $\hat{\mO}_j$ being the output of the \emph{Transformer} when using the \emph{Xpressor} representation as input. We add an optional \gls{MLP} after cross-attention to a transformation of the embeddings prior to the self-attention round. In our example, the decompression is done with gene ID tokens as input only ($\mG$) (see Figure~\ref{fig:first}A). These tokens remain the same for all cells of a given species and thus do not depend on $j$. In the context of protein language models, for example, this would be replaced by positional tokens.

As can be seen in Figure~\ref{fig:first}A, the \emph{Transformer} blocks are applied twice. The first application act as an encoder, only using self attention, while the \emph{Xpressor} and second application of the \emph{Transformer} blocks act as decoders. We follow these definitions from the original "Attention is All You Need" paper \citep{vaswaniAttentionAllYou2023}. It has to be noted that in our case cross-attention is performed first instead of last.
Related ideas have also been explored in \citet{leeNVEmbedImprovedTechniques2025}, where the authors propose a cross-attention-based method to update tokens using "latent" embeddings followed by a classical mean-pooling.


%% training
The goal of the \emph{Xpressor} and the entire model can be seen as to perform compression of the gene tokens into a set of cell tokens similar to the classical information bottleneck from \citet{tishbyInformationBottleneckMethoda} (see Supplementary Material~\ref{sec:tishbi}). This is our main training objective to train the \emph{Xpressor} blocks, while the \emph{Transformer} is also trained with masking.

In our case, each embedding represents different cell components. At training time, we present multiple losses to both regularize it and ensure differences across them, similar to what can be done in \gls{VAE}s (see Supplementary Material~\ref{sec:otherlosses}).

\subsection{Results}

We show that such an instantiation of the transformer leads to better performance over the gymnasium of tasks available in the \gls{scPRINT} \gls{cFM}. 

\begin{table}[t]
\caption{Comparison of cell embedding approaches}
\label{table1}
\begin{center}
\begin{tabular}{@{}l@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{}}
\toprule
\textbf{Model} & \begin{tabular}[c]{@{}c@{}}Cell Label\\Pred.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Embed.\\Quality\end{tabular} & \begin{tabular}[c]{@{}c@{}}Gene-Net\\Infer.\end{tabular} \\
\midrule
Class-pooling & 0.60 & 0.48 & \textbf{5.2,2.0} \\
\textbf{Xpressor} & \textbf{0.72} & \textbf{0.52} & 4.1,2.1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Indeed, we now look at three specific tasks: cell-type prediction, embedding quality, and gene-network inference. The tasks are the same as presented in \citet{scprint}. 

"Embedding quality" refers to the average \gls{scIB} \citep{lueckenBenchmarkingAtlaslevelData2022} score for batch correction and biological consistency of cell embeddings. In this context \gls{scIB} looks at the quality of the embeddings based on measures of similarity, nearest neighbors, and clustering. 

Cell-label predictions are generated using a classifier on top of the cell embeddings generated by each model. We follow the approach of \citet{scprint} here, which was recently presented with a different mechanism in \citet{wangHierarchicalInterpretationOutofDistribution2024}. This classification task allows us to see how one can steer the model's embeddings to represent meaningful biological features. 

Finally, we display two different metrics for gene-network inference. The gene network inference benchmark tries to estimate the quality of the self-attention matrices based on similarity to a gene-gene ground-truth matrix. Here we use \gls{EPR}, an odds-ratio measure where, e.g. a value of N means that the predictions are N-times as likely to be correct as a random guess. One is the \gls{EPR} score on the genome-wide perturb-seq gene-network from BenGRN \citep{scprint}, while the second is the average \gls{EPR} of multiple predicted gene-networks across various cell types compared to the BenGRN's omnipath ground truth gene network \citep{tureiOmniPathGuidelinesGateway2016}.

In our comparison, the regular transformer's class-pooling is done similarly to \gls{scGPT}'s \citep{cuiScGPTBuildingFoundation2024} approach, where a class token is added to the model's input and an additional loss is placed on it: $argmin_{C_j}(||E_j - \mC_j \mG_j^T||_2)$. Both models use the same latent dimensions, architectures, training paradigm, and number of input tokens for both genes and cells.

We see that the Xpressor outperforms the simpler class-pooling approach on embedding quality and cell-label prediction, while the gene-network inference results remain roughly similar.

We will now see how we can further train -or fine-tune- these representations using information from the upper scale. While Xpressor layers with their small set of low-dimensional tokens are best suited for this task, we will focus on commonly available foundation models and architectures, presenting a general approach.

\section{Multi-scale Fine-tuning}

\subsection{Background}

To merge foundation models, we need a way to connect the lower-scale models to the upper one. It had been proposed in \citet{rosenUniversalCellEmbeddings2023, scprint} to use protein language model-based representations, like those of \gls{ESM}2, as input tokens for the models. This decreases the number of parameters the model has to learn; It allows the model to work on genes unseen at training time; Moreover, it also lets the model use information that it would not have gained otherwise, such as protein structure, homology, and mutations.

\subsection{Approach}

We propose going beyond simply reusing lower-scale models' representations and fine-tuning them during the pre-training of the upper-scale model using an adapter layer (see Figure~\ref{fig:first}B). With such layer, each output embedding $\ve$ is transformed with a differentiable function $f$ (here, an \gls{MLP}):

\begin{equation}
\vi_{k} = f(\ve_{k})
\label{eq:adapter}
\end{equation}

By using an \gls{MLP}, the adapter layer not only applies a transformation of its input but also adds information (see Supplementary Material~\ref{sec:adapter}). In our case, we use \gls{ESM}2 as the lower-scale model and \gls{scPRINT} as the upper-scale model. The initial \gls{ESM}2 embedding is known to contain a representation of the protein's sequence, evolutionary similarity, and constraints. 

Indeed, this is what allows this representation to replace the \gls{MSA} step in ESMfold \citep{esmfold}. We posit that this initial embedding already contains the information necessary to understand some of the rules in gene interactions (homology and similar evolutionary constraints). However, representations from \gls{ESM}2 are very different from those from single-cell foundation models. Our goal is to enrich these representations with knowledge gained from co-expression information across millions of cells.

\subsection{Results}

We show that a \gls{cFM} trained using the pooled embeddings of a pretrained \gls{nFM} performs better in most tasks from the \citet{scprint} gymnasium benchmark than one with learned representations (see Table~\ref{table2}). This is possible because we allow the model to start from a very rich representation instead of a random set of vectors, while still giving it the flexibility to incorporate additional knowledge. Each foundation model tested uses the same latent dimensions, architectures, training, and number of input tokens. We report the performance at the best epoch, and the training is stopped after 20 epochs.

\begin{table}[t]
\caption{Comparison of input-gene embedding approaches}
\label{table2}
\begin{center}
\begin{tabular}{@{}l@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{}}
\toprule
\textbf{Model} & \begin{tabular}[c]{@{}c@{}}Cell Label\\Pred.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Embed.\\Quality\end{tabular} & \begin{tabular}[c]{@{}c@{}}Gene-Net\\Infer.\end{tabular} \\
\midrule
Random init. & 0.62 & 0.48 & 4.5,1.0 \\
\gls{ESM}2 frozen & 0.60 & 0.48 & 5.2,2.0 \\
\textbf{\gls{ESM}2 fine-tuned} & \textbf{0.70} & \textbf{0.49} & \textbf{4.8,2.4} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

We also show the difference in cell embeddings obtained between the regular transformer and the Xpressor (see Figure~\ref{fig:second}). The dataset is a very challenging mix of modalities with various batch effects and amounts of noise. Cell types are also quite similar, making the task more difficult. We can see that the Xpressor embeddings contain more structure and resolve different cell types better than a transformer with class-pooling.

Using \gls{ESM}2's embeddings allows \gls{scPRINT} to work on genes and sequences unseen at training time, to learn from an unlimited number of species, and to integrate \gls{DNA}, \gls{RNA}, and protein-level information such as mutations and structural variants.

Finally, contrary to other methods, this version does not require an update to the original model and can be added to the new model. Moreover, with this approach, \gls{scPRINT} still maintains its ability to work on genes and sequences unseen at training time.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.9\linewidth] {figures/xpressor/second}
\end{center}
\caption[Comparison of cell embeddings]{between the regular transformer with class-pooling (left), \gls{scIB}: 0.43, and the Xpressor (right), \gls{scIB}: 0.48. The Xpressor embeddings contain more structure and resolve different cell types better.}
\label{fig:second}
\end{figure}

\section*{Conclusion}

We have proposed a framework towards building compositional hierarchical foundation models for life, from atoms to tissues. We highlighted progress and challenges remaining for each specific scale of biological representations. While data generation efforts focusing on breadth and quality remain paramount to progress, we believe that the composition of foundation models could drive progress forward. Having a vocabulary for biological entities will allow us to better reference them, helping us define the impact of a molecule on a tissue or the interaction between \gls{RNA} and proteins. Such a model of life should not be seen as one being trained end-to-end but as a set of models distilling the key information that they have learned and that the next one requires.

We have presented one small piece in this approach, where a cell foundation model (\gls{scPRINT}) uses and fine-tunes a protein sequence foundation model (\gls{ESM}2). We have also shown how XPressor can compress the output representations of transformers into a small set of lower-dimensional vectors, bridging proteins to cells. Such an approach could be used to bridge molecules to proteins and cells to tissues by using compressed representations that are then fine-tuned. This is a promising back-bone architecture for a general model going from atoms to tissues.

Future work should focus on using Xpressor's representations to power upper scale models or the ability to learn a Xpressor on top of a pre-trained foundation model. The Xpressor approach could also be extended to decoder-based language models. Finally, fine-tuning using and adaptor layer suffers from a main drawback, the non-additivity of \gls{MLP}s and therefore the limited use of such fine-tuned models in other contexts than for their compressed representations. Implementing intelligent \gls{GPU} scheduling and using \gls{LoRA}-type methods to fine-tune only XPressor blocks will allow for more complex fine-tuning in \gls{GPU}-rich settings. We will need to show that this can be applied to the other scales of biological representations and generate benchmarks that better capture the diversity of real-world biological tasks across these scales.

\section*{Supplementary}

\subsection{proof that fine-tuning ESM2 with an adapter layer is at least sufficient to learn to add co-expression information}
\label{sec:adapter}

We show below that an \gls{MLP} (with at least one hidden layer and a sufficiently large number of neurons) can learn to map each of \(D\) input protein embeddings to an arbitrary desired output, even if that output corresponds to a unique lookup for each protein.

1. \textbf{Finite Data Interpolation:}
   Let the set of \(D\) protein embeddings be  
   \(\mathcal{E} = \{\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_D\} \subset \mathbb{R}^D\),
   and suppose that for each \(\mathbf{e}_k\) we want the \gls{MLP} to output  
   \(\mathbf{w}_k \in \mathbb{R}^D\).
   Because the set \(\mathcal{E}\) is finite, it is possible to design a function that exactly maps \(\mathbf{e}_k \mapsto \mathbf{w}_k\) for all \(k = 1, \dots, D\).

2. \textbf{Constructive Argument Using ReLU Networks:}
   For a ReLU-based \gls{MLP}, one can construct "bump" functions that are activated only in a small neighborhood around each \(\mathbf{e}_k\). For instance, one may define functions of the form
   \(\mathbf{r}_k(\mathbf{x}) = \sigma\!\left( -\|\mathbf{x}-\mathbf{e}_k\| + \delta \right)\),
   where \(\delta > 0\) is chosen so that \(\mathbf{r}_k(\mathbf{e}_k) > 0\) and \(\mathbf{r}_k(\mathbf{x})\) is nearly zero for \(\mathbf{x}\) that are not close to \(\mathbf{e}_k\). By associating one or more hidden neurons to each protein embedding \(\mathbf{e}_k\), one can form a linear combination
   \(\mathrm{MLP}(\mathbf{x}) = \sum_{k=1}^D c_k\, \mathbf{r}_k(\mathbf{x})\),
   where the coefficients \(c_k \in \mathbb{R}^D\) are chosen so that \(\mathrm{MLP}(\mathbf{e}_k) = \mathbf{w}_k\) for all \(k\). Because the supports of the functions \(\mathbf{r}_k(\mathbf{x})\) can be made nearly disjoint, the \gls{MLP} can "memorize" the mapping by acting as a lookup table.

3. \textbf{Conclusion:}  
   Thus there exists a configuration of weights (and biases) in an \gls{MLP} that yields
   \(\mathrm{MLP}(\mathbf{e}_k) = \mathbf{w}_k,\quad \text{for } k = 1, \dots, D\).
   Hence, even though the \gls{MLP} is simply performing a transformation, its capacity is sufficient to learn any arbitrary mapping for the \(D\) proteins. In other words, at worst, it can learn a mapping that is equivalent to a lookup table, thereby ensuring that each of the \(D\) proteins is assigned a specific, learned output value.

\subsection{argument about the Tishby et al. bottleneck learning approach}
\label{sec:tishbi}

The \gls{IB} method seeks a stochastic mapping \( p(t|x) \) that compresses the input variable \( X \) into a representation \( T \), while preserving as much information as possible about the relevant variable \( Y \). The trade-off is controlled by the Lagrange multiplier \( \beta \geq 0 \). The \gls{IB} objective is to minimize the following Lagrangian:
\begin{equation}
\mathcal{L}_{\text{IB}}[p(t|x)] = I(X;T) - \beta\, I(T;Y),
\label{eq:ib_objective}
\end{equation}
where \( I(\cdot;\cdot) \) denotes mutual information.

Under the Markov constraint 
\(\mathbf{Y} \leftrightarrow \mathbf{X} \leftrightarrow \mathbf{T}\),
the optimization leads to the following self-consistent equations:
\begin{align}
p(t|x) &= \frac{p(t)}{Z(x,\beta)} \exp\left(-\beta\, D_{\mathrm{KL}}\bigl(p(y|x) \,\|\, p(y|t)\bigr)\right), \label{eq:ib_ptx} \\
p(t) &= \sum_{x} p(x)\, p(t|x), \label{eq:ib_pt} \\
p(y|t) &= \frac{1}{p(t)} \sum_{x} p(y|x)\, p(x)\, p(t|x), \label{eq:ib_pyt}
\end{align}
where:
\begin{itemize}
    \item \( D_{\mathrm{KL}}\bigl(p(y|x) \,\|\, p(y|t)\bigr) \) is the Kullback-Leibler divergence between the conditional distributions \( p(y|x) \) and \( p(y|t) \),
    \item \( Z(x,\beta) \) is the normalization factor ensuring that \( \sum_t p(t|x) = 1 \).
\end{itemize}

\subsection{FSQ and other contrastive losses on the cell embeddings}
\label{sec:otherlosses}

While \(D_{\mathrm{KL}}\) over a non-informative Gaussian prior is a common formulation for regularizing the embedding space in \gls{VAE}s, other formulations have been used such as with the \gls{VQ-VAE} and \gls{FSQ-VAE}. In these contexts, the \(D_{\mathrm{KL}}\) is replaced with a discretization objective tailored to the respective quantization schemes.

\paragraph{VQ-VAE.} 
Value Quantized (VQ)-VAE employ a \emph{codebook} of size \(C\), where each codebook entry is a \(d\)-dimensional vector. The encoder produces a continuous latent vector, which is then mapped to its nearest entry in the codebook (a hard quantization). A commitment loss term encourages the encoder's outputs to stay close to the chosen codebook vector, making the entire latent representation discrete at the vector level.

\paragraph{FSQ-VAE.}
By contrast, Finite Scalar Quantization (FSQ)-VAE discretizes each latent dimension \emph{independently}. Specifically, the encoder outputs \(d\) values, each constrained to lie within a bounded range (e.g., \([-1, 1]\)). Each dimension is then quantized into one of \(M\) discrete levels within that range. This dimension-wise quantization can be implemented as either a hard nearest-bin assignment or a differentiable approximation thereof. Because \gls{FSQ} enforces scalar-level discretization, it provides a simpler and more fine-grained alternative to VQ's vector-level codebook approach, while still offering strong regularization of the latent space.

\paragraph{Contrastive regularization across embedding dimensions.}
We further encourage each of the \(d\) embedding dimensions to encode distinct information by adding a contrastive loss between them. Specifically, we compute pairwise similarities among embedding elements and penalize redundancy, thus pushing each dimension to capture complementary features. A general contrastive loss for this purpose can be written as
\begin{equation}
\mathcal{L}_{\text{contrastive}} = \sum_{i=1}^{d} \sum_{j \neq i} \ell\bigl(\mathbf{e}_i, \mathbf{e}_j\bigr),
\label{eq:contrastive}
\end{equation}
where \(\mathbf{e}_i\) denotes the \(i\)-th embedding dimension and \(\ell\) is a contrastive loss function (e.g., InfoNCE \citep{oordRepresentationLearningContrastive2019}) that encourages \emph{dissimilarity} among different embedding components.

\paragraph{Dimension-specific classifiers.}
To further steer each dimension's content, one can add a separate classifier on top of each dimension to learn about different classes. The classifier for dimension \(i\) is trained via a cross-entropy loss
\begin{equation}
\mathcal{L}_{\text{cls}}^{(i)} = - \sum_{c} y_{c} \log p\bigl(c \mid \mathbf{e}_i\bigr),
\label{eq:cls_loss_i}
\end{equation}
where \(y_c\) is the ground-truth label and \(p\bigl(c \mid \mathbf{e}_i\bigr)\) is the predicted probability for class \(c\). Summing these per-dimension losses yields an overall classification objective
\begin{equation}
\mathcal{L}_{\text{cls}} = \sum_{i=1}^{d} \mathcal{L}_{\text{cls}}^{(i)}.
\label{eq:cls_loss_total}
\end{equation}
Together, the contrastive and classification losses ensure each embedding dimension captures unique, discriminative information, resulting in more expressive representations.

\subsection*{Software and Data}

The software and data for training scPRINT as well as gymnasium tasks and code to reproduce the results of the manuscript are available at \url{https://github.com/cantinilab/XPressor}.

WandB logs, are available in the following link:
\url{https://api.wandb.ai/links/ml4ig/h370j6io}

Model checkpoints are available in the following link:
\url{https://huggingface.co/jkobject/scPRINT/tree/main}

\subsection*{Acknowledgments}

The project leading to this manuscript has received funding from the Inception program (Investissement d'Avenir grant ANR-16-CONV-0005) L.C. and the European Union (ERC StG, MULTIview-CELL, 101115618) L.C. We acknowledge the help of the HPC Core Facility of the Institut Pasteur and Déborah Philipps for the administrative support. L.C.

The work of G. Peyré was supported by the French government under management of Agence Nationale de la Recherche as part of the 'Investissements d'avenir' program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute). G.P.

\subsection*{Impact Statement}

This paper presents work whose goal is to advance the fields of computational biology and machine learning. No ethical issues are raised by the work other than what is typically noted in computational biology and foundation model papers. It might have an impact on building better models for drug discovery, target discovery, and improving our understanding of biological systems.