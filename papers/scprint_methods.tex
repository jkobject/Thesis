\section{Methods}
\label{sec:methodsprint}

we propose scPRINT, a foundation model designed for gene network inference. ScPRINT brings novel inductive biases and pretraining strategies better suited to GN inference while answering issues in current models. scPrint outputs cell type-specific genome-wide gene networks but also generates predictions on many related tasks, such as cell annotations, batch effect correction, and denoising, without fine-tuning.

\subsection{Architecture}\label{architecture}

The model architecture is composed of:

\begin{itemize}
\item
  An encoder that takes the raw data and embeds it in a high-dimensional
  space used by the transformer.
\item
  A bidirectional multi-head transformer
\item
  A decoder to transform the expression embeddings into expression
  values
\item
  A decoder that transforms the cell embeddings into cell-specific label
  prediction over a range of classes.
\end{itemize}

\subsubsection{Expression encoder}\label{expression-encoder}

In scPRINT, each gene in a cell is converted to an embedding: It
corresponds to the sum of 3 different elements:

1. An embedding representing the gene itself (see Supplementary Table \href{table-s1-list-of-novelties-in-scprint-and-comparison-to-scgpt-and-scfoundation} for model embedding size). ESM2 embedding of each gene's most common protein product was used to represent that gene. While imperfect in some ways, this inductive bias allows the model to learn representations that potentially apply to even unseen genes from unseen species or integrate specific genetic mutations into its representation. First implemented in UCE, this provides the model information related to the gene product's structure, ontology, and similarity to other genes. This also speeds up the training greatly, particularly for small models. We show that this is a great gene representation, but that model performance can be increased by refining gene embeddings further during training. However, we elect not to do so to maintain the model's versatility in working on unseen genes.

We encode the genes' embeddings using ESM2. The mapping process happens the following way:

\begin{itemize}
\item
  A gene name is mapped to its canonical protein name using Ensembl.
\item
  We recover the protein sequence of the protein using Ensembl
\item
  We use the protein sequence to generate an embedding using ESM2 by averaging all the amino-acid output embeddings, as done in the ESM2 paper.
\end{itemize}

With the embedding function provided in our code, one can easily do this with any species in Ensembl.

scPRINT can effectively be retrained with any set of gene embeddings, which can be frozen during training or used only for initialization (tried, for example, in our ablation studies, Table S3).

2. An embedding of the gene location in the genome. This has also been proposed in UCE and helps the model understand that genes with similar locations tend to be regulated by similar regulatory regions, a relationship well-known in cellular biology.

We encode the genes' locations using positional encoding. Every gene less than 10,000 bp from the next is said to be in the same location; otherwise, we increment location by 1. We do this for all genes in the Ensembl database per species.

We then embed these locations by applying the Positional Encoding (PE) algorithm of Vaswani et al. .

3. An embedding of the gene expression in the cell. For this, we embed
the gene's expression using an MLP. While GeneFormer devised a ranking
strategy based on a gene expression compared to a baseline expression,
scGPT instead used binning of log normalized counts. On our end, we
haven't found that this approach was the simplest, nor was it performing
better than only using the log-transformed counts. We thus directly take
the log-transformed counts

\begin{equation}
\mathbf{e}_{\mathbf{i,j}} = MLP(log_{2}(x_{i,j} + 1)),\ x_{i,j} \in \mathbb{R},\ \mathbf{e}_{\mathbf{i,j}} \in \mathbb{R}^{d}
\label{eq:expression_embedding}
\end{equation}

where \(\exp r_{i,j}\ \)is the embedding of the expression, \(x_{i,j}\)
is the expression value of the gene j in the cell i, and the MLP is a
two-layer neural network, where each layer is composed of

\begin{equation}
Dropout(ReLU(LayerNorm(Linear(\mathbf{e}_{\mathbf{i,j}}))))
\label{eq:mlp_layer}
\end{equation}

where the Dropout rate is fixed at 0.1, and the dimensions are specified
as 1 → \emph{d} for the first layer of the MLP and \emph{d → d} for the
second layer, with d representing the model dimension.

Of Note: Geneformer used positional encoding to encode gene expression,
a function often used to encode the position of words in a text.
Similarly to gene name token, scGPT learned an embedding for different
ranges of expression values, binning them to remove sampling noise.

Both approaches apply a specific prior for the metric that defines
expression. Geneformer defines expression amount as ranking based on how
each gene is expressed in the cell compared to its average across all
cells. Unregarding the batch effect issues, this is an assumption that
expression values are not meaningful and only the ranking of the
relative abundance is meaningful information. Meanwhile, scGPT has the
bias that an expression of 1, 2, or 3 are the same and that an
expression 1, and 5 are different by some amount learned by the model.

By using an MLP with two layers, we effectively let the model learn the
metric of transcription expression. Moreover, again, we decrease the
number of parameters used compared to scGPT while being able to make
predictions on count values unseen during training, such as those of
bulk or pseudo-bulk RNAseq.

Finally, when encoding a cell expression profile, only a subset of 2200
genes is used during pretraining. If less than 2200 genes are expressed,
we randomly choose 2200 expressed genes and pad them with randomly
sampled unexpressed genes (meaning with an expression value of 0). This
approach allows the model to see different patches of the same cell
profile during training. We chose 2200 genes as 2/3rds of the cells in
cellxgene had less than this number of genes expressed, striking a
balance between computation and gene usage.

We decided to add unexpressed genes because, combined with our denoising
methodology, this lets the model figure out that some genes are true 0s
during training. In contrast, others are only caused by dropout and a
function of the transcript counts. This causes scPRINT to model dropout
as a function of read depth (i.e., total transcript count).

Moreover, this completes the minibatch by token matrix without padding
and fully utilizes the GPU during the attention computation.

Of note, some models have been able to reach context lengths of 20,000
genes using the performer architecture. Performer is an often-cited
method and part of the literature on attention approximation. However,
most state-of-the-art transformer models do not use attention
approximation as they are known to lead to worse
performance.

Moreover, in cellxgene, more than 80\% of the cells have less than 2200
genes being measured. This means that most of the memory and compute
power is likely lost on tokens that are almost always zeros due to
dropout.

The full set of embeddings of cell i sent to the transformer is the
matrix \(X_{i}\) where

\begin{equation}
X_{i} = [\mathbf{g}_{\mathbf{0}} + \mathbf{e}_{\mathbf{i,0}} + \mathbf{l}_{\mathbf{0}}, \mathbf{g}_{\mathbf{1}} + \mathbf{e}_{\mathbf{i,1}} + \mathbf{l}_{\mathbf{1}}, ..., \mathbf{e}_{\mathbf{i,t}}, \mathbf{p}_{\mathbf{default}}, \mathbf{p}_{\mathbf{celltype}}, \mathbf{p}_{\mathbf{disease}}, ...]
\label{eq:cell_embeddings}
\end{equation}

where \(\mathbf{g}_{\mathbf{j}}\) is the gene j encoding,
\(\mathbf{e}_{\mathbf{i,j}}\) is the encoding of the expression of gene
j in cell i, \(\mathbf{l}_{\mathbf{j}}\) is the gene j location
encoding, and \(\mathbf{p}_{\mathbf{A}}\) is a learnt embedding for the
class A.

The total count information is stored separately and encoded similarly
to the expression,

\begin{equation}
\mathbf{e}_{\mathbf{t,i}} = MLP(log_{2}(1 + t_{i})), \text{ where } t_{i} = \sum_{j}x_{i,j}
\label{eq:total_count}
\end{equation}

with \(x_{i,j}\) the expression value of gene j in cell i, and the MLP
is a two-layer neural network similar to the previous one.

The full cell total count (\(t\)) lets scPRINT model its denoising based
on this required total count parameter.

The placeholder tokens (total count, default cell embedding, cell type,
disease, sex, ethnicity, assay, organism) are learned embeddings that
stay the same across all inputs. They only act as placeholders for the
model to fill in during the forward process. At the transformer's
output, they will have been modified to contain the embeddings
requested. At least two are used, one containing the default cell
embedding and another the profile's total depth. More tokens can be
used, one for each predicted cell label.

\subsubsection{Model}\label{model}

The model is a bidirectional autoencoder similar to
BERT with \emph{n} layers, \emph{h} attention heads,
and a dimension of \emph{d}. It uses the
flashattention2 methodology implemented in Triton to
compute its attention matrix. It uses the pre-normalization
technique, with a sped-up layer norm implemented in
Triton's tutorial. It uses a stochastic depth with
increasing dropout probability.

It has a 2-layer MLP with a 4x width increase in its hidden layer and a
GELU activation function.

\subsubsection{Expression decoder}\label{expression-decoder}

scPRINT uses a novel expression decoder for foundation models, which
outputs the parameters of a zero-inflated negative binomial
(\emph{ZiNB}) function for each gene \emph{i} in cell \emph{j}. The
\emph{ZiNB} distribution is defined as

\begin{equation}
X \sim ZiNB(\mu, \theta, \pi)
\label{eq:zinb_dist}
\end{equation}

where the parameters \(\mu,\ \theta,\ \pi\) are obtained from a
multi-layer perceptron (MLP) applied to the expression embeddings
outputted by the transformer model at its last layer (e), which are the:

\begin{equation}
\mu, \theta, \pi = MLP(\mathbf{e})
\label{eq:zinb_params}
\end{equation}

The MLP is a two-layer neural network with dimensions {[}\emph{d, d},
3{]}

Based on the work of Jiang et al., zero inflation is
the best distribution when considering a broad range of transcriptomic
measurements, where some have enough dropouts, and a zero inflation term
is needed to model it. In our case, and similarly to
scVI, we define our \emph{ZiNB} as

\begin{equation}
ZiNB(x | \mu, \theta, \pi) = \pi\delta_{0}(x) + (1 - \pi)NB(x | \mu, \theta)
\label{eq:zinb_def}
\end{equation}

where \(\delta_{0}(x)\) is a point mass at zero, and
\(NB(x\ |\ \mu,\theta)\) is the negative binomial distribution with mean
\(\mu\) and dispersion \(\theta\).

With these parameters, the negative binomial distribution is represented
in the following way

\begin{equation}
NB(x | \mu, \theta) = \frac{\Gamma(x + \theta)}{x!\Gamma(\theta)}\left(\frac{\mu}{\mu + \theta}\right)^{x}\left(\frac{\theta}{\mu + \theta}\right)^{\theta}
\label{eq:nb_dist}
\end{equation}

where \(\mu\) is the mean and \(\theta\) the overdispersion parameter,
representing the inverse of the dispersion. From Hibe et
al., we know that this is a parameter change from
the most used probability mass function (PMF) given by

\begin{equation}
P(X = x) = \binom{x + r - 1}{x}(1 - p)^{r}p^{x}
\label{eq:pmf}
\end{equation}

where r is the number of successes, \emph{p} is the probability of
success, and \emph{k} is the number of failures.

One can interpret such a negative binomial distribution as a Poisson
distribution with an additional overdispersion term that makes the
variance not tied to the mean. In scPRINT, we use the zero-inflated
Poisson for count downsampling as we can't easily infer the gene
overdispersion parameter from each cell profile. By removing this
zero-inflated Poisson from the gene expression profile, we keep the
potential overdispersion in the profile (see the
\hyperref[negative-binomial-to-poisson-relationship]{Negative Binomial
to Poisson relationship} section in Methods).

Compared to scVI, where the overdispersion parameter \(\theta\) is
learned for each gene, we make scPRINT output it together with
\(\mu,\ \pi\) (see Supplementary Figure \ref{fig-s13-graphical-model})

Effectively, the model learns that the dispersion might change depending
on the gene, the sequencer, the cell type, and the sequencing depth.

\subsubsection{Class decoder}\label{class-decoder}

scPRINT also outputs a variety of class embeddings, such as default cell
embedding, cell type embedding, disease embedding, etc., by filling the
different placeholder tokens given as input (see the
\hyperref[expression-encoder]{Expression encoder} section in the
Methods).

Effectively, for each class, we have the model learn to produce a new
disentangled embedding (e.g., cell type, disease, tissue, age). This
means the model uses an MLP to transform each token where A is a class.
For each, we jointly train a classifier:

\begin{equation}
\widehat{\mathbf{c}_{\mathbf{A}}} = \sigma(MLP_{A}(\widehat{\mathbf{e}_{A}}))
\label{eq:classifier}
\end{equation}

where:

\begin{itemize}
\item
  \(\widehat{\mathbf{c}_{A}}\ \)represents the logits for a class A of a
  dimension \(d_{A}\) whose size corresponds to the number of labels.
\item
  \(\sigma\) denotes the Sigmoid activation function.
\item
  \(MLP_{A}\) stands for the Multi-Layer Perceptron trained to predict
  the logits of the class \emph{A.}
\item
  \({\widehat{\mathbf{e}}}_{A}\) is the output embedding for the class A
  of dimension \emph{d}.
\end{itemize}

However, some classes, like cell type, have up to 800 labels.
Fortunately, cellxgene classes follow an ontology, a robust structure
that defines relationships among the labels. We reduce the size of the
output labels by training the model only on the leaf labels in the
ontology hierarchy (i.e., the most precise available). For cell types,
this represents around 400 different labels (see Supplementary Table \ref{table-s13-number-of-elements-predicted-per-class}).

Thus, when a label is not very specific for a cell type (e.g., neuron),
the model will predict the best leaf label (e.g., dopaminergic neuron).
This way, we can generate meaningful training signals from even very
coarse labels (see \hyperref[the-classification-task]{The classification
task} section in methods for more information and definition of the
loss). We only apply this hierarchical classifier to the cell type,
disease, and assay labels.

In the following section, we show how we train such classifiers. During
the classifiers' training, we sum up their loss without
applying any scaling between the different classes.

\subsection{Ablation study}\label{ablation-study}

We perform an ablation study of multiple of our additions in scPRINT for
its medium size version. Removing positional encoding, replacing
log-normalization with a total-normalization, replacing denoising with
masking, using the cell-gene product method of scGPT vs our own
encoder-decoder approach to learn a cell embedding, using 2 vs 4 heads
per attention blocks, not using weighted random sampling, not freezing
the gene ID embeddings, and using mean-squared-error instead of the ZINB
loss. For each, we re-train scPRINT entirely on the same dataset and
validate its test performance with our automated benchmark platform. We
provide the results in Table S3.

\subsection{Pretraining}\label{pretraining}

The three tasks of the multi-task pretraining are the denoising task,
the classification task, and the bottleneck learning task. While the
denoising loss enhances the model's ability to find
meaningful gene-gene connections, the other two try to make the model
and its underlying networks more robust and cell-type-specific. All
three losses are summed without rescaling.

\subsubsection{Optimization method}\label{optimization-method}

The optimization is done with fused ADAMW, with a weight decay of 0.01.
We noticed a total inability to learn when using base ADAM, which has a
similar weight decay. This can be explained by a known inequivalence
issue in ADAM.

We use the stochastic weight averaging method
during training with a learning rate of 0.03.

During pre-training, the hyperparameters are set to dropout of 0.1, a
learning rate (LR) of 1e-4, the precision is set to 16-mixed with
residuals in fp32. We clip gradients to 100 and train in many sub-epochs
of 7000 training batches and 2000 validation batches with a warmup
duration of 500 steps.

Across epochs, we use a linear LR decrease of 0.6 with a patience of 1
and stop training after three consecutive increases in validation loss
(patience: 3). In the final layer of the class decoders, we initialize
values to a normal distribution around 1 for weights, 0 for biases, and
-0.12 for biases.

Our batch size is 64, and we use a pre-norm strategy for the transformer
with a linearly increasing stochastic depth dropout rate of 0.02 per
layer. We use a noise parameter of 60\%. We split the cells in the
datasets into 98\% train and 2\% validation and reserve at minimum 2\%
of separated datasets for testing.

Finally, we use weighted random sampling on our training data based on
the different class values we have to predict. We use a factor of 50,
meaning the rarest elements will, on average, be sampled only 50 times
less than the most common ones. The sampling factor used for each group
is then \(\frac{50}{count + 50}\), instead of \(\frac{1}{count}\) where
count is the number of cells in each group.

\subsubsection{The classification task}\label{the-classification-task}

We perform label prediction during pretraining for different classes,
currently: cell type, disease, sequencer, ethnicity, sex, and organism.
Due to issues in the ontologies, we have omitted tissue type and age
classes.

Due to the hierarchical structure of the prediction, we also created a
hierarchical loss. Here, we compute the loss regularly when the label is
a leaf label. Otherwise, we replace all associated leaf labels to the
given label by the log-sum-exp, such that for a cell label, the loss is:

\begin{equation}
Loss_{classification} = CE(\sigma(\overline{\mathbf{c}},\mathbf{c}))
\label{eq:class_loss}
\end{equation}

with:

\begin{equation}
\overline{\mathbf{c}} = \left\{ \begin{array}{r}
\widehat{\mathbf{c}} \quad \text{if } \left\{ i | c_{i} = 1 \right\} \subseteq T \\
LSE\left( {\widehat{\mathbf{c}}}_{d} \right)||{\widehat{\mathbf{c}}}_{\sim d} \quad \text{else}
\end{array} \right.
\label{eq:c_bar}
\end{equation}

where:

\begin{itemize}
\item
  \(\widehat{\mathbf{c}}\) is the predicted vector with dimension equal
  to the number of leaf labels
\item
  \(\mathbf{T}\) being the set of label indices marking the labels that
  are leaf labels.
\item
  \({\widehat{\mathbf{c}}}_{d}\  = \ \{\widehat{c_{i}},\ \forall\ i\  \in \ T\}\)
  all the values in vector \(\widehat{\mathbf{c}}\) whose indices are in
  T. Same for \(\mathbf{c}\).
\item
  \({\widehat{\mathbf{c}}}_{\sim d}\  = \ \{\widehat{c_{i}},\ \forall\ i\  \notin \ T\}\)
  all the values in vector \(\widehat{\mathbf{c}}\) whose indices are
  not in T. Same for \(\mathbf{c}\).
\item
  LSE is the log-sum-exp operation
\end{itemize}

The CE (cross-entropy) is defined as:

\begin{equation}
CE(\mathbf{p},\mathbf{q}) = - \sum_{u}{q_{u}\log(p_{u})}
\label{eq:cross_entropy}
\end{equation}

And the LSE (log-sum-exp) is defined as

\begin{equation}
LSE(X) = \log\left(\sum_{p \in X}e^{p}\right)
\label{eq:lse}
\end{equation}

This loss allows the classifier to learn even in cases where the labels
can be of varying coarseness without the coarseness of some labels
impacting the ability of the model to predict the true fine-grained
labels (see Supplementary Figure \ref{fig-s14-hierarchical-classifier})

The loss is hierarchical for the classes: cell type, disease, sequencer,
ethnicity; the labels follow a hierarchy defined by (Cell Ontology,
MONDO, EFO, HANCESTRO), respectively.

We do not compute the loss for cells where a class has an unknown label.
We perform these classification tasks in one pass, using the embeddings
generated directly from the downsampled expression profile.

\subsubsection{The denoising task}\label{the-denoising-task}

Similarly to ADImpute, we expect a good gene network to help denoise an
expression profile by leveraging a sparse and reliable set of known
gene-gene interactions. In addition, we expect a good cell model to help
embed and reconstruct an expression profile by leveraging the
regularities of modules and communities within its network.

We view denoising similarly to upsampling, and inversely, we view adding
noise as downsampling a cell profile.

Noise is similar to downsampling because of the distribution we are
working with. Note that contrary to vision tasks (e.g. diffusion
models), where additive Gaussian noise is added, in the context of
expression data, where the distribution is often seen as a Poisson, NB,
or ZINB, the data is already noisy, and the more counts are sampled, the
less noise. No information is similar to not sampling data.

We downsample an expression profile using a zero-inflated Poisson model
of the data. With this formulation, on average, half of the counts to be
dropped are dropped by randomly removing a number of reads per gene,
given by sampling from a Poisson whose lambda parameter is proportional
to the number of counts in that gene. The remaining half of the counts
to be dropped are dropped by randomly setting some genes to 0, i.e. a
complete dropout of that gene. It is to be noted that with this
definition of downsampling, the exact average amount of counts dropped
for both parts depends slightly on the dropout \emph{r.} During our
pretraining, \emph{r} is set to 0.6, meaning, on average, 60\% of the
transcript counts are dropped per cell.

Let \(\mathbf{x}_{\mathbf{i}}\) be the gene expression vector of cell i
with dimensions \(n_{genes}\); we create a downsampled \emph{version} by
doing

\begin{equation}
{\widehat{\mathbf{x}}}_{\mathbf{i}} = \max((\mathbf{x}_{\mathbf{i}} - \mathbf{p}_{\mathbf{i}}) \cdot \mathbf{\pi}_{\mathbf{i}}, 0)
\label{eq:downsample}
\end{equation}

with:

\begin{itemize}
\item
  \(\mathbf{p}_{\mathbf{i}}\ \sim\ Poisson(\mathbf{x}_{\mathbf{i}}\  \times r \times 0.55)\)
  a vector of size \(n_{genes}\) where the poisson is samples for each
  element \(\mathbf{x}_{\mathbf{i}}\) of x
\item
  \(\mathbf{\pi}_{\mathbf{i}}\  = \ I(u \geq r \times 0.55)\) a vector
  of size \(n_{genes}\) , the binary mask vector indicating non-dropout
  genes.
\item
  \(\mathbf{u}_{\mathbf{i}}\ \sim\ Uniform(0,1)\), a vector of size
  \(n_{genes}\). of random values drawn from a uniform distribution.
\item
  \(\cdot\) denotes the element-wise multiplication.
\item
  \emph{r} being the dropout amount. We scale it by a tuning
  hyperparameter of 0.55 instead of 0.5 for numerical reasons.
\end{itemize}

The goal of the model is then using
\({\widehat{\mathbf{x}}}_{\mathbf{i}}\) as an input to output the
parameters
\(\mathbf{\mu}_{\mathbf{i}}\ ,\ \mathbf{\theta}_{\mathbf{i}}\ ,\ \mathbf{\pi}_{\mathbf{i}}\)
of a \emph{ZINB} distribution of the true profile
\(\mathbf{x}_{\mathbf{i}}\) \emph{,} all vectors of size \(n_{genes}\).
The contribution of cell i to the loss is then computed as the negative
log-likelihood of the count data given the distribution parameters being
generated by the model

\begin{equation}
Loss_{denoising} = Loss_{ZINB} = - \frac{1}{n_{gene}m}\sum_{i = 0,j = 0}^{n_{gene},m}{\log(L(x_{i,j}| \mu_{i,j}, \theta_{i,j}, \pi_{i,j}))}
\label{eq:denoising_loss}
\end{equation}

where \(n_{gene}\) is the size of the expression profile
\(\mathbf{x}_{\mathbf{i}}\) , m is the size of the minibatch and

\begin{equation}
L\left( x | \mu,\theta,\pi \right) = \left\{ \begin{array}{r}
\frac{\pi}{\pi - \theta \cdot (\log(\theta) - \log(\theta + \mu))} \quad \text{if } x = 0 \\
\frac{\left(\frac{\mu}{\theta + \mu}\right)^{x} \cdot \Gamma(x + \theta) \cdot \sigma(-\pi)}{\exp(\pi) \cdot \left( \frac{\mu}{\theta + \mu} \right)^{\theta} \cdot \Gamma(\theta) \cdot \Gamma(x + 1)} \quad \text{if } x > 0
\end{array} \right.
\label{eq:likelihood}
\end{equation}

with \(\sigma\) the sigmoid function.

We show that models trained with such a framework perform better than
regular MSE-trained models (see Supplementary Table \ref{table-s3-ablation-study-and-impact-on-performance-across-tasks}), for which one only outputs
one value instead of three, directly representing the
data's log-transformed count. In this case, the loss is
the mean squared error between the predicted and true count values.

scPRINT effectively lets the user choose between the three formulations:
\emph{ZINB} with a \emph{ZINB} loss, NB with an NB loss, and direct
log-transformed count reconstruction with an \emph{MSE} loss.

However, we have noted that the \emph{NB} and \emph{ZINB} loss still
have some notable issues. They can easily overflow, especially when
working with lower precision systems (like fp16, bf16, etc). These
losses are also proportional to the total expression count, meaning
cells with higher expression will have a higher loss on average. It also
appears that the log-likelihood cannot go below \textasciitilde1.1 loss
on average and plateaus quickly. This makes evaluation of the loss less
practical when comparing models. Finally, this minimal loss also depends
on the total number of zeros in the true expression dataset, as the
zero-inflation part of the loss converges smoothly to 0.

\subsubsection{The bottleneck learning
task}\label{the-bottleneck-learning-task}

Bottleneck learning is a method that drives the model to generate a cell
expression profile only from its embedding. Cell-embedding which can be
passed again to that same model without the gene expression information,
such that from the cell-embedding only, scPRINT can re-generate the
cell's expression profile. The model thus finds the best compression of
the cell's expression according to the information-theoretic theorem by
Tishbi et al. .

While many transformer models and Geneformer directly use the average of
gene embeddings to generate a cell embedding, this will likely squash
the expression information.\\
scGPT used another methodology (called MVC) to generate an embedding
vector such that

\begin{equation}
x_{i,j} = \mathbf{e}_{\mathbf{i}} \odot \mathbf{g}_{\mathbf{j}}
\label{eq:scgpt_mvc}
\end{equation}

where \(x_{i,j}\) is the expression of gene j in cell i, and \(\odot\)
is the dot product. For each gene embedding \(\mathbf{g}_{\mathbf{j}}\)
, the embedding only contains information about the gene name, not gene
expression. Regular MSE on each \(x_{i,j}\) is then used as the training
loss.

This pushes the cell embedding \(\mathbf{e}_{\mathbf{i}}\) to contain
all the expression information of the cell i.

This is less computationally intensive to train than our bottleneck
learning method. However, we have noticed poorer reconstruction through
this methodology than ours (see Supplementary Table \ref{table-s3-ablation-study-and-impact-on-performance-across-tasks}).

In our case, we consider that our model scPRINT can act as two parts of
an autoencoder. The encoding part is when we give scPRINT the expression
profile of a cell and retrieve a set of disentangled cell embeddings
(see the \hyperref[class-decoder]{Class decoder} section of the
methods). The decoder part is when we provide scPRINT only the gene
labels without their corresponding expression values and the
disentangled cell embedding in place of the empty placeholder embeddings
(see Supplementary Figure \ref{fig-s15-detailed-representation-of-the-bottleneck-learning-procedure}).

This means the encoder is considered as

\begin{equation}
\mathbf{e}_{\mathbf{A,i}} = scPRINT([\mathbf{g}_{\mathbf{O}} + \mathbf{e}_{\mathbf{0,i}} + \mathbf{l}_{\mathbf{0}}, \mathbf{g}_{\mathbf{1}} + \mathbf{e}_{\mathbf{1,i}} + \mathbf{l}_{\mathbf{1}}, ..., \mathbf{p}_{\mathbf{A}}])
\label{eq:encoder}
\end{equation}

where \(\mathbf{e}_{A,i}\) is the output embedding of the placeholder
embedding token A for the cell i (in our case, we use multiple (default,
totalcount, cell\_type, disease, sex, organism, ethnicity, sequencer).
Then the decoder is defined as

\begin{equation}
\mathbf{\mu}_{\mathbf{i}},\mathbf{\theta}_{\mathbf{i}},\mathbf{\pi}_{\mathbf{i}} = scPRINT([\mathbf{g}_{\mathbf{O}} + \mathbf{l}_{\mathbf{0}}, \mathbf{g}_{\mathbf{1}} + \mathbf{l}_{\mathbf{1}}, ...], \mathbf{e}_{\mathbf{0,i}}, \mathbf{e}_{\mathbf{1,i}}, ..., \mathbf{e}_{\mathbf{t,i}})
\label{eq:decoder}
\end{equation}

With
\(\mathbf{\mu}_{\mathbf{i}}\ ,\ \mathbf{\theta}_{\mathbf{i\ }},\ \mathbf{\pi}_{\mathbf{i}}\)
vectors of size \(n_{genes}\). Finally, the loss is given by the ZINB
loss:

\begin{equation}
Loss_{bottleneck} = \sum_{i = 0}^{m}{Loss_{ZINB}(\mathbf{x}_{\mathbf{i}} | \mathbf{\mu}_{\mathbf{i}}, \mathbf{\theta}_{\mathbf{i}}, \mathbf{\pi}_{\mathbf{i}})}
\label{eq:bottleneck_loss}
\end{equation}

where \(\mathbf{x}_{\mathbf{i}}\) is the cell i expression profile and
\emph{m} the minibatch size.

Implementing a set of disentangled embeddings is not straightforward. In
our case, we push the embeddings to be as different from one another as
possible with a contrastive loss defined as

\begin{equation}
Loss_{contrastive} = \frac{1}{m^{2}}\sum_{i = 1}^{m}{\sum_{i'}^{m}{1 - \cos(\mathbf{e}_{\mathbf{i}},\mathbf{e}_{\mathbf{i'}})}}
\label{eq:contrastive_loss}
\end{equation}

where \(\mathbf{e}_{\mathbf{i}}\) and \(\mathbf{e}_{\mathbf{i'}}\) are
the cell embeddings, \emph{m} is the minibatch size, and \emph{cos}
denotes the cosine similarity. This pushes each embedding to represent
the correct information using the classifiers. However, more is needed
to remove all the batch effects or entirely prevent information leakage
across embeddings.

Finally, we have also used the classifier output logits as cell
embeddings. This works particularly well for cell type, disease, or
sequencer classes containing many labels. It has been shown that
classifier logit outputs behave similarly to
embeddings and, in our case, offer an even better
removal of the batch effects (See Supplementary Figure \ref{fig-s7-full-scib-batch-correction-scores}).

For the bottleneck loss, we directly reconstruct expression using the
cell embeddings generated from the noisy, downsampled expression profile
of the denoising process, doing the entire process in one single pass.
We sum all the losses without scaling them:

\begin{equation}
Loss = Loss_{contrastive} + Loss_{bottleneck} + Loss_{denoising} + Loss_{class}
\label{eq:total_loss}
\end{equation}

\subsection{scDataloader}\label{scdataloader}

Parallel to this work, we worked with Lamin.ai to develop a dataloader
for large cell atlases, described and benchmarked in Rybakov et
al.. One key advantage of this dataloader is its
ability to perform weighted random sampling on hundreds of millions of
cells without being a bottleneck during pretraining.
scDataloader samples cells amongst the 800+
datasets of cellxgene's mid-2023 release, using the cell labels to
inform how rare the specific combination of labels is.

From this, the dataloader produces a cell sampling weight, rescaled with
a hyperparameter. The dataloader will sample, with replacement, more
consistently rare cell types than more common ones.

We have produced an additional wrapper package around the laminDB
``mapped-dataset'' called scDataloader. scDataloader works with lamin.ai
but can also interface with scVI and AnnData formats to enable
downloading, preprocessing, and QC of large single-cell databases and
datasets. It is very flexible and can represent expression data in the
formats used by scPRINT, scGPT, and Geneformer. It also implements a
lightning datamodule scheme and command line interfaces for quick setup
(see Supplementary Figure \ref{fig-s16-schematic-representation-of-our-dataloader}).

Overall, we preprocess each of the 1200 datasets in cellxgene by only
keeping primary cells from either humans or mice and dropping all the
spatial omics datasets. Spatial omics are not true single-cell assays,
and we decided for now not to include them. We also drop any cells with
less than 200 expressed genes. Finally, we drop any resulting dataset
smaller than 100 cells, with less than 10,000 genes, or from which more
than 95\% of the cells have been removed. This results in a new database
of 54,084,961 cells and 548 datasets.

We believe that the weighted random sampling strategy allowed our
pre-training to be much faster by creating more diverse minibatches.

\subsection{Extracting meta-cell gene networks from attention matrices
in
scPRINT}\label{extracting-meta-cell-gene-networks-from-attention-matrices-in-scprint}

Transformers compute multiple attention matrices per layer, called
attention heads. This is done by splitting the generated
\emph{\textbf{K}, \textbf{Q}}, and \emph{\textbf{V}} embedding into
\emph{m} sub-embeddings, thus defining \emph{m} attention heads. Each
attention head computes the attention matrix via the equation:

\begin{equation}
\text{softmax}\left(\frac{\mathbf{QK}^{T}}{\sqrt{d_{k}}}\right)
\label{eq:attention}
\end{equation}

However, we would want to aggregate those over multiple cells from a
similar cell state to increase the signal obtained from only one cell.
We are doing so by averaging the Keys and Queries embeddings over the
set of cells \(U\) passed to the model:

\begin{equation}
\text{softmax}\left(\frac{mean_{U}(\mathbf{Q}) \cdot mean_{U}(\mathbf{K})^{T}}{\sqrt{d_{k}}}\right)
\label{eq:meta_attention}
\end{equation}

By doing this, the attention matrix behaves as if each query vector for
cell i was ``looking'' across the key vectors of all the cells in U.

The resulting object is a row-wise normalized \emph{n*n} matrix, where
\emph{n} is the size of the input context (i.e. the number of genes
passed to the model). However, we also include the possibility to
generate large matrices and gene networks, referred to as genome-wide
gene networks. We take the average over different sets of expressed
genes for each cell in the set U. This allows us to compute a
genome-wide attention matrix while only doing forward passes on smaller
subsets of the genome per cell.

\subsection{Heads selection}\label{heads-selection}

With scPRINT, we present a method to select heads based on some
available ground truth data. This is inspired by the ESM2
paper and uses a somewhat similar method. Using all
the available attention matrices from all of the model's heads, we use a
linear classifier RidgeClassifier from scikit-learn
(with an L2 penalty set to 1, a positivity constraint on the
coefficients, and without an intercept) to classify the ground truth's
edges based on a combination of each head. The classifier converts the
target values into \{-1, 1\} equals to \{no connections, connections\}
and then treats the problem as a regression task with mean squared
error.

Instead of taking the classifier's output, we use the average of the
subset of heads associated with a non-zero coefficient in the
classifier, without weighting them. Thus, the classifier only serves as
a means to select the heads with relevant information in predicting a
ground truth of interest and decreases the possibility of overfitting
(see Figure 1C).

\subsection{\texorpdfstring{Normalization and network interpretation
}{Normalization and network interpretation }}\label{normalization-and-network-interpretation}

In scPRINT and scGPT, the attention matrix is normalized via the softmax
function over the query (i.e., row) dimensions. This means that all row
elements sum up to 1 or that the same mass flows from each network
component. This rescaling is essential as it corrects that some row
element scales can be much higher than others in the attention matrix.
Similarly, in regularized models like GENIE3, only a small set of genes
are connected for each gene in the matrix, meaning all genes have
directed edges toward a small subset of genes. Thus, our interpretation
is that the row elements are the targets in our network, each connected
to a small subset of genes. The column elements are thus the regulators
and can regulate many / most genes in the network.

For biological ground truths like MCalla et al. and gwps, which fit this
assumption of highly connected regulators and sparsely regulated
targets, we directly compare them to the inferred network. Tables S12
and S13 show that this performs better than taking the opposite view by
transposing the inferred networks.

This assumption is challenged for Omnipath, which has most of its
elements connected to a sparse set of other elements (see Supplementary Figure\ref{fig-s3-distribution-of-connection-amongst-the-three-ground-truths}). Due to the sparsity of connections for regulators (i.e., sources)
in the ground truth network and the large number of regulators (8000+),
the methods are challenged and perform much better when taking the
transpose of their network and matching the regulators to the sources
and sources to regulators.