\subsection{Simulated datasets, BoolODE and
Sergio}\label{simulated-datasets-boolode-and-sergio}

BoolODE is a method to generate count data via a stochastic differential
equation applied over a user-defined Boolean network. It was used and
developed as part of the BEELINE benchmark algorithm, which was created
as an improvement over the GeneNetWeaver algorithm. However, this model
is still very simple compared to cell biology. Due to its computational
complexity, it can only model up to a couple hundred gene relationships
over a few dozen genes.

Sergio, a slightly more recent ODE model marks an
improvement over BoolODE on the size of the networks it can simulate (up
to a thousand genes) and its similarity to scRNAseq data.

Indeed, Sergio's simulated data is not similar to real
expression data. This means that the biases that Transformer models
learn should not help them predict Sergio's data.
Correlation and regression-based methods do not have biases. They are
therefore expected and have traditionally shown better performance on
these benchmarks.

We generated the Sergio ground truth network and simulated single cell
expression by using the notebook:
\url{https://github.com/g-torr/SERGIO/blob/v2/minimal_example.ipynb}
from the repository: \url{https://github.com/g-torr/SERGIO} which
present some debugs and improvements to the initial repository:
\url{https://github.com/PayamDiba/SERGIO}. Indeed only this fork of the
initial Sergio repository led us to successfully generate a network.\\
\strut \\
We used RegNetwork as input and simulated 1000 cells
from its 3546 connections over 813 genes with default parameters from
the notebook.

\subsection{BenGRN and gene network
metrics}\label{bengrn-and-gene-network-metrics}

We use the packages benGRN and GRnnData released with this manuscript to
work With Gene networks and perform our benchmarks.

Our three main metrics are EPR, AUPRC, and enrichment. They all take
advantage of the fact that the predictions are generated as scores over
edges between nodes:

\begin{itemize}
\item
  We have computed the Early Precision Ratio (EPR) as the diagnostic
  odds ratio: (TP x TN) / (FP x FN) at the cutoff of the scores giving
  \emph{K} positive predictions, where \emph{K} is the number of
  positive elements in the ground truth.\\
  In this context, 1 is a random prediction, and inf is a perfect
  prediction; values below one mean that inverting the predictor would
  provide better results.
\item
  Area Under the Precision-Recall Curve (AUPRC) is the area (computed
  with the composite trapezoidal rule) under the curve defined by the
  precision (\emph{PR = TP / (TP + FP})) and recall (\emph{RE = TP / (TP
  + FN})) where \emph{TP} is the number of true positives, FP is the
  number of false positives, and \emph{FN} is the number of false
  negatives. This curve is obtained through a range of cutoffs going
  from 0 predicted positives to all predicted positives. Here, we
  compute a version of the AUPRC where the floor of the area is not
  given by the Precision=0 line but by the line of the prevalence of the
  positive class. Moreover, we do not interpolate the curve between the
  last recall value and the perfect recall: 1. We do this to properly
  compare AUPRC values across benchmarks and models. Random precision
  values are given in the supplementary data.
\item
  Enrichment is computed using the prerank
  methodology, where, given an ordered set of genes,
  it is computed by:

  \begin{itemize}
  \item
    1. Summing all scores of edges of the matrix row-wise. (Target -
    Hub) Or
  \item
    2. Summing all scores of edges of the matrix column-wise.
    (Regulators - Hub) Or
  \item
    3. Computing the eigenvector centrality of nodes in the
    graph using NetworkX's implementation.
    Prerank's background comprises all the genes in the set
    (centrality).
  \end{itemize}
\end{itemize}

Of note, we did not design an automated method for cell-type enrichment.
Instead, the assessment of whether or not a network is enriched for the
correct cell type is done manually, identifying cell type names in the
top 10 cell types listed in the enrichment results of the network.

\subsection{Other evaluation metrics}\label{other-evaluation-metrics}

All evaluation metrics from the section "%\href[scprintcompet]{
scPRINT is competitive on tasks orthogonal to
GN inference" of the results come from the openproblems benchmark and
are standards in the field.

scIB's batch correction score is an average of the avgBatch score and
the avgBio score, which are themselves averaged over many scores.
Details of each value are available in our package's notebooks.

\begin{itemize}
\item
  scIB avgBio is a combination of label-based and label-free metrics
  using for example: the Adjusted Rand Index (ARI)
  and the Normalized Mutual Information (NMI) on
  clusters computed from the K-Nearest Neighbor graph. Other scores are
  used, some using the conservation of trajectories and of the cell
  cycle variance, and some on the rare cell population conservation,
  overlap of highly variable genes (see scIB), and
  more.
\item
  scIB avgBatch is a similar combination of label-based and label-free
  metrics, using, for example, the average connectivity across clusters
  of different batches: ASW, the graph integration
  local inverse Simpson's Index: graph iLISI, the
  k-nearest-neighbor Batch Effect Test (kBET), and
  more.
\end{itemize}

Finally, we also use two metrics in our classification task:

\begin{itemize}
\item
  Macro-F1: also called macro-average, is the average of the F1 score
  across each class in a multi-class task. Where the F1 score is:
  \(2 \times \ \frac{PR*RE}{PR + RE}\).
\end{itemize}

\begin{itemize}
\item
  Accuracy: the accuracy is computed as
  \(\frac{TP\  + \ TN}{TP + TN\  + FN + FP}\)
\end{itemize}

\subsection{Denoising Benchmarks}\label{denoising-benchmarks}

To validate the denoising ability of scPRINT,
MAGIC, and
KNNsmoothing2, our test function, available in the
scPRINT package, uses a representative subset of 10,000 cells of each
dataset to generate the denoised expression over the 5000 most variable
genes in this dataset.

Before that, counts are removed from the dataset following the same
procedure as done for scPRINT's pretraining (see
\hyperref[the-denoising-task]{The denoising task} section of the
methods).

For each cell, we compare the denoised and un-denoised profiles to the
true profile (e.g. before denoising). We compute the Spearman's
correlation over the genes initially expressed in the cell, taking the
average across all cells. We do not use the unexpressed genes as we are
working with a dataset with high dropout and expect that a good denoiser
will set genes that are 0 in the profile with some value. We notice that
this improves the score of all denoising methods and makes more sense
given the data.

For the rare cell population test, we keep everything similar but
compute only the Spearman correlation over a rare cell population in the
dataset.

We run KNNsmoothing2 with default parameters and a K of 10. We run MAGIC
using the Scanpy implementation with default parameters and the
approximate solver for computational speed. When computing KNNsmoothing2
or MAGIC over a small set of cells we use a K of 5 for the nearest
neighbors.

\subsection{Fine-tuning}\label{fine-tuning}

Contrary to most other foundation models for scRNAseq, we do not
finetune scPRINT at any moment in our benchmark and all results are
provided for the pre-trained model only.

While we haven't assessed fine-tuning we believe this is an important
feature of foundation models and release various scPRINT models so that
they can be re-trained, fine-tuned, and modified by the community for
novel tasks or to improve its performance on the tasks we have
presented.

\subsection{State-of-the-art methods used in
benchmarking}\label{state-of-the-art-methods-used-in-benchmarking}

All methods presented here generate networks from their input data.
Given gene-level expression data, they will generate gene-networks.
Without additional information, no method can distinguish the type of
molecular interactions that underpin their predicted network edges.

\subsubsection{Gene network inference with an ensemble of trees
(GENIE3)}\label{gene-network-inference-with-an-ensemble-of-trees-genie3}

Developed originally for bulk transcriptional data, \emph{GENIE3}
computes the regulatory network for each gene independently. It uses a
random forest, a weak learner ensemble method, to predict the expression
profile of each target gene from profiles of all the other genes. The
weight of an interaction comes from the feature importance value of an
input gene in the predictor for a target gene's expression pattern.
Aggregating these weighted interactions over all the genes yields the
regulatory network. This method was the top performer in the DREAM4 in
silico network challenge (multifactorial subchallenge).

\emph{GENIE3} can be seen as a generalization of correlation-based
methods for inferring gene networks. Instead of looking at genes that
correlate most with another gene, GENIE3 finds how to combine a set of
correlated genes to get an even better correlation. We run GENIE3 on raw
counts as it is said from both the BEELINE benchmark and the R package
vignette that GENIE3 can be run on either log normalized or raw count
data and that while it will change the results, there are no preferred
methods. This is something we have also noted in our trials.\\
\strut \\
We use all default parameters and choose 100 trees for computational
feasibility reasons. We compute the networks on the same set of cells
and genes as the other methods.

We also use a TF-gene only version of the method where the regression is
performed only using the expressed transcription factors instead of all
expressed genes as input. This is the most used version of \emph{GENIE3}
and is much faster.

\subsubsection{DeepSEM}\label{deepsem}

DeepSEM is an autoencoder model made for gene network inference. It
learns to decompose a set of cells as a set of embedding and an
adjacency matrix (i.e., a gene network). The formula of the VAE then
becomes:
\(\mathbf{X} = f_{1}({(\mathbf{I} - \mathbf{W}^{\top})}^{- 1}\mathbf{Z})\),
for the decoder and
\(\mathbf{Z} = {(\mathbf{I} - \mathbf{W}^{\top})}^{- 1}f_{2}(\mathbf{X})\)
for the encoder, where \(\mathbf{X}\) is the expression data,
\(\mathbf{Z}\) is the embedding dimension, \(\mathbf{W}\) is the
adjacency matrix, \(\mathbf{I}\) the identity, and \(f_{1},f_{2}\) are
MLPs.

We preprocess the anndata by normalizing gene expression to 10,000
genes, applying a logp1 transformation, and then computing the z-score
per gene, as explained in the associated research paper.\\
We use DeepSEM with default parameters and on the same set of cells and
genes as the other methods. We use the DeepSEM-provided functions for
loading and parsing Anndatas.

\subsubsection{Single-cell generative pretraining transformer
(scGPT)}\label{single-cell-generative-pretraining-transformer-scgpt}

scGPT is a transformer-based model of roughly 100M parameters,
pre-trained with a generative process similar to Language models. scGPT
proposes to build similarity networks based on the output gene
embeddings of the model but also based on its attention matrices. It
computes networks as the difference between the rank-normalized version
of the average attention matrix in a baseline expression profile vs a
perturbed one in perturb-seq data. The attention matrix is the average
of attention matrices over the heads of the last layer and over the
cells given to the model.

We run scGPT following the examples given in their
``Tutorial\_Attention\_GRN.ipynb'' notebook.

We use the ``scGPT\_human/best\_model.pt'' from the list of available
models with default parameters. All runs are in our fork:
``https://github.com/jkobject/scGPT'' in the ``mytests/'' folder.
Similarly, we take the mean over cells and over the heads of the last
layer. We compute softmax similarly to the attention computation but
without applying the rescaling factor \(\sqrt{d_{k}}\) . We finally drop
the first element corresponding to the cell embedding token.

We extract cell embeddings from scGPT by directly using the cell
embedding token of the model without fine-tuning it on a batch
correction task. This is done in order to compare it to scPRINT which is
itself not fine-tuned. We compute the networks on the same set of cells
and genes as the other methods.

\subsubsection{Geneformer}\label{geneformer}

Geneformer is a BERT model. Gene expression data is transformed into a
sentence or genes ordered by their scaled expression. It is trained with
mask language modeling and contains somewhere around 80M parameters. We
use the new versions of 2024 Geneformer models
trained on 100M cells (2x more than scPRINT). We follow the
preprocessing and inference scripts used in the geneformer huggingface
repository and notebooks:
\url{https://huggingface.co/ctheodoris/Geneformer/tree/main}. Our
inference script updates to extract gene networks from Geneformer are
available in our scPRINT repository:
\url{https://github.com/cantinilab/scPRINT/tree/dev/tools}.

We extract gene networks from Geneformer using the mean of all attention
heads per cell. Since Geneformer only uses expressed genes in a cell, we
have to map the attention matrices back to the full network size before
computing its average over cells, taking into account the NaN values. We
compute the networks on the same set of cells and genes as the other
methods.

We extract a cell embedding from Geneformer using the cell embedding
from the ``gf-12L-95M-i4096\_MTLCellClassifier\_CELLxGENE\_240522''
model that has been fine-tuned on predicting the cell labels of
cellxgene datasets.

\subsubsection{scFoundation}\label{scfoundation}

scFoundation is a foundation model for single-cell RNAseq based on the
xtrimogene architecture. It was built by the Biomap
company. It is able to work on the full genome sequence of transcripts
for each cell by considering the high number of zeros and embedding them
separately. The tool is aimed at performing a range of tasks, such as
denoising, embedding, and predicting perturbation response. It has been
trained with a mixed masking and denoising pre-training. However, we
could not compare scFoundation to scPRINT and MAGIC on the denoising
benchmark, as scFoundation's denoising only happens at the level of the
cell embedding at inference time.

We could not validate scFoundation on our Gene network inference
benchmark as extracting a network from the attention matrices was much
more complex due to the xtrimogene architecture. scFoundation mentions
the generation of gene modules using clustering of its output gene
embeddings. It also mentions the interference of gene networks. However,
it is achieved using RcisTarget, a prior gene network
based on motif analysis. This approach is not comparable to the gene
networks generated by scPrint, Geneformer, and scGPT. Indeed, RcisTarget
could be applied to every model we have benchmarked and would prevent us
from doing an unbiased benchmark. Neither our approach nor Hao et al.'s
could extract gene networks directly from scFoundation. It is being left
to further investigations.

For batch effect correction, we use scFoundation with default parameters
and follow the steps for cell embedding in the ``model/README.md'' file
in their GitHub repository:
\href{https://github.com/biomap-research/scFoundation/}{https://github.com/biomap-research/scFoundation}.
However, we give scFoundation single cell profiles of the 5000 most
variable genes in each dataset. This is because we could not run
scFoundation on genome-wide expression profiles with our GPU. We then
apply a PCA to the output embedding to reduce the dimensionality from
3224 to 512. This is because the initial dimension was too high for scIB
to compute a score from on our machine (40CPU Intel Xeon, 32GB RAM +
64GB SWAP, GPU NVIDIA A4500 with 20GB of memory).

\subsubsection{Marker-based cell type prediction with CellTypist}\label{marker-based-cell-type-prediction-with-celltypist}

To showcase the novel ability of scPRINT to perform zero-shot prediction of cell type labels, we use the CellTypist method, which similarly performs de-novo prediction of cell type labels given its precomputed databases of cell type markers.

CellTypist works by mapping cell gene expression to genes known to be specifically expressed in combination in a cell type. Thus, it predicts cell type from these marker genes.

We use it with default parameters on the normalized and log-transformed counts over the full set of genes in the dataset. We use the `Adult\_Human\_PancreaticIslet' database, which contains markers for 14 cell types and overlaps with only four of the cell types in the dataset.

We decided to still use it as is to showcase the marker-based method's inability to recover the full set of cells and the tradeoff between the number of cell types and accuracy.

Fortunately, these four cell types (A, B, D, PP) represent 70\% of the dataset. With its current database, CellTypist can only reach a maximum accuracy of 70\%. Even when taking this into account, CellTypist only overperforms scPRINT on the accuracy metric and by roughly 9 points.

\subsubsection{Classification benchmark and associated
methods}\label{classification-benchmark-and-associated-methods}

Our classification benchmark is run using following the openproblems benchmark. It uses the same input, output data, and metric. It also similarly splits the train-test by batch and preprocesses the expression matrix to what is presented in the open problem benchmarks.

For this task, methods can access the full set of genes by default. scPRINT will use its random sampling of genes approach with a context of 4000 genes. Classifiers like logistic regression and xgboost were run according to the openproblem process, using the 25 principal components of the count normalized, logp1 transformed expression data. CellTypist was run on the normalized and logp1-transformed cell expression profile.

\subsection{Ground truth preparation}\label{ground-truth-preparation}

\subsubsection{McCalla et al.}\label{mccalla-et-al.}

For the MCalla et al. dataset, we downloaded the data from the supplementary datasets of \href{https://www.biorxiv.org/content/10.1101/2021.06.01.446671v2.supplementary-material?versioned=true}{their paper} . After undoing the logp1 transform, we re-generate the true count expression matrix from the normalized one by dividing the expression of each cell by the smallest value in its expression profile. This fully recovered the true counts, all values being integers. For the additional human dataset we used, we downloaded it from the \href{https://www.ebi.ac.uk/gxa/sc/experiments/E-GEOD-36552/downloads}{gene expression atlas database}.

We used the intersection (gold standard) ground truth dataset for both human and mouse, converting this list of sources to target genes into a directed binary network.

\subsubsection{Omnipath}\label{omnipath}

We generate the Omnipath network using all the interactions from the Omnipath Python package, excluding small molecules, lncRNAs, and any element without a unique HGNC symbol. We then transform it into a directed binary network of source to target. These interactions are extracted from the literature and represent mainly TF to gene connections as well as many protein-protein interaction connections and a small number of other connections known from the literature like RNA-RNA interactions, protein-RNA interactions, and more. All interactions are mapped back to their gene IDs, generating a gene-gene network encompassing the various interactions the genes and their molecular products can have.

\subsubsection{Gene networks from genome-wide perturb-seq}\label{gene-networks-from-genome-wide-perturb-seq}

We created a gene network from the genome-wide perturb-seq dataset using the supplementary matrix containing the results of differential expression in the dataset. This matrix represents the multiple hypothesis testing corrected p-values of a differential expression test of cells with KO of gene A compared to the baseline cell expression. This is available for all 8000+ expressed genes in the K562 cell line. We used a cutoff of 0.05 on these values to define the directed binary connection between genes.

This effectively gives a gene x gene-directed binary graph that tells if a statistically significant connection exists from the source \({gene}_{A}\) to the target \({gene}_{B}\) according to genome-wide perturb-seq.

For all ground truths, download, preprocessing, and extraction of the network and expression data are available in the BenGRN package.

\subsection{\texorpdfstring{Details on the Benign Prostatic Hyperplasia analysis}{Details on the Benign Prostatic Hyperplasia analysis }}\label{details-on-the-benign-prostatic-hyperplasia-analysis}

We download our dataset from cellxgene under the reference: \emph{574e9f9e-f8b4-41ef-bf19-89a9964fd9c7}.

We preprocess the dataset using scDataloader's preprocessing function. We generate embedding and classification using 3000 expressed genes in each cell. Similarly to pretraining, we take 3000 randomly expressed genes; if less than 3000 are expressed, we complete with randomly selected unexpressed genes. We display embeddings generated using the cell type classifier logits (see section \hyperref[the-classification-task]{The classification task} in methods)

We use the Scanpy toolkit to generate our Umap plots directly from the embeddings, as well as our differential expression results and our clusters. We define the clusters using the Louvain algorithm with 10 k-nearest-neighbors and a resolution of 1. We perform denoising on 5000 genes per cell selected similarly to the embedding and classification part. We use the 4000 most variable genes in each cell type to generate our gene networks in the BPH and normal fibroblasts.

On the gene networks, we perform gene set enrichment with the Enrichr method on the GO\_MF\_2023 gene sets. For community detection, we use the Louvain algorithm with parameter 1.5. We perform analysis only on the communities with between 200 and 20 genes. (4 and 5 in the BPH-associated fibroblasts, 3 and 4 in the normal fibroblasts)

All analysis and results are available in the \emph{cancer\_usecase\_1} and \emph{cancer\_usecase\_2} notebooks.

\subsection{Negative Binomial to Poisson
relationship}\label{negative-binomial-to-poisson-relationship}

As explained in \hyperref[the-denoising-task]{The denoising task} and \hyperref[expression-decoder]{Expression decoder} section of the methods, in our model, we have used the ZINB as our loss, an extension of the NB distribution to zero-inflated data. 

Moreover, we have also used the zero-inflated Poisson mechanism to downsample the cell expression profiles. These are consistent because we can view the Poisson distribution as a NB without overdispersion. The relationship between \emph{NB} and \emph{Poisson} is given by making the dispersion term go to 0 and the inverse dispersion term \(\theta \rightarrow \infty\). Doing so, the term \(\frac{\theta}{\theta + \mu}\) approaches 1. Thus, the PMF simplifies to:

\begin{equation}
P(X = x) \approx \frac{\Gamma(x + \theta)}{x!\Gamma(\theta)}1^{\theta}\left(\frac{\mu}{\theta + \mu}\right)^{x}
\label{eq:pmf_simplified}
\end{equation}

For large \(\theta\), we use Stirling's approximation of the Gamma function: \(\Gamma(\theta) \approx \sqrt{2\pi\theta}{(\frac{\theta}{e})}^{\theta}\)

we get:

\begin{equation}
\Gamma(x + \theta) \approx \sqrt{2\pi(x + \theta)}\left(\frac{x + \theta}{e}\right)^{x + \theta}
\label{eq:stirling1}
\end{equation}

\begin{equation}
\Gamma(\theta) \approx \sqrt{2\pi\theta}\left(\frac{\theta}{e}\right)^{\theta}
\label{eq:stirling2}
\end{equation}

Simplifying the ratio of the Gamma functions:

\begin{equation}
\frac{\sqrt{2\pi(x + \theta)}\left(\frac{x + \theta}{e}\right)^{x + \theta}}{\sqrt{2\pi\theta}\left(\frac{\theta}{e}\right)^{\theta}} = \sqrt{\frac{x + \theta}{\theta}}\left(\frac{x + \theta}{\theta}\right)^{\theta}\left(\frac{x + \theta}{e}\right)^{x}
\label{eq:gamma_ratio}
\end{equation}

For large \(\theta\), \(\frac{x\  + \ \theta}{\theta}\sim 1\), so: \(\sqrt{\frac{x\  + \ \theta}{\theta}} \approx 1\)

\[{(\frac{\ x\  + \ \theta}{\theta})}^{\theta} \approx \ 1\ \]

Thus, the expression simplifies to:

\begin{equation}
P(X = x) \approx \frac{1}{x!}\left(\frac{\mu}{\theta + \mu}\right)^{x}\left(\frac{\theta + x}{\theta}\right)^{x}
\label{eq:pmf_approx}
\end{equation}

Finally, \({(\frac{x\  + \ \theta}{\theta\  + \ \mu})}^{x} \approx \ 1\ \)for large \(\theta\), so:

\begin{equation}
\lim_{\theta \rightarrow \infty}P(X = x) = \frac{\mu^{x}}{x!}e^{-\mu}
\label{eq:poisson_limit}
\end{equation}

This is the PMF of the Poisson distribution with mean \(\mu\).

\subsection{Data availability}

The model weights are publicly available on 
\href{https://huggingface.co/jkobject}{Hugging Face}. Pre-training 
logs to assess the modelâ€™s training are available on 
\href{https://wandb.ai/ml4ig/scprint_scale/reports/scPRINT-trainings--Vmlldzo4ODIxMjgx?accessToken=80metwx7b08hhourotpskdyaxiflq700xzmzymr6scvkp69agybt79l341tv68hp}{Weights and Biases}. The full pre-training dataset is publicly available on CellxGene under its census data release version: LTS 2023-12-15, accessible at \url{https://cellxgene.cziscience.com/}. All other datasets used in this work can be downloaded through their respective public databases via the helper scripts on the scPRINT, BenGRN, GRnnData, and scDataLoader packages. Source data are provided to re-generate the figures. Code to generate the large UMAP of Figure 1 is available as a notebook on GitHub at \url{https://github.com/cantinilab/scPRINT/blob/1.6.4/figures/nice_umap.ipynb}. Code to re-generate the source data is available as notebooks on our \href{https://github.com/cantinilab/scPRINT/tree/1.6.4/}{Github}.

\subsection{Code availability}

The code and notebooks used to develop the model, perform the 
analyses, and generate results in this study are publicly available 
and have been deposited in cantinilab/scPRINT at \url{https://github.com/cantinilab/scPRINT} under MIT license. The specific version of the code associated with this publication is archived in the same repository under the tag 1.6.4 and is accessible via \url{https://github.com/cantinilab/scPRINT/tree/1.6.4/} and DOI:\href{https://doi.org/10.5281/zenodo.14749466}{10.5281/zenodo.14749466}.

Additional developed packages for this analysis are defined in the 
pyproject file and project submodules. They are available on GitHub:

\begin{itemize}
    \item \textbf{GrnnData}: \url{https://github.com/cantinilab/GRnnData}, DOI:\href{https://doi.org/10.5281/zenodo.10573141}{10.5281/zenodo.10573141}
    \item \textbf{BenGRN}: \url{https://github.com/jkobject/benGRN}, DOI:\href{https://doi.org/10.5281/zenodo.10573209}{10.5281/zenodo.10573209}
    \item \textbf{scDataLoader}: \url{https://github.com/jkobject/scDataLoader}, DOI:\href{https://doi.org/10.5281/zenodo.10573143}{10.5281/zenodo.10573143}
    \item \textbf{scGPT and notebooks to reproduce the results}: \url{https://github.com/jkobject/scGPT/tree/main/mytests}
\end{itemize}

