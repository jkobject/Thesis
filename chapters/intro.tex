\chapter*{Avant-propos} % Main chapter title
\addcontentsline{toc}{chapter}{Avant-propos}  

%Le but de l'avant-propos est d'améliorer les conditions dans lesquelles les membres d’un jury, vont pouvoir l’apprécier.
%\textbf{C’est une partie facultative du travail de recherche dans laquelle l’auteur peut expliquer les raisons qui l’ont incité à étudier le sujet en question, tout en exposant le but poursuivi et les difficultés rencontrées en cours de recherche.}

This Ph.D. started quite late in my carreer. Indeed I always loved science and research which got me into quite a few Ph.D. propositions in the last 7 years of my life. However it seemed I always found some other directions I was more inclined to. I was "pushing it for later".

\section{My background to this thesis}

All the opportunities I had were very exciting to me. They were allowing me to learn in some of the best environments.

Initially, I decided to create a company called PiPle with a friend, Paul Best, now a Post-doctoral Researcher at the University of Vienna in Machine Learning for bio-accoustic. 

Funily enough it was completely unrelated to biology. We worked on the creation of novel means of communication. We had grand ideas about how meassing apps, emails and the likes were utterly bad and how we could improve them using machine learning, and design. Doing this we learnt a lot about managing a complex projects, selling ideas, building large codebases, team-work and designing interfaces.

However we did not gain enough money from this and we felt that after a year of hard work, the road ahead was paved with too many sacrifices. 

This is when I passed on other albeit great Ph.D. opportunities to got work at the Broad Institute. Having visited the labs, Boston and Kendall square, I knew that this is the kind of experience I wanted to have and Ph.D.s seemed long, cumbersome and the Ph.D. projects all felt low-ambition. At Broad, however, I worked on many very-high impact research projects, I felt I was part of something bigger than myself. I published as first-author, and even started my own research projects. I learned how to do research, biology, genomics, and how to use cutting-edge tools and techniques. This is where I started mature some research project ideas.

While I still understood that a Ph.D. was the best place to undergo such projects. I also understood the length, harshness and arbitraryness of U.S. Ph.D. programs. I also wanted to continue working on team-based projects and felt that start-up companies could accomodate some of my research ideas and that I could start a team in such environments.

This and many other factos led me to go back to France and work as the team lead of the computational biology group at Whitelab Genomics in Paris.

In Whitelab I learned many things. First, I learned how to build a team, how to manage people. I learned a lot about what it means to grow companies from 20 to 50 people. I also learned about the biotech industry, how to build and sell such product. Whitelab had a good mix of expertise in computational biology, machine learning, structural biology and business development.

While starting the first project there. I realized the potential of foundation models for the biotech industry. From DNA language models to cell foundation models and knowledge-graph-based models, it became clear that they would be the path forward to aggregate the sparse and disparate information across many fields of biology and medicine. 

\subsection{Why did I do it}

While at Whitelab, I was not looking for any other positions, and had in mind to stay at least a few years and assess how we had grown during that time.

However, I was already in contact with Laura Cantini with whom I had discussions about Ph.D. projects before joining Whitelab. At some point, Laura came back to me with this Ph.D. proposal. I spent the good part of a month in a tough position. Thinking about which decision would not become a regret in the future.

There was no perfect time to do this but it felt like it was now or never. The Ph.D. topic and group was what I was expecting all along. I knew I could do it in less than 3 years, the environment would be welcoming, with family, friends surrounding me. And I knew what I wanted to work on, what I wanted to learn.

While this proposal matched what I wanted to do, I was also very impressed by the level of various Ph.D. students in Laura's nad Gabriel's lab. Seeing people 4 years younger than me, already having such a high level of expertise and knowledge, was very humbling.

\subsection{Issues and realizations}

I also had a couple disillusions during job interviews where I understood that not having a Ph.D. was an issue, while other companies were very clear that my experience, to them was the same as a Ph.D.

I also could see the speed of development of new papers and the breadth of possible research endeavors available to computational biologists. So there was some FOMO there for sure and I knew that if an AI-heavy transcriptomic research project was coming my way I might not say no.

Finally, I started to feel that building non-open-source projects and packages was contra-productive. We have so many poor computational tools and missing packages in the community. I wanted to build things that would not just serve me and my team but also the community at large.

I would argue that for projects in which the result is very far off or unclear, not open-sourcing can be problematic. If you are building a product like a lamp, say. You cannot cheat, will the lamp turn on? For how long? For something like Facebook, even, you get usage metrics from very early on. For Biotechs, computational biology, and research, The clear-cut goal is often “a drug that passed some clinical trials”. This creates many untangible and not very engineering-like objectives along the way like: “Can you get people to believe in you?” how many partnerships do you enter?”.

In this context, you can lie. You can lie to the client but also to yourself. You can design a very nice-looking “car” that doesn't drive at all. but since you are selling to people who don't even really understand what is a car and what to do with it, it is ok.

This is why I think open source is so important: when you let people know how to use a car, try your car, the iteration cycle is much faster.

But I think building a startup has many similarities with being a researcher, for example: you have ideas you need to sell through a powerful story. This is how you get people and funding. These ideas although incomplete have to convince people. You spend a lot of time finding money and building a team of experts and beginners. Putting yourself out there: going to as many conferences and events, with a solution to people's problems.

\subsection{Cell and Gene Therapies}

I joined Whitelab because I strongly believe in the future of genetic therapies. Covid opened our eyes to the power of mRNAs, while CAR-Ts have had a big impact already on cancer therapies, and many more drugs are in the pipeline.

Cell and gene therapies (CGTs) represent a revolutionary approach to disease treatment, moving away from traditional methods towards more targeted and personalized solutions. These therapies work by altering the genetic material within a patient's cells to fight or prevent disease. They can replace, remove or repair abnormal genes or introduce totally new genes to fight diseases.

A very famous subcategory: mRNA therapies, such as those used in some COVID-19 vaccines, are a type of gene therapy that works by introducing a small piece of mRNA into the body. This mRNA carries some instructions and once inside our cells, it uses the body's own machinery to execute it. The advantage of mRNA therapies is that they don't alter the DNA in our cells and are not permanent, reducing the risk of long-term side effects.

Antisense oligonucleotides (ASOs) are another type of CGT that work by binding to the mRNA produced by certain genes and preventing them from being translated into proteins. This can be useful for diseases where a particular gene is overactive. ASOs can be designed to target almost any gene, making them a highly versatile tool in the fight against disease.

Overall, the main advantage of CGTs is their precision. They target the root cause of diseases at the genetic level, offering the potential for more effective and longer-lasting treatments with fewer side effects. This precision, combined with the rapid advancements in technology and our understanding of the human genome, is why many believe CGTs represent the future of medicine.

Being able to target only these specific diseased cell types is within reach for CGTs but not for small molecules. Moreover being specific in what we are doing: what genes we are triggering is also what CGTs can do that most small molecules can't.

Now that I had a better understanding of the issues that plague small molecules and with the advant of really good bio-structural methods like Alphafold, it felt to me that even more so today than before, the tough nut to crack was finding the right target, the right gene for the right disease. For this one needs better models of the cell.

\section{Introduction}

In this Thesis we will interest ourselves with models of the Cell. We will investigate the main data modality used to create them and how we can train modern machine learning methods based on neural networks to create so-called "Foundation models". We will explore how these models can be usefull today and how we might make them better. We will need to understand some key facts about the cell. How biologists think about it and some key facts about our measurement methods, single cell omics. Finally we will learn about neural networks, and the suite of tools that computational researchers have built that could help us in this endeavour.

Modelling the cell is extremelly important and indeed it has reached the imagination of many researchers along the years. Hundreds of companies, from tech to bio and dozens of institutes are pursuing efforts to create such capable tools. Being able to even achieve moderate gain and accuracy would impact the way we do experiments, develop drugs and assess diseases.

For proteins, the constituents of cells, we have already seen how Alphafold has revolutionized the field by using high quality structural protein data, sequencing information and novel deep learning approaches. These models are now revolutionizing many industries. We would expect that cell models would have a similar impact on the field of biology and medicine. 

\subsection{GRN and the cell}

The cell is the fundamental unit of life and is composed of various components, including proteins, nucleic acids, lipids, and carbohydrates. Each of these components plays a crucial role in the cell's structure and function. Proteins are responsible for most cellular processes, while nucleic acids (DNA and RNA) carry genetic information. Lipids form cell membranes, and carbohydrates serve as energy sources and structural components.

RNA biology is a critical aspect of cellular function, encompassing processes such as transcription, translation, and regulation. Transcription is the process by which DNA is copied into RNA, which then serves as a template for protein synthesis during translation. Regulation of these processes is essential for maintaining cellular homeostasis and responding to environmental changes. This regulation can occur at multiple levels, including transcriptional control, RNA processing, and post-translational modifications.

the RNA hypothesis posits that RNA molecules were the first self-replicating entities, leading to the evolution of life as we know it. This hypothesis suggests that early life forms relied on RNA for both genetic information storage and catalytic functions, paving the way for the development of DNA and proteins. Showing how RNA might be some of the most central components of the cell.

Indeed many different types of RNA exist, each with distinct functions. Messenger RNA (mRNA) carries genetic information from DNA to ribosomes for protein synthesis, while transfer RNA (tRNA) and ribosomal RNA (rRNA) allow translation of mRNAs into proteins. Other types of RNA, such as small interfering RNA (siRNA) and microRNA (miRNA), are involved in gene regulation and silencing. Long-non-coding RNAs (lncRNAs) also play crucial roles in regulating gene expression and chromatin structure. Unfortunately, many of these RNA types are still poorly understood, and their functions are an active area of research.

Gene expression is the process by which information from a gene is used to synthesize a functional gene product, typically a protein. This process involves several key steps, including transcription, where DNA is transcribed into mRNA, and translation, where mRNA is translated into a protein. Transcription factors (TFs) are proteins that bind to specific DNA sequences to regulate the transcription of genes. They play a crucial role in determining which genes are expressed in a cell at any given time, influencing cellular function and identity.

They also interact with other proteins, such as cohesin, which helps maintain chromatin and the specific 3D structure of the DNA. Chromatin is the complex of DNA and proteins that forms chromosomes within the nucleus of eukaryotic cells. The organization of chromatin is essential for regulating gene expression, as it determines the accessibility of DNA to transcription machinery.

In this context, biologists have relied on the concept of gene regulatory networks (GRNs) to simplify the complex interactions within the cell. GRNs are networks of molecular interactions that govern gene expression levels in a cell. They consist of genes, transcription factors, and other regulatory elements that interact with each other to control the timing and level of gene expression. Although very coarse and likely wrong in many ways, these modelled interactions allows researchers to gain insights into how cells respond to various stimuli, differentiate into specific cell types, and maintain homeostasis.

Gene networks (GNs) are a more general concept that encompasses not only gene regulatory networks but also other types of interactions, such as protein-protein interactions and metabolic pathways. While GRNs focus specifically on the regulation of gene expression, GNs provide a broader view of the cellular processes and interactions that contribute to the overall function of the cell.

\subsection{Single-cell omics}

Genomics is the study of an organism's complete set of DNA, including all of its genes. It involves sequencing and analyzing the entire genome to understand its structure, function, and evolution. The development of high-throughput sequencing technologies has revolutionized genomics, allowing researchers to sequence entire genomes quickly and cost-effectively.

Initially, done by Sanger sequencing, which was the first method developed for DNA sequencing. It involves selectively incorporating chain-terminating dideoxynucleotides during DNA replication, allowing researchers to determine the sequence of nucleotides in a DNA molecule. This method was labor-intensive and time-consuming, but it laid the foundation for modern sequencing techniques.

Nowadays we use next-generation sequencing (NGS) technologies, which allow for massively parallel sequencing of millions of DNA fragments simultaneously. This has significantly reduced the time and cost required for genome sequencing, enabling large-scale genomic studies and personalized medicine approaches.

Large scale sequencing efforts such as the 1 million genomes project, the Human Genome Project, and the 1000 Genomes Project have provided valuable insights into human genetic variation and disease susceptibility. Genetic sequencing now allows to define the genetic basis of many diseases, identify which drug might work on specific patients, and define follow ups for high risks patients. It is driving more and more clinical decisions and is becoming a standard part of patient care.

But sequencing also allowed many new applications, such as the study of gene expression by sequencing instead the mRNAs in tissues, a process known as RNA sequencing (RNA-seq). The sequencing of DNA state such as methylation (BS-seq), open chromatin (ATAC-seq), chromatin immunoprecipitation (ChIP-seq) which all give a view of how the genome is being read at a point in time. From DNA and its mutation, to its state in different context and how they lead to changes in RNAs and their state we start to have a hollistic view of the different cellular mechanisms.

However, these methods are only giving us a view of the average state of the tissue, and not of the individual cells.

In 2014, the first single-cell RNA sequencing (scRNA-seq) methods were developed, allowing researchers to analyze gene expression at the single-cell level. This was a major breakthrough in genomics, as it enabled the study of cellular heterogeneity and the identification of rare cell populations that may be missed in bulk RNA-seq analyses.

Since then, single-cell sequencing technologies have rapidly advanced, with new methods being developed to sequence the other omics modalities at the single-cell level. Studies that were made at the scale of 10,000s of cells in the 2019s are now being conducted at the scale of millions of cells. 

Moreover the development of spatial transcriptomics and imaging techniques has allowed researchers to study the spatial organization of gene expression and within tissues, providing a more comprehensive view of cellular function in its native context together with imaging of the cell. Protein measurements are also being developped unlocking an additional layer of information. 

Current applications have been mostly focused to the understanding of diseases and drug effects within tissues. The technique allowed the identification of hundreds of new cell types and states, improved the study of cellular development and differentiation. It had strong impact on cancer, neurological diseases and immunology. 

Finally, together with the development of perturbation techniques such as CRISPR screens, we can now go beyond observations and start to understand the causal relationships between genes and their functions. Indeed, CRISPR screens allow researchers to systematically deactivate genes in individual cells and observe the resulting changes in gene expression, providing insights into gene function and regulatory networks.

However, single-cell sequencing also comes with its own set of challenges. One of the main issues is the sparsity and noise underlying most of the current single-cell sequencing methods. Secondly there is also some strong "batch-effect"; biases in the data generation process that can make it challenging to perform analysis across datasets.

Worse, the dataset themselves have issues. We currently miss many tissues and cell types that are rare and hard to sequence. We have yet to perform such analysis on other species than humans and mice.. and specific cell states in aging, foetal growth, disease, treatment are missing. A hope is that machine learning and AI migh allow us to computationally solve these issues and infer the missing data.  

\subsection{AIVC: the virtual cell models}

A virtual cell model has been the dream of computational and systems biologists for decades. Initially, these models were based on simplified representations of cellular processes, often focusing on specific pathways or interactions. The models looked at chemical reactions parameters between proteins, RNAs and the DNA. However in additional to computational challenges these models were not able to generate realistic predictions of cellular behavior.

Nowadays the dominant idea is that Artificial Intelligence or AI would allow us to solve some of these problems. But first, what is AI and what is the difference with ML, Data science and informatics?

Data Science is anything related to gathering, management, and analysis of data in informatic systems. Machine Learning happens when one uses methods like statistics to generate predictions from data. But these methods can be more complex and while they can be seen in the framework of statistics they also have underpinning in other domains, like neuroscience (with neural networks) and applied mathematics with Algebra, Topology, Analysis and most importantly, Optimization. These methods often allow powerful modelling of the data statistics.

Artificial Intelligence is a broad term that has had multiple meanings in society and culture. But it mostly refers to applications of machine learning methods to human and animal related tasks. Like understanding images, videos, speech, text, robots and others. Thus creating many bridges and links between data science, machine learning, statistics, informatics and AI.

Recently ML has made great strides in many areas mostly thanks to a large increase in data being generated, as well as improvements in optimization methods and neural networks.

Large Language Models (LLMs) have become ubiquitous nowadays and many in the field are trying to generalize them to world models. These models, trained on and physics simulations and videos try to predict the next state of complex physical phenomas like what happens when someone throws a ball to the physics of water flowing down a river.

They have already shown promise powering humanoid robots. Google DeepMind, recently released a model for predicting the weather better and faster than the best "physics based" models.

But why and how are these models so powerful? While this remains an active area of research, a lot of theories have been presented. 
Contrary to the commonly accepted theory of learning, deep learning researcher showed in the 2010s that increasing the number of parameters did not necesarrily lead to overfitting of the model, i.e. the model only learning by heart to reproduce the data. Instead, thanks to some light regularization methods such as dropout and weight decay. Methods that consist of randomly removing some neurons and their connections during training, and penalizing the model for having large weights, the models were able to learn more complex patterns in the data.

Furthermore, thanks to  GPUs allowing large parallel matrix operations and advanced optimization methods such as Adam, even such large number of parameters could be trained efficiently. Indeed stochastic gradient descent (SGD) methods like Adam do not simply minimize the loss function but also allow to escape local minima and saddle points. 

To understand this we need to understand the loss landscape, imagine a 3D landscape where the height of the landscape is the loss value and the two other "surface" dimensions are the model's parameters. The goal of the model is to find the lowest point in this landscape, which corresponds to the best parameters for the model. However, this landscape is often very complex, with many local minima and saddle regions, the model wanders blindly and is only able to sense its direct surroundings, if it falls in a local minimum, it should get stuck.
In very high dimensions however, when the model has millions of parameters, and SGD makes the loss landscape a bit different every time, the model becomes able to escape these local minima and saddle points.

Behind these unexpected behavior is the unsettling theory of emergence. Or how small objects can combine and interact in ways that would be unexpected and difficult to predict based on their individual properties. These theories behind sand dunes, snowflakes but also ant colonies and life itself, might bridge to how large neural networks are able to achieve such complex behaviors.

While LLMs are everywhere nowadays, reaching real virtual cell model is still out of reach. We are now switching from small machine learning models trained per datasets to larger neural architectures trained across the entirety of the available data. These models are called foundation models. By definition they should learn the general patterns in the data and be able to generalize to new datasets and tasks, common to single cell biology with a similar if not better performance than state-of-the-art small models.

These foundation models are still in their infancy. We would want to understand how they work, if they can be useful to current research and how we might improve on their training, architecture and generalization capabilities.

losses, fine-tuning, perturbation, embeddings, sequencing depth, reads, alignment

\subsection{Current single cell foundation models}

The first practical example of a single cell foundation model could have been scBERT, released in 2021, however it was only used and benchmarked on cell type classification and pretrained on 1 million single cells. The first real foundational model, geneformer, got released a year later. There, the author displayed the model's ability to perform various single-cell tasks, such as cell type classification, gene regulatory network inference, and perturbation prediction. Geneformer was trained on a much larger dataset of 33 million single-cells.

However Geneformer, like scBERT, was really a complete reuse of the BERT model architecture. Furthermore while showcasing interesting behaviors it did not perform classifcal benchmarks on such domains and comparison to state of the art methods. 

A year after geneformer, in 2023 a few additional foundation models got released, from scGPT, which showcased a take on the GPT architecture and presented various losses for fine-tuning the model. It was the first example of fine-tuning in single cell and a more in depth benchmark of the model across 4 different abilities, cell type prediction, gene network inference, perturbation prediction and batch correction. 

At the same time, Universal Cell Embedding showed how one could train these models across multiple species and perform good quality cross-species embeddings. It also showed a new kind of loss function to generate embeddings of the cells, vectors that represent the cell according to the model.

Finally scFoundation, showcased a truly novel architecture, built specifically for single-cell data and a truly novel training method based on the noise to sequencing depth relationship of single cell data in particular.

\subsection{Current single cell tasks}

While these models could be very performant, reliable and reproducible benchmarks that align with real use-case from the user's perspectives remain scarce. In single cell RNA-seq data, a typical pipeline is as follow:

1. It all start with preprocessing the raw sequencing data: detecting / imputing the cell index, aligning the sequencing reads with a reference genome's gene locations, detecting lo quality cells, reads, doublet events, cell death events and more. These choices will impact the output dataset as biases. 

2. the data might be normalized to correct for sequencing depth and other gene-level biases, clusters of cells would be defined based on their expression profile similarity. Finally differential-expression analysis is performed, whereby clusters of cells are compared to each other to identify genes that are differentially expressed between them. This might be done also after the different other steps.

2.bis The dataset might be aligned to a reference atlas in cases where this exist. This is done by using batch-correction methods, often built around nearest neighbor mapping, matrix factorization or Neural Networks called Variational Auto-Encoders.

3. cluster-level and dataset-level labels might be infered such as cell-type, tissue, disease, age... These often come from prior knowledge, and manual annotation based on differential expression features, some automated tools or based on the alignment to other prelabelled datasets. Thanks to these correlation between gene expression and these labels might be defined and guide further research. 

4. In the case where specific clusters contain low amount of cells, denoising or zero imputation methods can be used but they haven't shown usefulness in practice since they are based on using cluster level information which is itself already used for most other tasks.

5. In the case where users would have access to similar tissue dataset of another modality like sc-ATAC-seq, sc-BS-seq, or protein measurements, imaging data and more. Such multimodal alignment methods exist, often re-using prior knowledge and paired datasets where these two modalities are measured in the same cells. Thanks to these aligment one can define more complex relationship between how DNA readings and related DNA mutations impact the expression of genes, and how these impact the downstream expression of proteins. Bridging the map between the genotype (DNA) and the phenotype (cell features, patient diseases, etc).

6. If the dataset was measured in a non-static context, one can infer "cellular trajectories" meaning how cells seems to go from one state to another based on single measurements of many cells, such as in differentiation or during perturbations. Many tools exist for such trajectory inference but it is an open question often require specific RNA-sequencing methods.

7. If the dataset contains spatial information, meaning how cells were positionned in a tissue, one can infer "cell-cell interactions" meaning how cells influence each other based on their proximity and expression profiles. This often requires specialized imaging techniques where a cell morphology is also assessed. Cell-cell interaction is 

8. If the dataset includes perturbation experiments, one can predict how cells respond to specific perturbations, such as drug treatments or genetic modifications. This can help identify key regulatory pathways and potential therapeutic targets.

6. gene regulatory network inference