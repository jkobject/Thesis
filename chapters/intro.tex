\chapter*{Preamble} % Main chapter title
\addcontentsline{toc}{chapter}{Preamble}  

This Ph.D. started relatively late in my career. I'd like to spend some of the introduction mentioning the reasons that pushed me to do a Ph.D. now.

\section{My background for this thesis}

\subsubsection{PiPle}

Many of the opportunities I had coming out of school have been very exciting. Initially, I decided to create a company called PiPle with a friend, Paul Best, who is now a Post-doctoral Researcher at the University of Vienna in Machine Learning for bio-acoustics. 

Funily enough, it was completely unrelated to biology. We worked on creating novel means of communication. We had—and still have—big ideas about how to improve utterly inadequate messaging apps, emails, and similar tools using machine learning and innovative designs. Doing this, we learnt a lot about managing complex projects, selling ideas, building large codebases, teamwork, and designing interfaces.

However, we did not gain enough traction from this, and we felt that after a year of hard work, the road ahead was paved with too many sacrifices. 

\subsubsection{the Broad Institute}

This is when I passed on Ph.D. opportunities a second time to work at the Broad Institute instead. Having visited the labs, Boston, and Kendall Square, I knew that this was the kind of experience I wanted to have, and Ph.D.s seemed long and cumbersome. At Broad, I worked on many very-high-impact research projects, and I felt I was part of something bigger than myself. I published as the first author and even started my own research projects, which would inform the thesis I am presenting here. 

While I still understood that a Ph.D. was the best place to undergo such projects, I was uncertain about the specifics. I also understood the length, harshness, and sometimes arbitrary nature of U.S. Ph.D. programs. I also wanted to continue working on team-based projects and wanted to experience the start-up environment.

\subsubsection{whitelab genomics}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./figures/whitelabgx.jpeg}
  \caption{Whitelab Genomics.}
  \label{fig:whitelabgx}
\end{figure}

Along with some other personal decisions, it led me to return to France and work as the team lead of the computational biology group at Whitelab Genomics in Paris.

In Whitelab, I learned how to build a team and how to manage people. I learned a lot about what it means to grow companies from 10 to 50 people. I also learned about the biotech industry and how to build and sell such products.

Whitelab had a good mix of expertise in computational biology, machine learning, structural biology, and business development. While starting the first project there, I significantly enhanced the potential of foundation models for the biotech industry. 

From \gls{DNA} language models to cell foundation models and knowledge-graph-based models, it became clear that they would be the path forward to aggregate the sparse and disparate information across many fields of biology and medicine. 

\subsubsection{a Ph.D.}

I was not looking for any other positions and intended to stay at least a few years to assess how we had grown during that time.

However, I was already in contact with Laura Cantini, with whom I had previously discussed Ph.D. projects. At some point, Laura came back to me with this Ph.D. proposal. I spent the good part of a month in a challenging position. Thinking about which decision would not become a regret in the future.

There was no perfect time to do this, but it felt like it was now or never. I was also very impressed by the level of various Ph.D. students in the labs of Laura and Gabriel. Seeing people 4 years younger than me, already having such a high level of expertise and knowledge, was very humbling. Finally, the Ph.D. topic and group were really on point with what I wanted to do. But mostly, my work/life environment was welcoming, surrounded by family, friends, and activities. I knew what I wanted to work on and what I wanted to learn.

Therefore, I decided to start this Ph.D. journey.

\section{Introduction}

In this Thesis, we will focus on models of the Cell.

In the mid-17th century, Robert Hooke made a groundbreaking discovery while observing a piece of cork through his microscope. He observed structures that he named ``cells" and, as a result, marked the beginning of cellular biology\cite{petersCellsRobertHooke2024}. Cells have since been identified as life's fundamental structural and functional units, and biologists have endeavored to map the diverse cell types that comprise multicellular organisms. Additionally, they have sought to understand the transient cell states that occur during development, disease progression, and tissue regeneration\cite{Alberts2015CellsGenomes}.

The objectives of cellular biologists are to understand and control, with the dream of engineering life from plants to animals and even generating entirely new synthetic life\cite{daviesSyntheticBiologyVery2018}. But what for?

\subsection{The promises of cellular biology}

\subsubsection{drug design}

Before developing a drug for a disease, one must understand the disease and identify a potential target gene or set of target genes. It refers to the genes in specific cell types that need to be reactivated, deactivated, or modified to address the disease's underlying mechanism.

But drugs don't have to be small molecules. CAR-T cell therapies have revolutionized blood cancer treatment by modifying a patient's own immune cells to fight the cancer. Similar approaches could be developed for many other conditions\cite{zugastiCARTCellTherapy2025}. Here, the drug becomes a cell.

Helping creating these cellular drugs as well as more classic ones is one of the key applications of the work we will present in this thesis.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/cell therapy.jpg}
  \caption{Illustration of the CAR-T cell therapy.}
  \label{fig:celltherapy}
\end{figure}

\subsubsection{other applications}

But diseases are not the only nails one could hit with such a mighty hammer. Indeed, life is everywhere, and engineering has already helped us make better crops, create synthetic meat, and design fungi that remove pollution. When people talk about nanorobots, I urge you to think about engineered cells\cite{daviesSyntheticBiologyVery2018}.

Richard Feynman famously said, "What I cannot create, I do not understand." Therefore, the Modeling of the cell stands as a key milestone in cellular biology, and indeed, one cannot succeed in the aforementioned promises without a correct cellular blueprint.

\subsubsection{virtual cells}

Therefore, hundreds of companies, from tech to bio, and dozens of institutes are pursuing efforts to create such virtual cellular models \cite{bunneHowBuildVirtual2024}. Achieving even limited predictive accuracy would have significant impacts on cellular biology.

In this thesis we look into virtual cell modelling using modern machine learning methods but interogating them using the classic biological concepts.

We will investigate the primary data modality used to create cell models and how we can train them using modern machine learning methods based on neural networks. We will explore how these models can be useful today and how we might make them better in the future. For that, we will need to understand some essential facts about the cell and how biologists think about it.

\subsection{GRN and the cell}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/aivc.jpg}
  \caption{Graphical view of a small part of the cell.}
  \label{fig:thecell}
\end{figure}

Let's first look at the cell. The cell is the fundamental unit of life and is composed of various components, including proteins, nucleic acids, lipids, and carbohydrates. Each of these components plays a crucial role in the cell's structure and function. Proteins are responsible for most cellular processes, while nucleic acids (\gls{DNA} and \gls{RNA}) carry genetic information. Lipids form cell membranes, and carbohydrates serve as energy sources and structural components.

It is at the Institut Pasteur in the 1950s that André Lwoff, Jacques Monod and his wife Agnes Ullmann made significant discoveries in the role of messenger \gls{RNA}, gene regulations, and genetic programs to the function of the cell. Together with François Jacob, yet another Pasteur Institute scientist, they proposed the operon model of gene regulation in prokaryotes, which explained how genes are turned on and off in response to environmental signals. For their discoveries, François, André and Jacques were awarded the Nobel Prize in Physiology or Medicine in 1965\cite{FrontMatter2003}. This is again at Institut Pasteur, near the Monod's and Jacob's buildings that this Ph.D. was undertaken, trying to understand further mRNA's role and the cell's regulation through AI models.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/monod.jpg}
  \caption{Lwoff, Jacob, Monod.}
  \label{fig:lwoffjacobmonod}
\end{figure}

\subsubsection{RNA}

Indeed, \gls{RNA} biology is a critical aspect of cellular function, encompassing processes such as transcription, translation, and regulation. Transcription is the process by which \gls{DNA} is copied into \gls{RNA}, which then serves as a template for protein synthesis during translation. Regulation of these processes is essential for maintaining cellular homeostasis and responding to environmental changes. This regulation can occur at multiple levels, including transcriptional control, RNA processing, and post-translational modifications\cite{Alberts2015CellsGenomes}.

The \gls{RNA} hypothesis posits that \gls{RNA} molecules were the first self-replicating entities, leading to the evolution of life as we know it. This hypothesis suggests that early life forms relied on \gls{RNA} for both genetic information storage and catalytic functions, paving the way for the development of \gls{DNA} and proteins, showing how \gls{RNA} might be one of the most central components of the cell\cite{RNAWorld2024}.

Many different types of \gls{RNA} exist, each with distinct functions. Messenger \gls{RNA} (\gls{mRNA}) carries genetic information from \gls{DNA} to ribosomes for protein synthesis, while transfer \gls{RNA} (\gls{tRNA}) and ribosomal \gls{RNA} (\gls{rRNA}) allow translation of \gls{mRNA}s into proteins. Other types of \gls{RNA}, such as small interfering \gls{RNA} (\gls{siRNA}) and microRNA (\gls{miRNA}), are involved in gene regulation and silencing. Long-non-coding \gls{RNA}s (\gls{lncRNA}s) also play crucial roles in regulating gene expression and chromatin structure\cite{chenSmallLongNoncoding2024}. Unfortunately, many of these \gls{RNA} types are still poorly understood, and their functions are an active area of research. In eukaryotic cells, like our own, RNAs are produced through genetic expression and are actively regulated by the cell. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\textwidth]{./figures/dogma.png}
  \caption{The central dogma of biology.}
  \label{fig:dogma}
\end{figure}

\subsubsection{gene expression}

Gene expression is the process by which information from a gene is used to synthesize a functional gene product, typically a protein. This process involves several key steps, including transcription, where \gls{DNA} is transcribed into \gls{mRNA}, and translation, where \gls{mRNA} is translated into a protein. Transcription factors (\gls{TF}s) are proteins that bind to specific \gls{DNA} sequences to regulate the transcription of genes. They play a crucial role in determining which genes are expressed in a cell at any given time, influencing cellular function and identity\cite{Alberts2015CellsGenomes}.

They also interact with other proteins, such as cohesin, which helps maintain chromatin and the specific 3D structure of the \gls{DNA}. Chromatin is the complex of \gls{DNA} and proteins that forms chromosomes within the nucleus of eukaryotic cells. The organization of chromatin is essential for regulating gene expression, as it determines the accessibility of \gls{DNA} to transcription machinery.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/grn-inference_1.png}
  \caption{the classic view of the gene expression and regulation}
  \label{fig:grn_1}
\end{figure}

\subsubsection{regulation}

In this context, biologists have relied on the concept of gene regulatory networks (\gls{GRN}s) to simplify the complex interactions within the cell \cite{badia-i-mompelGeneRegulatoryNetwork2023}. GRNs are networks of molecular interactions that govern gene expression levels in a cell. They consist of genes, transcription factors, and other regulatory elements that interact to control the timing and level of gene expression. Although very coarse and likely wrong in many ways, these modeled interactions allow researchers to gain insights into how cells might respond to various stimuli, differentiate into specific cell types, and maintain homeostasis.

Gene networks (\gls{GN}s) are a more general concept that encompasses not only gene regulatory networks but also other types of interactions, such as protein-protein interactions and metabolic pathways. While GRNs focus specifically on the regulation of gene expression, GNs provide a broader view of the cellular processes and interactions that contribute to the overall function of the cell.

In this thesis it is using gene expression measurement that we will train models of the cell, and we will often refer to gene regulatory networks as a way to understand how these models work and what they have learned about the cell. We will now focus further on the measurement mechanisms underpinning the gene expression data we will use in this thesis.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/grn-inference_2.png}
  \caption{infering gene networks with single cell data}
  \label{fig:grn_2}
\end{figure}

\subsection{Single-cell genomics}

\subsubsection{sequencing}

These measurements started within the field of genomics. Genomics is the study of an organism's complete set of \gls{DNA}, including all of its genes. It involves sequencing and analyzing the entire genome to understand its structure, function, and evolution. Then, the development of high-throughput sequencing technologies revolutionized genomics, allowing researchers to sequence entire genomes quickly and cost-effectively.

Initially, \gls{DNA} sequencing was performed using Sanger sequencing, the first method developed for this purpose. It involves selectively incorporating chain-terminating dideoxynucleotides during \gls{DNA} replication, allowing researchers to determine the sequence of nucleotides in a \gls{DNA} molecule. This method was labor-intensive and time-consuming, but it laid the foundation for modern sequencing techniques\cite{sangerDNASequencingChainterminating1977}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/sequencing_init.png}
  \caption{main method for sequencing DNA}
  \label{fig:sequencing}
\end{figure}


\subsubsection{next-generation sequencing}

Nowadays, we use next-generation high throughput sequencing (\gls{NGS}) technologies, which allow for massively parallel sequencing of millions of \gls{DNA} fragments—also called reads, simultaneously. This has significantly reduced the time and cost required for genome sequencing, enabling large-scale genomic studies and personalized medicine approaches\cite{huNextgenerationSequencingTechnologies2021}.

Reads, small chunks of \gls{DNA}, often likened to tiny puzzle pieces, are multiplied and sequenced in parallel. The resulting sequences are then aligned (or mapped) to a reference genome, which serves as a template for assembling the reads into a complete genome sequence. The average number of overlapping reads that cover a specific region of the genome is referred to as sequencing depth or coverage. Higher sequencing depth generally leads to more accurate and reliable results, as it reduces the likelihood of errors and increases the confidence in variant detection.

In the last decade, large-scale sequencing efforts such as the 1 million genomes project, the Human Genome Project, and the 1000 Genomes Project have provided valuable insights into human genetic variation and disease susceptibility\cite{1000genomesprojectconsortiumGlobalReferenceHuman2015,landerInitialSequencingAnalysis2001}. Genetic sequencing now allows us to define the genetic basis of many diseases, identify which drug might work for specific patients, and establish follow-ups for high-risk patients. It is driving more and more clinical decisions and is becoming a standard part of patient care.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./figures/illumina.jpg}
  \caption{illumina next-generation sequencing chip}
  \label{fig:illumina}
\end{figure}

\subsubsection{new modalities in sequencing}

But sequencing also allowed many new applications, such as the study of gene expression using sequencing. Here we are using sequencer to read the \gls{mRNA}s present within specific tissues, a process known as \gls{RNA} sequencing (\gls{RNA-seq})\cite{starkRNASequencingTeenage2019}. Other examples abound, such as the sequencing of \gls{DNA} states such as methylation (\gls{BS-seq}), open chromatin (\gls{ATAC-seq}), and chromatin immunoprecipitation (\gls{ChIP-seq}) provides a view of how the genome is being read at a point in time. From \gls{DNA} and its mutations to its state in different contexts and how these lead to changes in \gls{RNA}'s expression and state, we have begung to develop a holistic view of various cellular mechanisms\cite{buenrostroATACseqMethodAssaying2015}.

However, these methods only provided a view of the average of sequences across the cells of a tissue, not of the individual cells. In 2014, the first single-cell RNA sequencing (\gls{scRNA-seq}) methods were developed, allowing researchers to analyze gene expression at the single-cell level\cite{macoskoHighlyParallelGenomewide2015}. This was a breakthrough in genomics, as it enabled the study of cellular heterogeneity and the identification of rare cell populations that was previously missed from bulk analyses.

Since then, single-cell sequencing technologies have rapidly advanced, with new methods being developed to sequence the other omics modalities at the single-cell level. Studies conducted on tens of thousands of cells in the 2010s are now done on millions of cells\cite{regevHumanCellAtlas}, generating what has been called cell atlases.

In our work, we are gathering all the publicly available \gls{scRNA-seq} datasets and atlases across tissues, diseases and species, to train foundation models.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./figures/spatial.png}
  \caption{spatial transcriptomics data example}
  \label{fig:spatial}
\end{figure}

\subsubsection{future...}

Recently, the development of spatial transcriptomics and imaging techniques has allowed researchers to study the spatial organization of gene expression within tissues, providing a more comprehensive view of cellular function in its native context, along with cell imaging\cite{stahlVisualizationAnalysisGene2016}. Protein measurements are also being developed, unlocking an additional layer of information\cite{luoAdvancesProteinSequencing2025}.

Current applications have been primarily focused on the understanding of diseases and drug effects within tissues. The technique allowed the identification of hundreds of new cell types and states, and improved the study of cellular development and differentiation. It had a substantial impact on cancer, neurological diseases, and immunology\cite{josephSingleCellAnalysis2021,kongLandscapeImmuneDysregulation2023}. 

Together with the development of perturbation techniques such as \gls{CRISPR}-cas9 \cite{dixitPerturbseqDissectingMolecular2016, adamsonMultiplexedSingleCellCRISPR2016}, we can now go beyond observations and start to understand the causal relationships between genes and their functions. Indeed, these \gls{CRISPR} screens allow researchers to systematically deactivate genes in individual cells and observe the resulting changes in gene expression, providing further insights into gene function and regulatory networks.

\subsubsection{...and challenges}

However, single-cell sequencing also comes with its own set of challenges. One of the main issues is the sparsity and noise underlying most of the current single-cell sequencing methods. Secondly, some strong "batch-effect" biases in the data generation process can make it challenging to perform analysis across datasets.

Worse, the dataset themselves have issues. We currently miss many tissues and cell types that are rare and hard to sequence. We have yet to perform such analysis on species other than humans and mice, and specific cell states related to aging, fetal growth, disease, and treatment are missing. It is hoped that machine learning and artificial intelligence might allow us to solve these issues and infer the missing data computationally.

\subsection{Current single cell tasks}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./figures/grn-inference_3.png}
  \caption{single cell data analysis pipeline and relationship to gene networks}
  \label{fig:grn_3}
\end{figure}

While these models could be very performant, reliable, and reproducible, benchmarks that align with real use-cases from the user's perspective remain scarce. Fortunately, the field has matured sufficiently for the creation of standardized tasks and pipelines to understand, assess and analyse the data.

In single-cell \gls{RNA-seq} data, a typical pipeline is as follows and contains several possible steps:

\begin{enumerate}
  \item \textbf{Alignment.} It all starts with preprocessing the raw sequencing data: detecting/imputing the cell index, aligning the sequencing reads with a reference genome's gene locations, detecting low-quality cells, reads, doublet events, cell death events, and more\cite{dobinSTARUltrafastUniversal2013}. These choices will impact the output dataset as biases.

  \item \textbf{Normalization and clustering.} The data might be normalized to correct for sequencing depth and other gene-level biases, and clusters of cells would be defined based on their expression profile similarity. Finally, differential-expression analysis is performed, whereby clusters of cells are compared to identify genes that are differentially expressed between them. This might also be done after the other steps\cite{haghverdiBatchEffectsSinglecell2018}.

  \item \textbf{Batch correction / atlas alignment.} The dataset might be aligned to a reference atlas in cases where this exists. This is done by using batch-correction methods, often built around nearest-neighbor mapping, matrix factorization, or neural networks (\gls{NN}) called variational auto-encoders (\gls{VAE}s), which are the current state of the art in the domain \cite{scvi}.

  \item \textbf{Annotation and labeling.} Cluster-level and dataset-level labels might be inferred, such as cell type, tissue, disease, and age. These often come from prior knowledge, manual annotation based on differential expression features, automated tools, or alignment with other prelabeled datasets. Thanks to these correlations between gene expression, labels might be defined and guide further research. Clustering tools and dimensionality reduction techniques used for visualizing high-dimensional datasets as point clouds on 2D surfaces often employ nearest-neighbor-based methods\cite{hwangSinglecellRNASequencing2018}.

  \item \textbf{Denoising / imputation.} In cases where specific clusters contain a low number of cells, denoising or zero-imputation methods can be used \cite{eraslanSinglecellRNAseqDenoising2019}, but they haven't shown consistent usefulness in practice since they rely on cluster-level information, which is itself used for most other tasks. Nearest-neighbors-based smoothing was the previous state of the art\cite{dijkRecoveringGeneInteractions2018}.

  \item \textbf{Multimodal integration.} If users have access to datasets of the same tissue from other modalities (\gls{scATAC-seq}, \gls{BS-seq}, protein measurements, imaging, etc.), multimodal alignment methods can be applied, often reusing prior knowledge and paired datasets where modalities are measured in the same cells. Such alignments help bridge genotype (\gls{DNA}/mutations) and phenotype (expression, protein levels, pathology). State-of-the-art tools combine \gls{VAE}s and heuristics.

  \item \textbf{Trajectory inference.} If the dataset was measured in a non-static context, one can infer "cellular trajectories", i.e., how cells transition from one state to another based on many single-cell snapshots, such as during differentiation or perturbations. Many tools exist for trajectory inference; state-of-the-art methods often employ techniques like optimal transport (\gls{OT}) \cite{chenSTREAMSinglecellTrajectories2018}.

  \item \textbf{Spatial analysis and cell-cell interactions.} If the dataset contains spatial information—how cells are positioned in a tissue—one can infer cell-cell interactions, meaning how cells influence each other based on proximity and expression profiles. This often requires specialized imaging techniques, where cell morphology is also assessed. Spatial interaction analysis is still nascent; methods frequently use foundation models and heuristics based on cell type and proximity\cite{pallaSquidpyScalableFramework2022}.

  \item \textbf{Perturbation response prediction.} If the dataset includes perturbation experiments, one can predict how cells respond to specific perturbations, such as drug treatments or genetic modifications. This helps identify key regulatory pathways and potential therapeutic targets\cite{lotfollahiScGenPredictsSinglecell2019}.
\end{enumerate}

These analysis and tools are available in a set of packages called scverse, which our work relies heavily on and has contributed to.

\subsection{The AI virtual cell}

%TODO: it inspired my research, what are the scientific questions from it and what did I do there
% SPOIL the PAPERS

%TODO: better state of the art

% TODO: pitch of why this section and transition to the next

% TODO: conclude what is related there with the thesis (what is important) si je vous ai parlé de ça c'est pour ça... prendre par la main (garder similaire par section: symmetrie / musique)

A virtual cell model has been the dream of computational and systems biologists for decades. Initially, these models were based on simplified representations of cellular processes, often focusing on specific pathways or interactions. The models examined chemical reaction parameters involving proteins, \gls{RNA}s, and \gls{DNA}. However, in addition to computational challenges, these models were not able to generate realistic predictions of cellular behavior.

Nowadays, the dominant idea is that artificial intelligence would allow us to solve some of these problems. But first, what is \gls{AI}, and what is the difference between machine learning, data science, and informatics?

\subsection{AI and neural networks}

\subsubsection{definitions}

\textbf{Data Science} encompasses the gathering, management, and analysis of data in information systems. Machine Learning happens when one uses statistical methods to generate predictions from data. But these methods can be more complex, and while they can be seen in the framework of statistics, they also have an underpinning in other domains, like neuroscience (with neural networks) and applied mathematics with Algebra, Topology, Analysis, and most importantly, Optimization. These methods often allow powerful modeling of the data statistics.

Artificial Intelligence (\gls{AI}) is a broad term that has had multiple meanings in society and culture. But it mainly refers to applications of machine learning methods to human and animal-related tasks, such as understanding images, videos, speech, and text, as well as robot manipulation. This creates many bridges and links between data science, machine learning, statistics, informatics, and \gls{AI}.

Recently, Machine Learning (\gls{ML}) has made great strides in many areas, primarily due to a significant increase in data generation, along with improvements in optimization methods and neural networks.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\textwidth]{./figures/definitions_ai.png}
  \caption{placing terms in their context. From Deep Learning Book.}
  \label{fig:definitions}
\end{figure}

\subsubsection{intuition}

These tools convert concepts and objects into high-dimensional vectors, called embeddings. In Neural Networks (\gls{NN}) these embedding vectors are then processed through layers of mathematical operations, often involving matrix multiplications and non-linear functions\cite{Goodfellow-et-al-2016}. The layers are designed to learn hierarchical representations of the input data, capturing increasingly complex features as the data passes through the network.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\textwidth]{./figures/ffn.png}
  \caption{example of a feed-forward neural network architecture, drawn in 2 different styles. From Deep Learning Book.}
  \label{fig:ffn}
\end{figure}

Large Language Models (\gls{LLM}s) have become ubiquitous nowadays, and many in the field are trying to generalize them to \textbf{world models}\cite{haWorldModels2018}. These models, trained on physics simulations and videos, aim to predict the next state of complex physical phenomena, such as what happens when someone throws a ball or the physics of water flowing down a river.

They have already shown promise in powering humanoid robots. Google DeepMind recently released a model for predicting the weather better and faster than the best "physics-based" models\cite{lamLearningSkillfulMediumrange2023}. But why and how are these models so powerful? While this remains an active area of research, essential theories have been presented. 

\subsubsection{why does it work?}

Contrary to the commonly accepted machine learning dogma, deep learning researchers showed, in the 2010s, that increasing the number of parameters did not necessarily lead to overfitting, i.e., the model learning by heart to reproduce the data. Instead, thanks to some light regularization methods, such as dropout and weight decay, which involve randomly removing some neurons during training and penalizing large weights, the models were able to learn more complex patterns in the data.

Thanks also to \gls{GPU}s and interconnects allowing large parallel matrix operations in a distributed manner, such models could be scaled to orders of magnitude more parameters, thereby demonstrating the power of weak learning methods\cite{lecunDeepLearning2015}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/vgg.png}
  \caption{Example of the VGG architecture, which allowed training deeper neural networks thanks to skip connections.}
  \label{fig:vgg}
\end{figure}

Finally, advanced first-order optimization methods, such as Adam, together with back-propagation, allowed even such a large number of parameters to be trained efficiently\cite{kingmaAdamMethodStochastic2017}.

\subsubsection{optimization and loss landscapes}

Indeed, stochastic gradient descent (\gls{SGD}) methods like Adam not only minimize the loss function but also help escape local minima and saddle points\cite{liVisualizingLossLandscape2018}. 

To understand this, we need to understand the loss landscape. Imagine a 3D landscape where the height represents the loss value, and the two other "surface" dimensions are the model's parameters. The goal of the model is to find the lowest point in this landscape, which corresponds to the best set of parameters. However, this landscape is often very complex, with many local minima and saddle regions. The model wanders blindly and can only sense its immediate surroundings. If it falls into a local minimum, it should get stuck.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/loss-surface.png}
  \caption{visualization of a loss landscape with and without skip connections}
  \label{fig:loss_surface}
\end{figure}

In very high dimensions, however, when the model has millions of parameters and \gls{SGD}, by being stochastic, alters the loss landscape each time, the model can escape these local minima and saddle points.

Behind this unexpected behavior is the unsettling theory of emergence. Or how small objects can combine and interact in ways that would be unexpected and difficult to predict based on their individual properties. This theory tries to explain phenomena in dunes, snowflakes, ant colonies, and life itself, which might explain how large neural networks achieve such complex behaviors\cite{weiEmergentAbilitiesLarge2022}.

\subsection{Bio-Foundation models}

\subsubsection{large language models}

For text, images, videos, and audio, \gls{LLM}s are everywhere now. In other domains, we often call similar models, trained on all the available data, foundation models. 

This epitomizes a switch from small, simple neural architectures trained per dataset to larger neural architectures, called transformers \cite{vaswaniAttentionAllYou2023}, trained across the entirety of the available data. These models are called foundation models. The stated goal is for them to generalize better to their training data and task, and perform better than state-of-the-art small models.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/transformer.png}
  \caption{Transformer architecture overview. From Attention is all you need.}
  \label{fig:transformer}
\end{figure}

\subsubsection{fine tuning}

After this initial training phase using a very generic unsupervised task, often referred to as pre-training, these models can be further fine-tuned to specific downstream objectives. Fine-tuning involves training the model on a smaller dataset with a goal specific to the task we want it to learn. In other domains, this has allowed the model to adapt its learned representations, enabling it to quickly gain higher accuracy than would be achieved without pretraining. For example, going from predicting the next word in a sentence to classifying the sentiment of a text, or even to being a chatbot.
For this reason, essential innovations in fine-tuning methods involve creating losses that best align the model with the task\cite{bommasaniOpportunitiesRisksFoundation2022,duanOneShotImitationLearning}.

These foundation models are still in their infancy. We want to understand how they work, if they can be useful to current research, and how we can improve on their training, architecture, and generalization capabilities for the modality at hand.

\subsubsection{initial single-cell foundation models}

The first practical example of a single-cell (\gls{RNA-seq}) foundation model could have been scBERT, released in 2021. However, it was only used and benchmarked for cell type classification and pretrained on 1 million single cells\cite{yangScBERTLargescalePretrained2022a}. The first real foundational model, Geneformer, was released a year later \cite{theodorisTransferLearningEnables2023}. There, the author displayed the model's ability to perform various single-cell tasks, such as cell type classification, gene regulatory network inference, and perturbation prediction. Geneformer was trained on a much larger dataset of 33 million single-cells.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/geneformer.png}
  \caption{the geneformer model, where genes are represented as words and cells as sentences where the genes are ordered by their expression level. From Geneformer.}
  \label{fig:geneformer}
\end{figure}

However, Geneformer, like scBERT, was really a complete reuse of the \gls{BERT} model architecture. Furthermore, while showcasing interesting behaviors, it did not outperform simpler models on classifcal benchmarks of single-cell data\cite{boiarskyDeepDiveSingleCell2023,alsabbaghFoundationModelsMeet2023}. 

\subsubsection{current single-cell foundation models}

In 2023, a year after Geneformer, a few additional foundation models were released from scGPT \cite{cuiScGPTBuildingFoundation2024}, showcasing a take on the \gls{GPT} architecture and presenting various losses for fine-tuning the model. It was the first example of fine-tuning in a single cell and a more in-depth benchmark of the model across four different abilities: cell type prediction, gene network inference, perturbation prediction, and batch correction. However, it did not outperform state-of-the-art (\gls{SOTA}) methods\cite{boiarskyDeepDiveSingleCell2023}.

At the same time, Universal Cell Embedding \cite{rosenUniversalCellEmbeddings2023} demonstrated how one could train these models across multiple species to achieve state-of-the-art cross-species cell embeddings —vectors that represent the cell according to the model. It also showed a new kind of loss function to generate embeddings of the cells.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{./figures/uce.png}
  \caption{low dimensional visualization of universal cell embeddings across species. Each point is a cell where its position is position is near similar cells according to this Foundation Models. From Universal Cell Embeddings.}
  \label{fig:UCE}
\end{figure}

Finally, scFoundation \cite{haoLargescaleFoundationModel2024}, despite being closed-source, showcased a truly novel architecture specifically built for single-cell data and a novel training method based on the noise-to-sequencing-depth relationship of single-cell data.