\chapter*{Preamble} % Main chapter title
\addcontentsline{toc}{chapter}{Preamble}  

This Ph.D. started relatively late in my career. I'd like to spend some of the introduction mentioning the reasons that pushed me to do a Ph.D. now.

\section{My background for this thesis}

Many of the opportunities I had coming out of school have been very exciting. Initially, I decided to create a company called PiPle with a friend, Paul Best, who is now a Post-doctoral Researcher at the University of Vienna in Machine Learning for bio-acoustics. 

Funily enough, it was completely unrelated to biology. We worked on creating novel means of communication. We had—and still have—big ideas about how to improve utterly inadequate messaging apps, emails, and similar tools using machine learning and innovative designs. Doing this, we learnt a lot about managing complex projects, selling ideas, building large codebases, teamwork, and designing interfaces.

However, we did not gain enough traction from this, and we felt that after a year of hard work, the road ahead was paved with too many sacrifices. 

This is when I passed on Ph.D. opportunities a second time to work at the Broad Institute instead. Having visited the labs, Boston, and Kendall Square, I knew that this was the kind of experience I wanted to have, and Ph.D.s seemed long and cumbersome. At Broad, I worked on many very-high-impact research projects, and I felt I was part of something bigger than myself. I published as the first author and even started my own research projects, which would inform the thesis I am presenting here. 

While I still understood that a Ph.D. was the best place to undergo such projects, I was uncertain about the specifics. I also understood the length, harshness, and sometimes arbitrary nature of U.S. Ph.D. programs. I also wanted to continue working on team-based projects and wanted to experience the start-up environment.

Along with some other personal decisions, it led me to return to France and work as the team lead of the computational biology group at Whitelab Genomics in Paris.

In Whitelab, I learned how to build a team and how to manage people. I learned a lot about what it means to grow companies from 10 to 50 people. I also learned about the biotech industry and how to build and sell such products.

Whitelab had a good mix of expertise in computational biology, machine learning, structural biology, and business development. While starting the first project there, I significantly enhanced the potential of foundation models for the biotech industry. 

From DNA language models to cell foundation models and knowledge-graph-based models, it became clear that they would be the path forward to aggregate the sparse and disparate information across many fields of biology and medicine. 

I was not looking for any other positions and intended to stay at least a few years to assess how we had grown during that time.

However, I was already in contact with Laura Cantini, with whom I had previously discussed Ph.D. projects. At some point, Laura came back to me with this Ph.D. proposal. I spent the good part of a month in a challenging position. Thinking about which decision would not become a regret in the future.

There was no perfect time to do this, but it felt like it was now or never. I was also very impressed by the level of various Ph.D. students in the labs of Laura and Gabriel. Seeing people 4 years younger than me, already having such a high level of expertise and knowledge, was very humbling. Finally, the Ph.D. topic and group were really on point with what I wanted to do. I knew I could do it in way less than 3 years, and Laura was ok with it. But mostly, my work/life environment was welcoming, surrounded by family, friends, and activities. I knew what I wanted to work on and what I wanted to learn.

\section{Introduction}

In this Thesis, we will focus on models of the Cell.

In the mid-17th century, Robert Hooke made a groundbreaking discovery while observing a piece of cork through his microscope. He observed structures that he named ``cells" and, as a result, marked the beginning of cellular biology. Cells have since been identified as life's fundamental structural and functional units, and biologists have endeavored to map the diverse cell types that comprise multicellular organisms. Additionally, they have sought to understand the transient cell states that occur during development, disease progression, and tissue regeneration. 

The objectives of cellular biologists are to understand and control, with the dream of engineering life from plants to animals and even generating entirely new synthetic life. But what for?

\subsection{The promises of cellular biology}

Before developing a drug for a disease, one must understand the disease and identify a potential target gene or set of target genes. It refers to the genes in specific cell types that need to be reactivated, deactivated, or modified to address the disease's underlying mechanism.

But drugs don't have to be small molecules. CAR-T cell therapies have revolutionized blood cancer treatment by modifying a patient's own immune cells to fight the cancer. Similar approaches could be developed for many other conditions. Here, the drug becomes a cell.

But diseases are not the nails one could hit with such a mighty hammer. Indeed, life is everywhere, and engineering has already helped us make better crops, create synthetic meat, and design fungi that remove pollution. When people talk about nanorobots, I urge you to think about engineered cells.

Richard Feynman famously said, "What I cannot create, I do not understand." Therefore, the Modeling of the cell stands as a key milestone in cellular biology, and indeed, one cannot succeed in the aforementioned promises without a correct cellular blueprint.

Therefore, hundreds of companies, from tech to bio, and dozens of institutes are pursuing efforts to create such virtual cellular models. Achieving even limited predictive accuracy would have significant impacts on cellular biology.

We will investigate the primary data modality used to create cell models and how we can train them using modern machine learning methods based on neural networks. We will explore how these models can be useful today and how we might make them better in the future. For that, we will need to understand some essential facts about the cell and how biologists think about it.

\subsection{GRN and the cell}

The cell is the fundamental unit of life and is composed of various components, including proteins, nucleic acids, lipids, and carbohydrates. Each of these components plays a crucial role in the cell's structure and function. Proteins are responsible for most cellular processes, while nucleic acids (DNA and RNA) carry genetic information. Lipids form cell membranes, and carbohydrates serve as energy sources and structural components.

RNA biology is a critical aspect of cellular function, encompassing processes such as transcription, translation, and regulation. Transcription is the process by which DNA is copied into RNA, which then serves as a template for protein synthesis during translation. Regulation of these processes is essential for maintaining cellular homeostasis and responding to environmental changes. This regulation can occur at multiple levels, including transcriptional control, RNA processing, and post-translational modifications.

The RNA hypothesis posits that RNA molecules were the first self-replicating entities, leading to the evolution of life as we know it. This hypothesis suggests that early life forms relied on RNA for both genetic information storage and catalytic functions, paving the way for the development of DNA and proteins, showing how RNA might be one of the most central components of the cell.

Indeed, many different types of RNA exist, each with distinct functions. Messenger RNA (mRNA) carries genetic information from DNA to ribosomes for protein synthesis, while transfer RNA (tRNA) and ribosomal RNA (rRNA) allow translation of mRNAs into proteins. Other types of RNA, such as small interfering RNA (siRNA) and microRNA (miRNA), are involved in gene regulation and silencing. Long-non-coding RNAs (lncRNAs) also play crucial roles in regulating gene expression and chromatin structure. Unfortunately, many of these RNA types are still poorly understood, and their functions are an active area of research.

Gene expression is the process by which information from a gene is used to synthesize a functional gene product, typically a protein. This process involves several key steps, including transcription, where DNA is transcribed into mRNA, and translation, where mRNA is translated into a protein. Transcription factors (TFs) are proteins that bind to specific DNA sequences to regulate the transcription of genes. They play a crucial role in determining which genes are expressed in a cell at any given time, influencing cellular function and identity.

They also interact with other proteins, such as cohesin, which helps maintain chromatin and the specific 3D structure of the DNA. Chromatin is the complex of DNA and proteins that forms chromosomes within the nucleus of eukaryotic cells. The organization of chromatin is essential for regulating gene expression, as it determines the accessibility of DNA to transcription machinery.

%TODO: add refs and figures%

In this context, biologists have relied on the concept of gene regulatory networks (GRNs) to simplify the complex interactions within the cell. GRNs are networks of molecular interactions that govern gene expression levels in a cell. They consist of genes, transcription factors, and other regulatory elements that interact to control the timing and level of gene expression. Although very coarse and likely wrong in many ways, these modeled interactions allow researchers to gain insights into how cells might respond to various stimuli, differentiate into specific cell types, and maintain homeostasis.

Gene networks (GNs) are a more general concept that encompasses not only gene regulatory networks but also other types of interactions, such as protein-protein interactions and metabolic pathways. While GRNs focus specifically on the regulation of gene expression, GNs provide a broader view of the cellular processes and interactions that contribute to the overall function of the cell.

\subsection{Single-cell genomics}

Genomics is the study of an organism's complete set of DNA, including all of its genes. It involves sequencing and analyzing the entire genome to understand its structure, function, and evolution. The development of high-throughput sequencing technologies has revolutionized genomics, allowing researchers to sequence entire genomes quickly and cost-effectively.

Initially, DNA sequencing was performed using Sanger sequencing, the first method developed for this purpose. It involves selectively incorporating chain-terminating dideoxynucleotides during DNA replication, allowing researchers to determine the sequence of nucleotides in a DNA molecule. This method was labor-intensive and time-consuming, but it laid the foundation for modern sequencing techniques.

Nowadays, we use next-generation sequencing (NGS) technologies, which allow for massively parallel sequencing of millions of DNA fragments—also called reads—simultaneously. This has significantly reduced the time and cost required for genome sequencing, enabling large-scale genomic studies and personalized medicine approaches.

Reads, these small chunks of DNA, often likened to tiny puzzle pieces, are multiplied and sequenced in parallel. The resulting sequences are then aligned (or mapped) to a reference genome, which serves as a template for assembling the reads into a complete genome sequence. The average number of overlapping reads that cover a specific region of the genome is referred to as sequencing depth or coverage. Higher sequencing depth generally leads to more accurate and reliable results, as it reduces the likelihood of errors and increases the confidence in variant detection.

Large-scale sequencing efforts such as the 1 million genomes project, the Human Genome Project, and the 1000 Genomes Project have provided valuable insights into human genetic variation and disease susceptibility. Genetic sequencing now allows us to define the genetic basis of many diseases, identify which drug might work for specific patients, and establish follow-ups for high-risk patients. It is driving more and more clinical decisions and is becoming a standard part of patient care.

But sequencing also allowed many new applications, such as the study of gene expression by sequencing instead the mRNAs in tissues, a process known as RNA sequencing (RNA-seq). The sequencing of DNA states such as methylation (BS-seq), open chromatin (ATAC-seq), and chromatin immunoprecipitation (ChIP-seq) provides a view of how the genome is being read at a point in time. From DNA and its mutations to its state in different contexts and how these lead to changes in RNAs and their states, we begin to develop a holistic view of various cellular mechanisms.

However, these methods only provide a view of the average state of the tissue, not of the individual cells. In 2014, the first single-cell RNA sequencing (scRNA-seq) methods were developed, allowing researchers to analyze gene expression at the single-cell level. This was a breakthrough in genomics, as it enabled the study of cellular heterogeneity and the identification of rare cell populations that may be missed in bulk RNA-seq analyses.

Since then, single-cell sequencing technologies have rapidly advanced, with new methods being developed to sequence the other omics modalities at the single-cell level. Studies conducted on tens of thousands of cells in the 2010s are now done on millions of cells. 

Moreover, the development of spatial transcriptomics and imaging techniques has allowed researchers to study the spatial organization of gene expression within tissues, providing a more comprehensive view of cellular function in its native context, along with cell imaging. Protein measurements are also being developed, unlocking an additional layer of information. 

Current applications have been primarily focused on the understanding of diseases and drug effects within tissues. The technique allowed the identification of hundreds of new cell types and states, and improved the study of cellular development and differentiation. It had a substantial impact on cancer, neurological diseases, and immunology. 

Finally, together with the development of perturbation techniques such as CRISPR screens, we can now go beyond observations and start to understand the causal relationships between genes and their functions. Indeed, CRISPR screens allow researchers to systematically deactivate genes in individual cells and observe the resulting changes in gene expression, providing insights into gene function and regulatory networks.

However, single-cell sequencing also comes with its own set of challenges. One of the main issues is the sparsity and noise underlying most of the current single-cell sequencing methods. Secondly, some strong "batch-effect" biases in the data generation process can make it challenging to perform analysis across datasets.

Worse, the dataset themselves have issues. We currently miss many tissues and cell types that are rare and hard to sequence. We have yet to perform such analysis on species other than humans and mice, and specific cell states related to aging, fetal growth, disease, and treatment are missing. It is hoped that machine learning and AI might allow us to solve these issues and infer the missing data computationally.

\subsection{Current single cell tasks}

While these models could be very performant, reliable, and reproducible, benchmarks that align with real use-cases from the user's perspective remain scarce. In single-cell RNA-seq data, a typical pipeline is as follows:

\beging{enumerate}
  \item It all starts with preprocessing the raw sequencing data: detecting/imputing the cell index, aligning the sequencing reads with a reference genome's gene locations, detecting low-quality cells, reads, doublet events, cell death events, and more. These choices will impact the output dataset as biases. 

  \item The data might be normalized to correct for sequencing depth and other gene-level biases, and clusters of cells would be defined based on their expression profile similarity. Finally, differential-expression analysis is performed, whereby clusters of cells are compared to identify genes that are differentially expressed between them. This might also be done after the other steps.

  \item The dataset might be aligned to a reference atlas in cases where this exists. This is done by using batch-correction methods, often built around nearest neighbor mapping, matrix factorization, or Neural Networks (NN) called Variational Auto-Encoders (VAEs), which are the current state of the art in the domain.

  \item Cluster-level and dataset-level labels might be inferred, such as cell-type, tissue, disease, and age. These often come from prior knowledge, manual annotation based on differential expression features, automated tools, or alignment with other prelabeled datasets. Thanks to these correlations between gene expression, labels might be defined and guide further research. Clustering tools and dimensionality reduction techniques used for visualizing high-dimensional datasets as point clouds on 2D flat surfaces often employ nearest-neighbor-based methods. 
  \item In the case where specific clusters contain a low amount of cells, denoising or zero imputation methods can be used, but they haven't shown usefulness in practice since they are based on using cluster-level information, which is itself already used for most other tasks. Nearest-neighbors-based smoothing is the state of the art in this domain.
  \item In the case where users would have access to a dataset of a similar tissue from other modalities like sc-ATAC-seq, sc-BS-seq, or protein measurements, imaging data, and more. Such multimodal alignment methods exist, often reusing prior knowledge and paired datasets where these two modalities are measured in the same cells. Thanks to these alignments, one can define more complex relationships between how DNA readings and related DNA mutations impact the expression of genes, and how these affect the downstream expression of proteins—bridging the map between the genotype (DNA) and the phenotype (cell features, patient diseases, etc). State-of-the-art tools there combine VAEs and heuristics.

  \item If the dataset was measured in a non-static context, one can infer "cellular trajectories", meaning how cells seem to go from one state to another based on single measurements of many cells, such as in differentiation or during perturbations. Many tools exist for such trajectory inference, but it is an open question that often requires specific RNA-sequencing methods. State-of-the-art methods usually employ techniques like optimal transport (OT).

  \item If the dataset contains spatial information: how cells were positioned in a tissue, one can infer "cell-cell interactions", meaning how cells influence each other based on their proximity and expression profiles. This often requires specialized imaging techniques, where cell morphology is also assessed. Cell-cell interaction is still in its infancy, but methods there frequently use foundation models and heuristics based on cell type analysis and proximity.

  \item If the dataset includes perturbation experiments, one can predict how cells respond to specific perturbations, such as drug treatments or genetic modifications. This can help identify key regulatory pathways and potential therapeutic targets.
\end{enumerate}

\subsection{AIVC: the virtual cell model}

A virtual cell model has been the dream of computational and systems biologists for decades. Initially, these models were based on simplified representations of cellular processes, often focusing on specific pathways or interactions. The models examined chemical reaction parameters involving proteins, RNAs, and DNA. However, in addition to computational challenges, these models were not able to generate realistic predictions of cellular behavior.

Nowadays, the dominant idea is that Artificial Intelligence, or AI, would allow us to solve some of these problems. But first, what is AI, and what is the difference between ML, Data science, and informatics?

\subsection{AI and neural networks}

Data Science encompasses the gathering, management, and analysis of data in information systems. Machine Learning happens when one uses statistical methods to generate predictions from data. But these methods can be more complex, and while they can be seen in the framework of statistics, they also have an underpinning in other domains, like neuroscience (with neural networks) and applied mathematics with Algebra, Topology, Analysis, and most importantly, Optimization. These methods often allow powerful modeling of the data statistics.

Artificial Intelligence is a broad term that has had multiple meanings in society and culture. But it mainly refers to applications of machine learning methods to human and animal-related tasks, such as understanding images, videos, speech, and text, as well as robot manipulation. This creates many bridges and links between data science, machine learning, statistics, informatics, and AI.

Recently, ML has made great strides in many areas, primarily due to a significant increase in data generation, along with improvements in optimization methods and neural networks.

These tools convert concepts and objects into high-dimensional vectors, called embeddings. 

These embedding vectors are then processed through layers of mathematical operations, often involving matrix multiplications and non-linear functions. The layers are designed to learn hierarchical representations of the input data, capturing increasingly complex features as the data passes through the network.

Large Language Models (LLMs) have become ubiquitous nowadays, and many in the field are trying to generalize them to world models. These models, trained on physics simulations and videos, aim to predict the next state of complex physical phenomena, such as what happens when someone throws a ball or the physics of water flowing down a river.

They have already shown promise in powering humanoid robots. Google DeepMind recently released a model for predicting the weather better and faster than the best "physics-based" models.

But why and how are these models so powerful? While this remains an active area of research, essential theories have been presented. 

Contrary to the commonly accepted machine learning dogma, deep learning researchers showed, in the 2010s, that increasing the number of parameters did not necessarily lead to overfitting, i.e., the model learning by heart to reproduce the data. Instead, thanks to some light regularization methods, such as dropout and weight decay, which involve randomly removing some neurons during training and penalizing large weights, the models were able to learn more complex patterns in the data.

Thanks also to GPUs and interconnects allowing large parallel matrix operations in a distributed manner, such models could be scaled to orders of magnitude more parameters, thereby demonstrating the power of weak learning methods.

Finally, advanced first-order optimization methods, such as Adam, together with back-propagation, allowed even such a large number of parameters to be trained efficiently. 

\subsection{Optimization and loss landscapes}

Indeed, stochastic gradient descent (SGD) methods like Adam not only minimize the loss function but also help escape local minima and saddle points. 

To understand this, we need to understand the loss landscape. Imagine a 3D landscape where the height represents the loss value, and the two other "surface" dimensions are the model's parameters. The goal of the model is to find the lowest point in this landscape, which corresponds to the best set of parameters. However, this landscape is often very complex, with many local minima and saddle regions. The model wanders blindly and can only sense its immediate surroundings. If it falls into a local minimum, it should get stuck.

%TODO: add images

In very high dimensions, however, when the model has millions of parameters and SGD, by being stochastic, alters the loss landscape each time, the model can escape these local minima and saddle points.

Behind this unexpected behavior is the unsettling theory of emergence. Or how small objects can combine and interact in ways that would be unexpected and difficult to predict based on their individual properties. This theory tries to explain phenomena in dunes, snowflakes, ant colonies, and life itself, which might explain how large neural networks achieve such complex behaviors.

\subsection{LLMs \& Bio-Foundation models}

For text, images, videos, and audio, LLMs are everywhere now. In other domains, we often call similar models, trained on all the available data, foundation models. 

This epitomizes a switch from small, simple neural architectures trained per dataset to larger neural architectures, called transformers, trained across the entirety of the available data. These models are called foundation models. The stated goal is for them to generalize better to their training data and task, and perform better than state-of-the-art small models.

After this initial training phase using a very generic unsupervised task, often referred to as pre-training, these models can be further fine-tuned to specific downstream objectives. Fine-tuning involves training the model on a smaller dataset with a goal specific to the task we want it to learn. In other domains, this has allowed the model to adapt its learned representations, enabling it to quickly gain higher accuracy than would be achieved without pretraining. For example, going from predicting the next word in a sentence to classifying the sentiment of a text, or even to being a chatbot.
For this reason, essential innovations in fine-tuning methods involve creating losses that best align the model with the task.

These foundation models are still in their infancy. We want to understand how they work, if they can be useful to current research, and how we can improve on their training, architecture, and generalization capabilities for the modality at hand.

\subsection{Current single cell foundation models}

The first practical example of a single-cell (RNA-seq) foundation model could have been scBERT, released in 2021. However, it was only used and benchmarked for cell type classification and pretrained on 1 million single cells. The first real foundational model, Geneformer, was released a year later. There, the author displayed the model's ability to perform various single-cell tasks, such as cell type classification, gene regulatory network inference, and perturbation prediction. Geneformer was trained on a much larger dataset of 33 million single-cells.

However, Geneformer, like scBERT, was really a complete reuse of the BERT model architecture. Furthermore, while showcasing interesting behaviors, it did not outperform simpler models on classifcal benchmarks of single-cell data. 

In 2023, a year after Geneformer, a few additional foundation models were released from scGPT, showcasing a take on the GPT architecture and presenting various losses for fine-tuning the model. It was the first example of fine-tuning in a single cell and a more in-depth benchmark of the model across four different abilities: cell type prediction, gene network inference, perturbation prediction, and batch correction. However, it did not outperform state-of-the-art (SOTA) methods.

At the same time, Universal Cell Embedding demonstrated how one could train these models across multiple species to achieve state-of-the-art cross-species cell embeddings —vectors that represent the cell according to the model. It also showed a new kind of loss function to generate embeddings of the cells.

Finally, scFoundation, despite being closed-source, showcased a truly novel architecture specifically built for single-cell data and a novel training method based on the noise-to-sequencing-depth relationship of single-cell data.