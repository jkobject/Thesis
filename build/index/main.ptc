\babel@toc {french}{}\relax 
\babel@toc {french}{}\relax 
\contentsline {chapter}{Résumé}{i}{chapter*.1}%
\contentsline {chapter}{Abstract}{iii}{chapter*.5}%
\contentsline {chapter}{Remerciements}{vii}{chapter*.9}%
\contentsline {chapter}{Liste des figures}{xiii}{chapter*.11}%
\contentsline {chapter}{Liste des tableaux}{xv}{chapter*.12}%
\contentsline {chapter}{Liste des abréviations}{xvii}{chapter*.13}%
\contentsline {chapter}{Avant-propos}{1}{chapter*.14}%
\contentsline {section}{\numberline {0.1}My background to this thesis}{1}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}Why did I do it}{2}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}issues and realizations}{2}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}Cell and Gene Therapies}{3}{subsection.0.1.3}%
\contentsline {section}{\numberline {0.2}Introduction}{4}{section.0.2}%
\contentsline {subsection}{\numberline {0.2.1}GRN and the cell}{4}{subsection.0.2.1}%
\contentsline {subsection}{\numberline {0.2.2}single-cell omics}{6}{subsection.0.2.2}%
\contentsline {subsection}{\numberline {0.2.3}AIVC}{7}{subsection.0.2.3}%
\contentsline {chapter}{Objectifs de la thèse}{9}{chapter*.15}%
\contentsline {section}{\numberline {0.3}Objectifs de la thèse}{9}{section.0.3}%
\contentsline {subsection}{\numberline {0.3.1}Initial Thesis Proposal: Deep learning approaches as predictors of the cell's regulatory networks}{10}{subsection.0.3.1}%
\contentsline {subsubsection}{Summary}{10}{section*.16}%
\contentsline {subsubsection}{Background and objectives}{10}{section*.17}%
\contentsline {subsubsection}{Detailed description of the project}{12}{section*.18}%
\contentsline {paragraph}{WP1: Review of current tools and creation of a set of benchmarks}{12}{section*.19}%
\contentsline {paragraph}{WP2: GNN model/layers to better predict TF-gene relationships}{12}{section*.20}%
\contentsline {paragraph}{WP3: Collaboration to test the model's prediction on novel data}{13}{section*.21}%
\contentsline {subsection}{\numberline {0.3.2}Objectif 1: assessment and review of existing methods}{13}{subsection.0.3.2}%
\contentsline {subsection}{\numberline {0.3.3}Objectif 2: novel model and benchmark}{13}{subsection.0.3.3}%
\contentsline {subsection}{\numberline {0.3.4}Objectif 3: novel architecture and approaches for training multi modal, multi scale models}{14}{subsection.0.3.4}%
\contentsline {subsection}{\numberline {0.3.5}Objectif 4: General improvements demonstrating novel applications for large models}{14}{subsection.0.3.5}%
\contentsline {chapter}{\numberline {1}scPRINT: pre-training on 50 million cells allows robust gene network predictions}{15}{chapter.1}%
\contentsline {section}{\numberline {1.1}Summary}{15}{section.1.1}%
\contentsline {section}{\numberline {1.2}Introduction}{15}{section.1.2}%
\contentsline {section}{\numberline {1.3}Results}{17}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}scPRINT: a scRNAseq foundation model for gene network inference}{17}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}scPRINT recovers biological features in its gene networks}{19}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}scPRINT outperforms the state of the art on cell type-specific ground truths}{22}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}scPRINT is competitive on tasks orthogonal to GN inference}{24}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}scPRINT highlights the role of ion exchange and fibrosis in the ECM of Benign Prostatic Hyperplasia}{26}{subsection.1.3.5}%
\contentsline {section}{\numberline {1.4}Discussion}{28}{section.1.4}%
\contentsline {section}{\numberline {1.5}Methods}{30}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Architecture}{30}{subsection.1.5.1}%
\contentsline {subsubsection}{Expression encoder}{30}{section*.28}%
\contentsline {subsubsection}{Model}{33}{section*.29}%
\contentsline {subsubsection}{Expression decoder}{33}{section*.30}%
\contentsline {subsubsection}{Class decoder}{34}{section*.31}%
\contentsline {subsection}{\numberline {1.5.2}Ablation study}{35}{subsection.1.5.2}%
\contentsline {subsection}{\numberline {1.5.3}Pretraining}{35}{subsection.1.5.3}%
\contentsline {subsubsection}{Optimization method}{35}{section*.32}%
\contentsline {subsubsection}{The classification task}{36}{section*.33}%
\contentsline {subsubsection}{The denoising task}{37}{section*.34}%
\contentsline {subsubsection}{The bottleneck learning task}{38}{section*.35}%
\contentsline {subsection}{\numberline {1.5.4}scDataloader}{39}{subsection.1.5.4}%
\contentsline {subsection}{\numberline {1.5.5}Extracting meta-cell gene networks from attention matrices in scPRINT}{40}{subsection.1.5.5}%
\contentsline {subsection}{\numberline {1.5.6}Heads selection}{41}{subsection.1.5.6}%
\contentsline {subsection}{\numberline {1.5.7}Normalization and network interpretation }{41}{subsection.1.5.7}%
\contentsline {subsection}{\numberline {1.5.8}Simulated datasets, BoolODE and Sergio}{42}{subsection.1.5.8}%
\contentsline {subsection}{\numberline {1.5.9}BenGRN and gene network metrics}{42}{subsection.1.5.9}%
\contentsline {subsection}{\numberline {1.5.10}Other evaluation metrics}{43}{subsection.1.5.10}%
\contentsline {subsection}{\numberline {1.5.11}Denoising Benchmarks}{44}{subsection.1.5.11}%
\contentsline {subsection}{\numberline {1.5.12}Fine-tuning}{44}{subsection.1.5.12}%
\contentsline {subsection}{\numberline {1.5.13}State-of-the-art methods used in benchmarking}{44}{subsection.1.5.13}%
\contentsline {subsubsection}{Gene network inference with an ensemble of trees (GENIE3)}{45}{section*.36}%
\contentsline {subsubsection}{DeepSEM}{45}{section*.37}%
\contentsline {subsubsection}{Single-cell generative pretraining transformer (scGPT)}{45}{section*.38}%
\contentsline {subsubsection}{Geneformer}{46}{section*.39}%
\contentsline {subsubsection}{scFoundation}{46}{section*.40}%
\contentsline {subsubsection}{Marker-based cell type prediction with CellTypist}{47}{section*.41}%
\contentsline {subsubsection}{Classification benchmark and associated methods}{48}{section*.42}%
\contentsline {subsection}{\numberline {1.5.14}Ground truth preparation}{48}{subsection.1.5.14}%
\contentsline {subsubsection}{McCalla et al.}{48}{section*.43}%
\contentsline {subsubsection}{Omnipath}{48}{section*.44}%
\contentsline {subsubsection}{Gene networks from genome-wide perturb-seq}{48}{section*.45}%
\contentsline {subsection}{\numberline {1.5.15}Details on the Benign Prostatic Hyperplasia analysis}{49}{subsection.1.5.15}%
\contentsline {subsection}{\numberline {1.5.16}Negative Binomial to Poisson relationship}{49}{subsection.1.5.16}%
\contentsline {subsection}{\numberline {1.5.17}Data availability}{50}{subsection.1.5.17}%
\contentsline {subsection}{\numberline {1.5.18}Code availability}{50}{subsection.1.5.18}%
\contentsline {chapter}{\numberline {2}Xpressor: Towards foundation models that learn across biological scales}{59}{chapter.2}%
\contentsline {section}{\numberline {2.1}Summary}{59}{section.2.1}%
\contentsline {section}{\numberline {2.2}Introduction}{59}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Foundation models across scales}{60}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Architectural modifications: compressed representations}{62}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Training modifications: fine-tuning}{62}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Contributions}{62}{subsection.2.2.4}%
\contentsline {section}{\numberline {2.3}Xpressor}{63}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Background}{63}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Approach}{63}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Results}{64}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Multi-scale Fine-tuning}{65}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Background}{65}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Approach}{66}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Results}{66}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}proof that fine-tuning ESM2 with an adapter layer is at least sufficient to learn to add co-expression information}{68}{subsection.2.4.4}%
\contentsline {subsection}{\numberline {2.4.5}argument about the Tishby et al. bottleneck learning approach}{68}{subsection.2.4.5}%
\contentsline {subsection}{\numberline {2.4.6}FSQ and other contrastive losses on the cell embeddings}{69}{subsection.2.4.6}%
\contentsline {paragraph}{VQ-VAE.}{69}{section*.53}%
\contentsline {paragraph}{FSQ-VAE.}{69}{section*.54}%
\contentsline {paragraph}{Contrastive regularization across embedding dimensions.}{69}{section*.55}%
\contentsline {paragraph}{Dimension-specific classifiers.}{70}{section*.56}%
\contentsline {chapter}{\numberline {3}Résultats sous forme d'article}{75}{chapter.3}%
\contentsline {chapter}{\numberline {4}Discussion et perspectives}{77}{chapter.4}%
\contentsline {chapter}{Discussion et perspectives}{77}{chapter.4}%
\ttl@starttoc {chapters@1}
\contentsline {section}{\numberline {4.1}AIVC}{77}{section.4.1}%
\contentsline {section}{\numberline {4.2}perturbations}{77}{section.4.2}%
\contentsline {section}{\numberline {4.3}The missing data}{77}{section.4.3}%
\contentsline {chapter}{\numberline {5}Conclusion générale}{79}{chapter.5}%
\contentsline {chapter}{Bibliography}{81}{chapter*.60}%
\contentsfinish 
